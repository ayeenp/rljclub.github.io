[{"content":"Teaching “MammadAI” to Compose — From Noise to Music with RL Abstract This blog post describes a multi-stage project in which MammadAI, a robot musician, evolves from producing random noise to composing music aligned with human preferences. The progression moves through imitation learning, reinforcement learning with music theory constraints, and finally reinforcement learning from human feedback (RLHF). Drawing on prior studies — including Google’s work in 2016 and in 2024 — this post presents methodology, results, and implications for music generation.\n1. Introduction Machine learning models have made strong progress in generating music, but they still struggle with long-term structure, audience feedback, and matching what humans like. MammadAI is a model used to explore how combining different learning approaches can overcome these challenges. The three main phases are:\nLearning from demonstrations (imitation learning) to get a sense of musical style and structure. Using music theory constraints via reinforcement learning (RL) to impose rules. Aligning outputs with human preferences via RLHF (reinforcement learning from human feedback). 2. Imitation Learning \u0026amp; Early Challenges Problem: Pure generative models often lack long-term musical structure. They don’t improve based on audience feedback, and defining a reward function for music is hard.\nSolution: Use imitation learning (for example, GAIL) to learn a reward function from demonstrations.\nSetup:\nA dataset of 10 melodies and 5 rhythmic accompaniments. States defined in several ways: A 128-dimensional binary snapshot of the current 16th note (which pitches just played). A 128-dimensional vector counting how many times each pitch has been played so far. Similar, but giving lower weight to notes played a long time ago. Actions correspond to 89 possible piano notes + a “silent” action.\nOutcome: The model begins to make sounds that feel musical, but still quite far from satisfying human standards.\n3. Reinforcement Learning + Music Theory (Google, 2016) In 2016, Google’s Magenta team introduced a method to combine generative models (Note-RNN) with reinforcement learning. The RL algorithm used was DQN (Deep Q-Network). Reward: Two parts\nA term preserving what the model learned from data (probability from the Note RNN). A music theory reward enforcing constraints, such as: staying in one key, beginning and ending on tonic, avoiding excessive repetition, avoiding very large leaps, having distinct highest and lowest notes, encouraging motifs and their repetition, etc. Results: Compared to the pure Note-RNN, the RL-tuned model improved significantly on behaviors defined in music theory (such as fewer out-of-key notes, fewer repeated notes, etc.). Listeners preferred the RL-tuned versions. :contentReference\n4. Reinforcement Learning from Human Feedback (RLHF) \u0026amp; Music Language Models (Google, 2024) More recently, Google (2024) introduced MusicRL, built by fine-tuning a pretrained music language model called MusicLM with human feedback. How it works:\nMusicLM is trained on pairs of captions (text prompts) and melodies. This lets it generate music in response to prompts. To handle the fact that users may dislike what it generates, they build a reward model. The reward model takes a prompt + a generated piece and outputs a scalar showing how much users prefer that output. Training the reward model involves showing humans two generated versions for the same prompt; asking which one they prefer; repeating this many times to build a dataset of pairwise preferences. Algorithm: They used PPO (Proximal Policy Optimization) to fine-tune the MusicLM model using the reward model.\nResults:\nModels fine-tuned with RLHF (MusicRL-R, MusicRL-U, MusicRL-RU) perform significantly better than the baseline MusicLM according to human raters. :contentReference The combination of text adherence, audio quality, and user preference signals leads to the best overall performance. 5. Implications \u0026amp; Lessons Combining generative modeling with reinforcement learning helps address issues like structure and followability in music. Embedding music theory as reward constraints is effective: it reduces undesirable behaviors and improves musicality. Human feedback is essential: what sounds “good” is subjective. RLHF allows the model to adapt to human taste, not just formal rules. The multi-stage approach (from imitation learning to theory-driven RL to RLHF) seems promising because each stage fixes some but not all problems. 6. Conclusion MammadAI’s journey — from imitation learning to theory-based reinforcement to human-preference alignment — mirrors the evolution of music generation research. The latest results with RLHF show that we can build systems that don’t just obey rules, but also produce music people genuinely prefer.\nThere are still open challenges: balancing creativity versus coherence; ensuring variety without losing style; extending to more instruments, rhythms, and genres; adapting to diverse listeners. But the advances so far suggest the gap between machine-made and human-loved music is steadily closing.\nReferences Lowe, Sam. An Inverse Reinforcement Learning Approach to Generative Music. 2020. https://doi.org/10.17615/nmvf-e943\nJaques, Natasha, Shixiang Gu, Dzmitry Bahdanau, José Miguel Hernández-Lobato, Richard E. Turner, and Douglas Eck. “Sequence Tutor: Conservative Fine-Tuning of Sequence Generation Models with KL-Control.” arXiv, 2017, https://arxiv.org/abs/1611.02796.\nKotecha, Nikhil. “Bach2Bach: Generating Music Using a Deep Reinforcement Learning Approach.” arXiv, 2018, https://arxiv.org/abs/1812.01060\nAgostinelli, Andrea, Timo I. Denk, Zalán Borsos, Jesse Engel, Mauro Verzetti, Antoine Caillon, Qingqing Huang, Aren Jansen, Adam Roberts, Marco Tagliasacchi, Matt Sharifi, Neil Zeghidour, and Christian Frank. “MusicLM: Generating Music From Text.” arXiv, 2023, https://arxiv.org/abs/2301.11325\nCideron, Geoffrey, Sertan Girgin, Mauro Verzetti, Damien Vincent, Matej Kastelic, Zalán Borsos, Brian McWilliams, Victor Ungureanu, Olivier Bachem, Olivier Pietquin, Matthieu Geist, Léonard Hussenot, Neil Zeghidour, and Andrea Agostinelli. “MusicRL: Aligning Music Generation to Human Preferences.” arXiv, 2024, https://arxiv.org/abs/2402.04229.\n","permalink":"http://localhost:1313/posts/rl_for_music/","summary":"\u003ch1 id=\"teaching-mammadai-to-compose--from-noise-to-music-with-rl\"\u003eTeaching “MammadAI” to Compose — From Noise to Music with RL\u003c/h1\u003e\n\u003ch2 id=\"abstract\"\u003eAbstract\u003c/h2\u003e\n\u003cp\u003eThis blog post describes a multi-stage project in which MammadAI, a robot musician, evolves from producing random noise to composing music aligned with human preferences. The progression moves through imitation learning, reinforcement learning with music theory constraints, and finally reinforcement learning from human feedback (RLHF). Drawing on prior studies — including Google’s work in 2016 and in 2024 — this post presents methodology, results, and implications for music generation.\u003c/p\u003e","title":"RL for Music"},{"content":"Actor-Critic vs. Value-Based: Empirical Trade-offs Introduction In Reinforcement Learning (RL), one of the fundamental questions is:\nShould we focus on learning the value of states, or should we directly learn the optimal policy?\nBefore diving into this comparison, let’s briefly recall what RL is: a branch of machine learning in which an agent interacts with an environment to learn how to make decisions that maximize cumulative reward.\nTraditionally, RL algorithms fall into two main families:\nValue-Based methods : which aim to estimate the value of states or state–action pairs. Policy-Based methods : which directly optimize a policy that maps states to actions. Actor-Critic algorithms combine the strengths of both worlds, simultaneously learning a value function and an explicit policy. This hybrid structure can often lead to more stable and efficient learning.\nValue-Based and Actor-Critic approaches represent fundamentally different perspectives: one focuses solely on learning state values, while the other integrates both value and policy learning. Comparing these two perspectives helps us better understand the impact of incorporating value or policy components in different environments.\nIn this project, we empirically evaluate these two families in both discrete and continuous action spaces. Four representative algorithms ( DQN, A2C, NAF, and SAC ) were implemented, along with a user-friendly graphical user interface (GUI) for training and evaluation.\nOur ultimate goal is to analyze trade-offs such as convergence speed, learning stability, and final performance across diverse scenarios.\nBackground The reinforcement learning algorithms used in this project fall into two main families:\nValue-Based\nIn this approach, the agent learns only a value function, such as $Q(s, a)$.\nThe policy is derived implicitly by selecting the action with the highest value:\n$\\pi(s) = \\arg\\max_a Q(s,a)$\nThis method is typically simpler, more stable, and computationally efficient.\nHowever, it faces limitations when dealing with continuous action spaces.\nExample algorithms: DQN, NAF.\nValue-Based methods are often well-suited for discrete action spaces with relatively small state–action domains, where enumerating or approximating the value for each action is feasible.\nActor-Critic\nIn this framework, the agent consists of two components:\nActor : a parameterized policy that directly produces actions. Critic : a value function that evaluates the Actor’s performance and guides its updates. This combination can provide greater learning stability, improved performance in complex environments, and high flexibility in continuous action spaces.\nExample algorithms: A2C, SAC.\nActor-Critic methods are generally more suitable for continuous or high-dimensional action spaces, as the Actor can output actions directly without exhaustive value estimation.\nMethodology Project Design This project was designed to perform an empirical comparison between two major families of reinforcement learning algorithms: Value-Based and Actor-Critic.\nFour representative algorithms were selected and implemented in diverse discrete and continuous environments.\nTraining, evaluation, and comparison were carried out through a fully interactive, user-friendly graphical interface.\nImplemented Algorithms Representative algorithms from the two families were selected based on their reported performance in different environments according to the literature.\nFor each algorithm, the training procedure was reproduced in accordance with its original paper.\nThe overall structure of each algorithm is summarized below:\nAlgorithm Family Action Space Description Reference Deep Q-Network (DQN) Value-Based Discrete Uses experience replay and a fixed target network to stabilize learning. 1 Normalized Advantage Function (NAF) Value-Based Continuous Value-based method for continuous spaces using a specific Q-structure to simplify action selection. 2 Advantage Actor-Critic (A2C) Actor-Critic Discrete/Continuous Direct policy optimization guided by an advantage function. 3 Soft Actor-Critic (SAC) Actor-Critic Continuous Off-policy actor-critic method maximizing entropy for stability in complex environments. 4 Deep Q-Network (DQN) The pseudocode of DQN highlights the use of experience replay and a target network, which together reduce correlations between samples and stabilize training.\nNormalized Advantage Function (NAF) NAF handles continuous action spaces by constraining the Q-function into a quadratic form, which makes action selection computationally efficient.\nAdvantage Actor-Critic (A2C) A2C directly optimizes a parameterized policy (Actor) with guidance from the Critic, using advantage estimation to reduce gradient variance and improve learning stability.\nSoft Actor-Critic (SAC) SAC introduces entropy maximization in the objective, encouraging exploration and robustness in complex continuous environments.\nEnvironments There were many 2D and 3D environments available so that we could compare them.\nThe 12 famous environments are listed below:\nFour environments from the Gym library were selected to provide a diverse set of challenges that cover both discrete and continuous action spaces, as well as varying levels of complexity and dynamics:\nMountainCar-v0 (Discrete): A classic control problem where the agent must drive a car up a hill using discrete acceleration commands. This environment tests basic exploration and planning in a low-dimensional, discrete action space. Pendulum-v1 (Continuous): Requires applying continuous torque to keep a pendulum upright. This environment is ideal for evaluating continuous control algorithms and stabilizing dynamics. FrozenLake-v1 (Discrete): A gridworld task where the agent navigates an icy lake to reach a goal while avoiding holes. This environment emphasizes decision-making under uncertainty in a discrete setting. HalfCheetah-v4 (Continuous): A high-dimensional continuous control environment where the agent controls a bipedal cheetah to run efficiently. It challenges advanced continuous control and balance strategies. These environments were chosen to allow a comprehensive comparison of algorithms across different action types, state complexities, and control challenges.\nEnvironment Type Description MountainCar-v0 Discrete Drive a car up a hill by controlling acceleration in a discrete space. Pendulum-v1 Continuous Apply torque to keep a pendulum upright and stable. FrozenLake-v1 Discrete Navigate an icy grid to reach the goal without falling into holes. HalfCheetah-v4 Continuous Control the speed and balance of a simulated bipedal cheetah for fast running. Environment snapshots were recorded during training and appear as GIFs in the Results section.\nConfiguration and Evaluation Algorithms were run with optimized settings for each environment. Key hyperparameters (learning rate, γ, batch size, buffer size) were tuned through trial-and-error, leveraging existing GitHub implementations for optimal performance. Comparisons were based on convergence speed, training stability, and final performance. Example configurations:\nAlgorithm Environment γ Learning Rate Batch Size Buffer Size DQN FrozenLake 0.93 6e-4 32 4,000 A2C MountainCar 0.96 1e-3 – – NAF Pendulum 0.99 3e-4 64 400,000 SAC HalfCheetah 0.99 3e-4 256 1,000,000 Graphical User Interface (GUI) A user-friendly GUI was developed to simplify training and comparing algorithms, enabling full project execution without direct coding.The code is available at: This Github Link The project structure is as follows:\nProject_Code/ ├── plots/ │ └── learning_curves/ │ │ ├── Continuous/ │ │ │ ├── Pendulum-v1/ │ │ │ │ ├── SAC/ │ │ │ │ └── NAF/ │ │ │ └── HalfCheetah-v4/ │ │ │ ├── SAC/ │ │ │ └── NAF/ │ │ └── Discrete/ │ │ ├── FrozenLake-v1/ │ │ │ ├── DQN/ │ │ │ │ ├── 4x4 │ │ │ │ └── 8x8 │ │ │ └── A2C/ │ │ │ ├── 4x4 │ │ │ └── 8x8 │ │ └── MountainCar-v0/ │ │ ├── DQN/ │ │ └── A2C/ │ ├── comparison_table.png │ └── directory_structure.png ├── requirements/ │ ├── base.txt │ ├── dev.txt │ ├── env.txt │ └── config.py ├── src/ │ ├── agents/ │ │ ├── a2c.py │ │ ├── dqn.py │ │ ├── naf.py │ │ └── sac.py │ ├── envs/ │ │ ├── continuous_envs.py │ │ └── discrete_envs.py │ ├── main.py │ ├── train.py │ └── utils.py ├── videos/ │ ├── Continuous/ │ │ ├── Pendulum-v1/ │ │ │ ├── SAC/ │ │ │ └── NAF/ │ │ └── HalfCheetah-v4/ │ │ ├── SAC/ │ │ └── NAF/ │ └── Discrete/ │ ├── FrozenLake-v1/ │ │ ├── DQN/ │ │ │ ├── 4x4 │ │ │ └── 8x8 │ │ └── A2C/ │ │ ├── 4x4 │ │ └── 8x8 │ └── MountainCar-v0/ │ ├── DQN/ │ └── A2C/ ├── models/ │ ├── Continuous/ │ │ ├── Pendulum-v1/ │ │ │ ├── SAC/ │ │ │ └── NAF/ │ │ └── HalfCheetah-v4/ │ │ ├── SAC/ │ │ └── NAF/ │ └── Discrete/ │ ├── FrozenLake-v1/ │ │ ├── DQN/ │ │ │ ├── 4x4 │ │ │ └── 8x8 │ │ └── A2C/ │ │ ├── 4x4 │ │ └── 8x8 │ └── MountainCar-v0/ │ ├── DQN/ │ └── A2C/ └── README.md Main features:\nSelect algorithm, environment, action space type (discrete/continuous), and execution mode (train/test) with just a few clicks. Launch training with a Run button, and view results via Show Plots and Show Videos. Compare algorithms interactively using a dedicated Compare Algorithms window. Display key settings such as hyperparameters and project structure. Real-time console output for monitoring execution status and system messages. Interactive Mobile Application In addition to the desktop GUI, a mobile application for Android and iOS has been developed to provide interactive access to the project. By simply scanning the poster, users can explore various features, including:\nViewing videos of agent executions in different environments. Opening the project’s website for additional resources and documentation. Displaying the poster digitally for interactive exploration. Comparing learning curves and results across different algorithms. The images below showcase some sections of the app interface:\nResults This section presents the empirical evaluation of four reinforcement learning algorithms (DQN, NAF, A2C, and SAC) from the Value-Based and Actor-Critic families, across both discrete and continuous action space environments. The performance is analyzed based on final reward, convergence speed, and training stability, supported by quantitative metrics, qualitative visualizations (GIFs), and learning curves with a moving average (MA) applied to reduce noise. The experiments were conducted using the Gymnasium library, with optimized hyperparameters (see Methodology for details) and a fixed random seed for reproducibility. Training was performed on a single NVIDIA RTX 4050 GPU, with average runtimes of 1–4 hours per algorithm-environment pair.\nDiscrete Environments The discrete action space environments tested were FrozenLake-v1 (8x8 grid) and MountainCar-v0, which challenge the algorithms with stochastic transitions and sparse rewards, respectively. The table below summarizes the performance, followed by detailed analyses and visualizations.\nEnvironment Algorithm Final Reward (Avg ± Std) Convergence Speed (Episodes to 90% of Max) Stability (Std of Reward) FrozenLake-v1 DQN 0.98 ± 0.14 ~1,475 0.50 A2C 1.00 ± 0.00 ~1,209 0.48 MountainCar-v0 DQN -22.21 ± 79.32 ~2,000 81.50 A2C -27.87 ± 62.35 ~2,000 40.83 FrozenLake-v1 In FrozenLake-v1, a stochastic gridworld, A2C outperformed DQN in final reward (1.00 vs. 0.98 success rate) and converged faster (~1209 vs. ~1475 episodes to reach 90% of max reward). A2C’s advantage estimation provided greater stability, as evidenced by its lower standard deviation (0.48 vs. 0.50). The GIF below illustrates agent behaviors, showing A2C’s smoother navigation to the goal compared to DQN’s occasional missteps.\nReward comparison in FrozenLake-v1: A2C converges quickly and maintains stable performance compared to DQN.\nMountainCar-v0 In MountainCar-v0, a deterministic environment with sparse rewards, DQN achieved a better final reward (-22.21 vs. -27.87 timesteps to goal) but both algorithms converged at similar speeds (~2000 episodes). However, A2C exhibited greater stability (std of 40.83 vs. 81.50), avoiding large oscillations in learning. The GIF below shows DQN’s quicker ascent to the hilltop, while A2C maintains more consistent swings.\nReward comparison in MountainCar-v0: DQN converges faster and reaches higher rewards, while A2C shows greater stability.\nContinuous Environments The continuous action space environments tested were Pendulum-v1 and HalfCheetah-v4, which require precise control and balance in low- and high-dimensional settings, respectively. The table below summarizes the performance.\nEnvironment Algorithm Final Reward (Avg ± Std) Convergence Speed (Episodes to 90% of Max) Stability (Std of Reward) Pendulum-v1 NAF -141.17 ± 85.58 ~246 199.46 SAC 287.66 ± 62.38 ~152 113.23 HalfCheetah-v4 NAF 3,693.35 ± 575.60 ~862 1,077.01 SAC 10,247.42 ± 584.31 ~1,127 2,493.55 Pendulum-v1 In Pendulum-v1, SAC significantly outperformed NAF in final reward (287.66 vs. -141.17, higher is better) and converged faster (~152 vs. ~246 episodes). SAC’s entropy maximization ensured smoother learning, with a standard deviation of 113.23 compared to NAF’s 199.46. The GIF below highlights SAC’s ability to stabilize the pendulum upright, while NAF struggles with inconsistent torque.\nReward comparison in Pendulum-v1: SAC achieves higher rewards with smoother convergence compared to NAF.\nHalfCheetah-v4 In HalfCheetah-v4, a high-dimensional control task, SAC achieved a much higher final reward (10247.42 vs. 3693.35) but converged slightly slower (~1127 vs. ~862 episodes). SAC’s stability (std of 2493.55 vs. 1077.01) reflects its robustness in complex dynamics, though NAF shows lower variance. The GIF below shows SAC’s fluid running motion compared to NAF’s less coordinated movements.\nReward comparison in HalfCheetah-v4: SAC consistently outperforms NAF in both speed and stability.\nDiscussion \u0026amp; Conclusion This project examined the performance differences between Value-Based and Actor-Critic algorithms in both discrete and continuous environments.\nThe experimental results indicate that no single algorithm is universally superior; rather, the environment characteristics and action space type play a decisive role in determining performance.\nAnalysis \u0026amp; Interpretation Continuous Action Spaces SAC consistently outperformed NAF in both Pendulum-v1 and HalfCheetah-v4, thanks to its entropy maximization strategy, which promotes exploration and robustness. NAF’s fixed quadratic Q-function structure limited its flexibility in high-dimensional or complex tasks, leading to slower convergence and higher variance in rewards. SAC’s ability to directly optimize a stochastic policy made it particularly effective in continuous control scenarios.\nDiscrete Action Spaces In FrozenLake-v1, a stochastic environment, A2C’s stability (due to advantage estimation) gave it an edge over DQN, achieving higher success rates and faster convergence. In MountainCar-v0, a deterministic environment with a small action space, DQN’s value-based approach excelled in final reward and convergence speed, though A2C remained more stable. This highlights the suitability of Value-Based methods for simpler, deterministic settings and Actor-Critic methods for stochastic or complex environments.\nKey Findings: In simple discrete environments, such as FrozenLake, Value-Based algorithms (e.g., DQN) achieved competitive performance, but Actor-Critic algorithms (e.g., A2C) showed faster convergence and more stable learning. In continuous and more complex environments, Actor-Critic algorithms — particularly SAC — outperformed their Value-Based counterparts in terms of final reward and convergence speed. Observed Trade-offs: Aspect Value-Based Actor-Critic Simplicity of implementation Yes More complex Initial learning speed High in simple environments Depends on tuning Training stability More oscillations More stable Suitability for continuous spaces Not always Yes Overall, the choice between Value-Based and Actor-Critic methods should be guided by the nature of the task, the complexity of the environment, and the available computational budget.\nObservations Based on Environment Characteristics Our experimental results further reveal that the nature of the environment—in terms of action space, state space, and reward structure—significantly impacts algorithm performance:\nAction Space (Discrete vs. Continuous) Discrete Action Spaces: Value-Based algorithms like DQN tend to perform competitively, especially in small and low-dimensional discrete action spaces. They converge quickly and reliably, but may struggle when the action space grows larger or stochasticity increases. Actor-Critic methods such as A2C can still provide improved stability in these scenarios, especially under stochastic transitions. Continuous Action Spaces: Actor-Critic methods (e.g. SAC) dominate due to their ability to output continuous actions directly. Value-Based methods require specialized approximations (like NAF), which often limit flexibility and performance in high-dimensional or continuous control tasks. State Space (Low-dimensional vs. High-dimensional) Low-dimensional states (e.g., MountainCar, FrozenLake) generally favor Value-Based methods, which can efficiently enumerate or approximate Q-values. High-dimensional states (e.g., HalfCheetah) require the policy network of Actor-Critic methods to generalize across large state spaces. These methods better handle complex dynamics and correlations among state variables. Reward Structure (Sparse vs. Dense) Sparse Reward Environments (e.g., FrozenLake) challenge Value-Based methods to propagate value signals efficiently, potentially slowing convergence. Actor-Critic algorithms can leverage advantage estimation and policy gradients to maintain learning stability even with sparse rewards. Dense Reward Environments (e.g., HalfCheetah, Pendulum) allow both families to learn effectively, but Actor-Critic methods often achieve smoother and faster convergence due to direct policy optimization combined with value guidance. The interplay between action space type, state space complexity, and reward sparsity fundamentally shapes the suitability of each algorithm. In general:\nDiscrete + Low-dimensional + Dense reward → Value-Based methods are competitive. Continuous + High-dimensional + Sparse or Dense reward → Actor-Critic methods provide superior learning stability and higher final performance. These insights complement the empirical trade-offs already observed in our study, providing a more nuanced understanding of when and why certain RL algorithms excel under different environment characteristics.\nLimitations and Future Work Limitations The present project, which evaluates the performance of reinforcement learning algorithms (DQN, A2C, NAF, and SAC) in discrete (FrozenLake-v1 and MountainCar-v0) and continuous (Pendulum-v1 and HalfCheetah-v4) environments, provides valuable insights but is subject to several limitations, outlined below:\nLimited Number of Environments:\nThe study only examines four specific environments, which may not provide sufficient diversity to generalize results across all types of reinforcement learning environments. More complex environments with larger state or action spaces or different dynamics could yield different outcomes. Lack of Random Seed Variation:\nThe reported results are based on a single run or an average of a limited number of runs. Conducting multiple experiments with different random seeds could better demonstrate the robustness and reliability of the results. Focus on Specific Metrics:\nThe evaluation metrics (final reward average, convergence speed, and stability) cover only certain aspects of algorithm performance. Other metrics, such as computational efficiency, training time, or robustness to environmental noise, were not assessed. Future Work To build upon the findings of this project, which evaluated the performance of reinforcement learning algorithms (DQN, A2C, NAF, and SAC) in discrete (FrozenLake-v1, MountainCar-v0) and continuous (Pendulum-v1, HalfCheetah-v4) environments, several directions for future research and development can be pursued to address the limitations and extend the scope of the study:\nBroader Range of Environments:\nFuture work could include testing the algorithms on a wider variety of environments, such as those with larger state and action spaces, partially observable states (e.g., POMDPs), or real-world-inspired tasks. This would help validate the generalizability of the observed performance trends. Incorporating Random Seed Variations:\nConducting multiple runs with different random seeds would improve the robustness of results and allow for statistical analysis of performance variability, ensuring that conclusions are not biased by specific initial conditions. Evaluation of Additional Metrics:\nFuture studies could incorporate metrics such as computational efficiency, memory usage, training time, and robustness to environmental perturbations (e.g., noise or dynamic changes). This would provide a more holistic view of algorithm suitability for practical applications. Real-World Application: Robotics The insights gained from this project have significant potential for real-world applications, particularly in robotics, where reinforcement learning can enable autonomous systems to perform complex tasks. The following outlines how the evaluated algorithms could be applied and extended in robotics contexts:\nRobotic Manipulation:\nAlgorithms like SAC, which performed well in continuous control tasks (e.g., Pendulum-v1, HalfCheetah-v4), could be applied to robotic arms for tasks such as grasping, object manipulation, or assembly. SAC’s ability to handle continuous action spaces makes it suitable for precise control in high-dimensional settings. Autonomous Navigation:\nDiscrete action space algorithms like DQN and A2C, tested in environments like FrozenLake-v1, could be adapted for robot navigation in grid-like or structured environments (e.g., warehouse robots). A2C’s stability in stochastic settings could be particularly useful for navigating dynamic or uncertain environments. Locomotion and Mobility:\nThe success of SAC in HalfCheetah-v4 suggests its potential for controlling legged robots or humanoid robots for locomotion tasks. Future work could involve applying SAC to real-world robotic platforms to achieve robust and efficient walking or running behaviors. These future research directions and real-world applications highlight the potential to extend the current study’s findings to more diverse and practical scenarios. By addressing the identified limitations and applying the algorithms to robotics, this work can contribute to the development of more robust, efficient, and adaptable autonomous systems.\nReferences A2C (Advantage Actor-Critic):\nMnih, V., et al. (2016). \u0026ldquo;Asynchronous Methods for Deep Reinforcement Learning.\u0026rdquo; ICML 2016. Paper Stable-Baselines3 A2C Implementation: GitHub SAC (Soft Actor-Critic):\nHaarnoja, T., et al. (2018). \u0026ldquo;Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor.\u0026rdquo; ICML 2018. Paper Stable-Baselines3 SAC Implementation: GitHub DQN (Deep Q-Network):\nMnih, V., et al. (2015). \u0026ldquo;Human-level control through deep reinforcement learning.\u0026rdquo; Nature, 518(7540), 529-533. Paper Stable-Baselines3 DQN Implementation: GitHub NAF (Normalized Advantage Function):\nGu, S., et al. (2016). \u0026ldquo;Continuous Deep Q-Learning with Model-based Acceleration.\u0026rdquo; ICML 2016. Paper Gymnasium:\nOfficial Gymnasium Documentation: Gymnasium ","permalink":"http://localhost:1313/posts/actor-critic-vs-value-based-empirical-trade-offs/","summary":"This study compares Value-Based (DQN, NAF) and Actor-Critic (A2C, SAC) reinforcement learning algorithms in diverse environments (MountainCar-v0, Pendulum-v1, FrozenLake-v1, HalfCheetah-v4). Through empirical evaluation, we analyze trade-offs in convergence speed, learning stability, and final performance, supported by a user-friendly GUI and mobile application for interactive training and visualization.","title":"Actor-Critic vs. Value-Based: Empirical Trade-offs"},{"content":"Introduction This paper critically examines the assumptions commonly accepted in modeling reinforcement learning problems, suggesting that these assumptions may impede progress in the field. The authors refer to these assumptions as \u0026ldquo;dogmas.\u0026rdquo;\nDogma: A fixed, especially religious, belief or set of beliefs that people are expected to accept without any doubts. —From the Cambridge Advanced Learner\u0026rsquo;s Dictionary \u0026amp; Thesaurus\nThe paper introduces three central dogmas:\nThe Environment Spotlight Learning as Finding a Solution The Reward Hypothesis (although not exactly a dogma) The author argues the true reinforcement learning landscape is actualy like this,\nIn the words of Rich Sutton:\nRL can be viewed as a microcosm of the whole AI problem.\nHowever, today\u0026rsquo;s RL landscape is overly simplified,\nThese three dogmas are responsible for narrowing the potential of RL,\nThe authors propose that we consider moving beyond these dogmas,\nTo reclaim the true landscape of RL,\nBackground The authors reference Thomas Kuhn\u0026rsquo;s book, \u0026ldquo;The Structure of Scientific Revolutions\u0026rdquo;,\nKuhn distinguishes between two phases of scientific activity,\nNormal Science: Resembling puzzle-solving. Revolutionary Phase: Involving a fundamental rethinking of the values, methods, and commitments of science, which Kuhn calls a \u0026ldquo;paradigm.\u0026rdquo; Here\u0026rsquo;s an example of a previous paradigm shift in science:\nThe authors explore the paradigm shift needed in RL:\nDogma One: The Environment Spotlight The first dogma we call the environment spotlight, which refers to our collective focus on modeling environments and environment-centric concepts rather than agents.\nWhat do we mean when we say that we focus on environments? We suggest that it is easy to answer only one of the following two questions:\nWhat is at least one canonical mathematical model of an environment in RL?\nMDP and its variants! And we define everything in terms of it. By embracing the MDP, we are allowed to import a variety of fundamental results and algorithms that define much of our primary research objectives and pathways. For example, we know every MDP has at least one deterministic, optimal, stationary policy, and that dynamic programming can be used to identify this policy. What is at least one canonical mathematical model of an agent in RL?\nIn contrast, this question has no clear answer! The author suggests it is important to define, model, and analyse agents in addition to environments. We should build toward a canonical mathematical model of an agent that can open us to the possibility of discovering general laws governing agents (if they exist).\nDogma Two: Learning as Finding a Solution The second dogma is embedded in the way we treat the concept of learning. We tend to view learning as a finite process involving the search for—and eventual discovery of—a solution to a given task.\nWe tend to implicitly assume that the learning agents we design will eventually find a solution to the task at hand, at which point learning can cease. Such agents can be understood as searching through a space of representable functions that captures the possible action-selection strategies available to an agent, similar to the Problem Space Hypothesis, and, critically, this space contains at least one function—such as the optimal policy of an MDP—that is of sufficient quality to consider the task of interested solved. Often, we are then interested in designing learning agents that are guaranteed to converge to such an endpoint, at which point the agent can stop its search (and thus, stop its learning).\nThe author suggests to embrace the view that learning can also be treated as adaptation. As a consequence, our focus will drift away from optimality and toward a version of the RL problem in which agents continually improve, rather than focus on agents that are trying to solve a specific problem.\nWhen we move away from optimality,\nHow do we think about evaluation? How, precisely, can we define this form of learning, and differentiate it from others? What are the basic algorithmic building blocks that carry out this form of learning, and how are they different from the algorithms we use today? Do our standard analysis tools such as regret and sample complexity still apply? These questions are important, and require reorienting around this alternate view of learning.\nThe authors introduce the book \u0026ldquo;Finite and Infinite Games\u0026rdquo;,\nAnd the concept of Finite and Infinite Games is summarized in the following quote,\nThere are at least two kinds of games, One could be called finite; the other infinite. A finite game is played for the purpose of winning, an infinite game for the purpose of continuing the play.\nAnd argues alignment is an infinite game.\nDogma Three: The Reward Hypothesis The third dogma is the reward hypothesis, which states \u0026ldquo;All of what we mean by goals and purposes can be well thought of as maximization of the expected value of the cumulative sum of a received scalar signal (reward).\u0026rdquo;\nThe authors argue that the reward hypothesis is not truly a dogma. Nevertheless, it is crucial to understand its nuances as we continue to design intelligent agents.\nThe reward hypothesis basically says,\nIn recent analysis by [2] fully characterizes the implicit conditions required for the hypothesis to be true. These conditions come in two forms. First, [2] provide a pair of interpretative assumptions that clarify what it would mean for the reward hypothesis to be true or false—roughly, these amount to saying two things (brwon doors).\nFirst, that \u0026ldquo;goals and purposes\u0026rdquo; can be understood in terms of a preference relation on possible outcomes. Second, that a reward function captures these preferences if the ordering over agents induced by value functions matches that of the ordering induced by preference on agent outcomes. This leads to the following conjecture,\nThen, under this interpretation, a Markov reward function exists to capture a preference relation if and only if the preference relation satisfies the four von Neumann-Morgenstern axioms, and a fifth Bowling et al. call $\\gamma$-Temporal Indifference.\nAxiom 1: Completeness \u0026gt; You have a preference between every outcome pair.\nYou can always compare any two choices. Axiom 2: Transitivity \u0026gt; No preference cycles.\nIf you like chocolate more than vanilla, and vanilla more than strawberry, you must like chocolate more than strawberry. Axiom 3: Independence \u0026gt; Independent alternatives can\u0026rsquo;t change your preference.\nIf you like pizza more than salad, and you have to choose between a lottery of pizza or ice cream and a lottery of salad or ice cream, you should still prefer the pizza lottery over the salad lottery. Axiom 4: Continuity \u0026gt; There is always a break even chance.\nImagine you like a 100 dollar bill more than a 50 dollar bill, and a 50 dollar bill more than a 1 dollar bill. There should be a scenario where getting a chance at 100 dollar and 1 dollar, with certain probabilities, is equally good as getting the 50 dollar for sure. These 4 axioms are called the von Neumann-Morgenstern axioms.\nAxiom 5: Temporal $\\boldsymbol{\\gamma}$-Indifference \u0026gt; Discounting is consistent throughout time. Temporal $\\gamma$-indifference says that if you are indifferent between receiving a reward at time $t$ and receiving the same reward at time $t+1$, then your preference should not change if we move both time points by the same amount. For instance, if you don\u0026rsquo;t care whether you get a candy today or tomorrow, then you should also not care whether you get the candy next week or the week after. Taking these axioms into account, the reward conjecture becomes the reward theorem,\nIt is essential to consider that people do not always conform to these axioms, and human preferences can vary.\nIt is important that we are aware of the implicit restrictions we are placing on the viable goals and purposes under consideration when we represent a goal or purpose through a reward signal. We should become familiar with the requirements imposed by the five axioms, and be aware of what specifically we might be giving up when we choose to write down a reward function.\nSee Also David Abel Presentation @ ICML 2023 David Abel Personal Website Mark Ho Personal Website Anna Harutyunyan Personal Website References [1] Abel, David, Mark K. Ho, and Anna Harutyunyan. \u0026ldquo;Three Dogmas of Reinforcement Learning.\u0026rdquo; arXiv preprint arXiv:2407.10583 (2024).\n[2] Bowling, Michael, et al. \u0026ldquo;Settling the reward hypothesis.\u0026rdquo; International Conference on Machine Learning. PMLR, 2023.\n","permalink":"http://localhost:1313/posts/three-dogmas-of-reinforcement-learning/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eThis paper critically examines the assumptions commonly accepted in modeling reinforcement learning problems, suggesting that these assumptions may impede progress in the field. The authors refer to these assumptions as \u0026ldquo;dogmas.\u0026rdquo;\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003eDogma: A fixed, especially religious, belief or set of beliefs that people are expected to accept without any doubts.\u003c/strong\u003e \u003cem\u003e—From the \u003ca href=\"https://dictionary.cambridge.org/dictionary/english\"\u003eCambridge Advanced Learner\u0026rsquo;s Dictionary \u0026amp; Thesaurus\u003c/a\u003e\u003c/em\u003e\u003c/p\u003e\u003c/blockquote\u003e\n\u003cp\u003eThe paper introduces three central dogmas:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eThe Environment Spotlight\u003c/li\u003e\n\u003cli\u003eLearning as Finding a Solution\u003c/li\u003e\n\u003cli\u003eThe Reward Hypothesis (although not exactly a dogma)\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eThe author argues the true reinforcement learning landscape is actualy like this,\u003c/p\u003e","title":"Three Dogmas of Reinforcement Learning"},{"content":"Introduction This paper studies the problem of zero-shot generalization in reinforcement learning and introduces an algorithm that trains an ensamble of maximum reward seeking agents and an maximum entropy agent. At test time, either the ensemble agrees on an action, and we generalize well, or we take exploratory actions with the help of maximum entropy agent and drive us to a novel part of the state space, where the ensemble may potentially agree again. This method combined with an invariance based approach achieves new state-of-the-art results on ProcGen.\nBackground I know it\u0026rsquo;s a lot to take in! You may be wondering:\nWhat is reinforcement learning? 🥺 What generalization means for RL? 🥲 What is zero-shot generalization? 🥹 What are max reward and max entropy agents?! ☹️ What is an ensamble of them?!! 😟 What is an invariance based approach? 😓 And what the heck is ProcGen?!!! 😠 Don\u0026rsquo;t worry! We are going to cover all of that and more! And you are going to fully understand this paper and finish reading this article with an smile 🙂!\nWhat is reinforcement learning? Reinforcement learning is learning what to do—how to map situations to actions—so as to maximize a numerical reward signal. Imagine yourself right now, in reinforcement learning terms, you are an agent and everything that is not you, is your environment. You perceive the world through your senses (e.g., eyes, ears, etc.) and what you perceive turns into electrical signals that your brain processes to form an understanding of your surroundings (state). Based on this understanding, you make decisions (actions) with the goal of achieving the best possible outcome for yourself (reward).\nIn a more formal sense, reinforcement learning involves the following components:\nAgent: The learner or decision maker (e.g., you). Environment: Everything the agent interacts with (e.g., the world around you). State ($s$): A representation of the current situation of the agent within the environment (e.g., what you see, hear, and feel at any given moment). Actions ($a$): The set of all possible moves the agent can make (e.g., moving your hand, walking, speaking). Reward ($r$): The feedback received from the environment in response to the agent’s action (e.g., pleasure from eating food, pain from touching a hot surface). Policy ($\\pi$): A strategy used by the agent to decide which actions to take based on the current state (e.g., your habits and decision-making processes). Value Function ($V$): A function that estimates the expected cumulative reward of being in a certain state and following a particular policy (e.g., your prediction of future happiness based on current actions). The objective of the agent is to develop a policy that maximizes the total cumulative reward over time. This is typically achieved through a process of exploration (trying out new actions to discover their effects) and exploitation (using known actions that yield high rewards).\nIn mathematical terms, the goal is to find a policy $\\pi$ that maximizes the expected return $G_t$, which is the cumulative sum of discounted rewards:\n$$ G_t = r_{t+1} + \\gamma r_{t+2} + \\gamma^2 r_{t+3} + \\cdots $$\nwhere $\\gamma$ (0 ≤ $\\gamma$ \u0026lt; 1) is the discount factor that determines the importance of future rewards.\nFor a more thorough and in depth explanation of reinforcement learning please refer to Lilian Weng excelent blog post A (Long) Peek into Reinforcement Learning.\nWhat generalization means for RL? In reinforcement learning, generalization involves an agent\u0026rsquo;s ability to apply learned policies or value functions to new states or environments that it has not encountered during training. This is essential because it is often impractical or impossible to train an agent on every possible state it might encounter.\nThere are bunch of methods that researchers have used to tackle the problem of generalization in reinforcement learning that are summarized in the following diagram.\nThis paper focuses on RL-Specific solutions and specifically exploration technique to tackle the problem of generalization. If you\u0026rsquo;re interested to learn more about the generalization problem in reinforcement learning please refer to reference [3].\nWhat is zero-shot generalization? Zero-shot generalization in RL refers to the ability of an agent to perform well in entirely new environments or tasks without any prior specific training or fine-tuning on those environments or tasks. This is a significant challenge because it requires the agent to leverage its learned representations and policies in a highly flexible and adaptive manner.\nIn order to define the objective of zero-shot generalization we first have to define what MDPs and POMDPs are.\nMarkov Decision Process (MDP):\nMDP is a mathematical framework used to describe an environment in reinforcement learning where the outcome is partly random and partly under the control of a decision-maker (agent). A MDP is a tuple $M = (S, A, P_{init}, P, r, \\gamma)$ where,\nStates ($S \\in \\mathbb{R}^{|S|}$): A finite set of states that describe all possible situations in which the agent can be. Actions ($A \\in \\mathbb{R}^{|A|}$): A finite set of actions available to the agent. Initial State Distribution ($P_{init}$): A distribution of starting state $(s_0 \\sim P_{init})$. Transition Probability ($P$): A function $P(s_{t+1}, s_t, a_t)$ representing the probability of transitioning from state $s_t$ to state $s_{t+1}$ after taking action $a_t$ $(s_{t+1} \\sim P(.|s_t,a_t))$. Reward ($r: S \\times A \\rightarrow \\mathbb{R}$): A function $r(s_t, a_t)$ representing the immediate reward $r_t$ received after transitioning from state $s_t$ due to action $a_t$ $(r_t = r(s_t, a_t))$. Discount Factor ($\\gamma$): $(0 \\leq \\gamma \u0026lt; 1)$ is a constant that determines the importance of future rewards. Partially Observable Markov Decision Process (POMDP):\nPOMDP extends MDPs to situations where the agent does not have complete information about the current state. Instead, the agent must make decisions based on partial observations. A POMDP is a tuple $M = (S, A, O, P_{init}, P, \\Sigma, r, \\gamma)$ where other that above definitions for MDP,\nObservation Space ($O$): A finite set of observations the agent can receive about the state. Observation Function ($\\Sigma$): A function that given current state $s_t$ and current action $a_t$ gives us the current observation $o_t$ $(o_t = \\Sigma(s_t, a_t) \\in O)$. If we set $O = S$ and $\\Sigma(s,a) = s$, the POMDP turns into regular MDP.\nLet the history at time $t$ be,\n$$ h_t = \\{ o_0, a_0, r_0, o_1, a_1, r_1, \\dots, o_t \\} $$\nThe agent’s next action is outlined by a policy $\\pi$, which is a stochastic mapping from the history to an action probability,\n$$ \\pi(a|h_t) = P(a_t=a|h_t) $$\nIn this formulation, a history-dependent policy (and not a Markov policy) is required both due to partially observed states, epistemic uncertainty, and also for optimal maxEnt exploration.\nWe assume a prior distribution over POMDPs $P(M)$, defined over some space of POMDPs. For a given POMDP, an optimal policy maximizes the expected discounted return,\n$$ \\mathbb{E}_{\\pi,M} \\left[ \\sum _{t=0}^{\\infty} \\gamma^t r(s_t,a_t) \\right] $$\nWhere the expectation is taken over the policy $\\pi(h_t)$, and the state transition probability $s_t \\sim P$ of POMDP $M$.\nOur generalization objective is to maximize the discounted cumulative reward taken in expectation over the POMDP prior,\n$$ \\mathcal{R}_ {pop}(\\pi) = \\mathbb{E}_ {M \\sim P(M)} {\\left[ \\mathbb{E}_{\\pi,M} {\\left[ \\sum _{t=0}^{\\infty} \\gamma^t r(s_t,a_t) \\right]} \\right]} $$\nSeeking a policy that performs well in expectation over any POMDP from the prior corresponds to zero-shot generalization.\nAnd as you may have guessed we don\u0026rsquo;t have access to true prior distribution of POMDPs so we have to estimate it with $N$ training POMDPs $M_1, M_2, \\dots, M_N$ sampled from the true prior distribution $P(M)$. So we are going to maximize empirical discounted cumulative reward,\n$$ \\begin{align*} \\mathcal{R}_ {emp}(\\pi) \u0026amp;= \\frac{1}{N} \\sum _ {i=1}^{N} \\mathbb{E} _ {\\pi,M_i} \\left[ \\sum _ {t=0}^{\\infty} \\gamma^t r(s_t,a_t) \\right] \\\\ \u0026amp;= \\mathbb{E}_ {M \\sim \\hat{P}(M)} {\\left[ \\mathbb{E}_{\\pi,M} {\\left[ \\sum _{t=0}^{\\infty} \\gamma^t r(s_t,a_t) \\right]} \\right]} \\end{align*} $$\nWhere the empirical POMDP distribution $\\hat{P}(M)$ can be different from the true distribution, i.e. $\\hat{P}(M) \\neq P(M)$. In general, a policy that optimizes the empirical reward may perform poorly on the population reward and this is known as overfitting in statistical learning theory.\nWhat are max reward and max entropy agents?! As we have seen, the goal of agents in reinforcement learning is to find a policy $\\pi$ that maximizes the expected discounted return by focusing on actions that lead to the greatest immediate or future rewards. We call these common RL agents \u0026ldquo;max reward agents\u0026rdquo; in this paper. On the other hand, \u0026ldquo;max entropy agents\u0026rdquo; aim to maximize the entropy of the policy for visiting different states. Maximizing entropy encourages the agent to explore a wider range of actions that lead the agent to visit new states even when they don\u0026rsquo;t contribute any reward.\nThis type of agent will help us to make decisions when we have epistemic uncertainty about what to do at test time. Epistemic uncertainty basically means the uncertainty that we have because of our lack of knowledge and can be improved by gathering more information about the situation.\nThe insight of the authors of this paper is that learning a policy that effectively explores the domain is harder to memorize than a policy that maximizes reward for a specific task, and therefore they expect such learned behavior to generalize well.\nWhat is an ensamble of them?!! Ensembling in reinforcement learning involves combining the policies of multiple individual agents to make more accurate and robust decisions. The idea is that by aggregating the outputs of different agents, we can leverage their diverse perspectives and expertise to improve overall performance.\nThis paper uses an ensamble of max reward agents to help the agent decide on the overal epistemic uncetainty that we have at test time.\nWhat is an invariance based approach? Invariance based algorithms in reinforcement learning focus on developing policies that are robust to changes and variations in the environment. These algorithms aim to identify and leverage invariant features or patterns that remain consistent across different environments or tasks. The goal is to ensure that the learned policy performs well not just in the training environment but also in new, unseen environments.\nThis paper uses IDAAC algorithm which is a special kind of DAAC algorithm as its invariance based algorithm.\nDAAC (Decoupled Advantage Actor-Critic) uses two separate networks, one for learning the policy and advantage, and one for learning the value. The value estimates are used to compute the advantage targets.\nIDAAC (Invariant Decoupled Advantage Actor-Critic) adds an additional regularizer to the DAAC policy encoder to ensure that it does not contain episode-specific information. The encoder is trained adversarially with a discriminator so that it cannot classify which observation from a given pair $(s_i, s_j)$ was first in a trajectory.\nFor more information about IDAAC algorithm please refer to reference [4].\nWhat the heck is ProcGen?!!! Procgen is a benchmark suite for evaluating the generalization capabilities of reinforcement learning agents. It was developed by OpenAI and consists of a collection of procedurally generated environments that vary in terms of visual appearance, dynamics, and difficulty. The goal of Procgen is to provide a standardized and challenging set of environments that can be used to assess the ability of RL algorithms to generalize to unseen scenarios.\nIf you are new to reinforcement learning, I know these explanations are a lot to take in! If you find yourself lost, I recommend to check out the following courses at your leisure:\nDeep Reinforcement Learning (by Hugging Face 🤗) Reinforcement Learning (by Mutual Information) Introduction to Reinforcement Learning (by David Silver) Reinforcement Learning (by Michael Littman \u0026amp; Charles Isbell) Reinforcement Learning (by Emma Brunskill) Deep Reinforcement Learning Bootcamp 2017 Foundations of Deep RL (by Pieter Abbeel) Deep Reinforcement Learning \u0026amp; Control (by Katerina Fragkiadaki) Deep Reinforcement Learning (by Sergey Levine) After reviewing all this we can focus on the rest of the paper!\nHidden Maze Experiment One of the key observation of the authors of this paper is that invariance is not enough for zero-shot generalization of reinforcemen learning algorithm. They designed the hidden maze experiment too demonstrate that. Imagine Maze, but with the walls and goal hidden in the observation. Arguably, this is the most task-invariant observation possible, such that a solution can still be obtained in a reasonable time.\nAn agent with memory can be trained to optimally solve all training tasks: figuring out wall positions by trying to move ahead and observing the resulting motion, and identifying based on its movement history in which training maze it is currently in. Obviously, such a strategy will not generalize to test mazes. Performance in Maze, where the strategy for solving any particular training task must be indicative of that task, has largely not improved by methods based on invariance\nThe following figure shows PPO performance on the hidden maze task, indicating severe overfitting.\nAs described by [5], an agent can overcome test-time errors in its policy by treating the perfect policy as an unobserved variable. The resulting decision making problem, termed the epistemic POMDP, may require some exploration at test time to resolve uncertainty. The article further proposed the LEEP algorithm based on this principle, which trains an ensemble of agents and essentially chooses randomly between the members when the ensemble does not agree, and was the first method to present substantial generalization improvement on Maze.\nIn this paper authors extend this idea and asked, How to improve exploration at test time?, and their approach is based on a novel discovery, when they train an agent to explore the training domains using a maximum entropy objective, they observe that the learned exploration behavior generalizes surprisingly well (much better than the generalization attained when training the agent to maximize reward).\nIn the following section we gonna dig deep into internals of maximum entropy policy.\nMaxEnt Policy For simplicity the authors discuss this part for the MDP case. A policy $\\pi$, through its interaction with an MDP, induces a t-step state distribution over the state space $S$,\n$$ d _ {t,\\pi} (s) = p(s_t=s | \\pi) $$\nThe objective of maximum entropy exploration is given by:\n$$ \\mathcal{H}(d(.)) = -\\mathbb{E} _ {s \\sim d} \\left[ \\log{d(s)} \\right] $$\nWhere $d$ can be regarded as either,\nStationary state distribution (infinite horizon): $d _ {\\pi} = \\lim _ {t \\rightarrow \\infty} d _ {t,\\pi} (s)$ Discounted state distribution (infinite horizon): $d _ {\\gamma, \\pi} = (1-\\gamma) \\sum _ {t=0} ^ {\\infty} \\gamma^t d _ {t,\\pi} (s)$ Marginal state distribution (finite horizon): $d _ {T, \\pi} = \\frac{1}{T} \\sum _ {t=0} ^ {T} d _ {t,\\pi} (s)$ In this work they focus on the finite horizon setting and adapt the marginal state distribution $d _ {T, \\pi}$ in which $T$ equals the episode horizon $H$, so we seek to maximize the objective:\n$$ \\begin{align*} \\mathcal{R} _ {\\mathcal{H}} (\\pi) \u0026amp;= \\mathbb{E} _ {M \\sim \\hat{P}(M)} \\left[ \\mathcal{H}(d _ {H,\\pi}) \\right] \\\\ \u0026amp;=\\mathbb{E} _ {M \\sim \\hat{P}(M)} \\left[ \\mathcal{H}(\\frac{1}{H} \\sum _ {t=0} ^ {H} d _ {t,\\pi} (s)) \\right] \\end{align*} $$\nwhich yields a policy that \u0026ldquo;equally\u0026rdquo; visits all states during the episode.\nTo maximize this objective we can estimating the density of the agent\u0026rsquo;s state visitation distribution, but in this paper the authors adapt the non-parametric entropy estimation approach; we estimate the entropy using the particle based k-nearest neighbor (k-NN estimator).\nTo estimate the distribution $d _ {H,\\pi}$ over the states $S$, we consider each trajectory as $H$ samples of states $\\{ s_t \\} _ {t=1} ^ {H}$ and take $s _ t ^ {\\text{k-NN}}$ to be the k-NN of the state $s_t$ within the trajectory,\n$$ \\hat{ \\mathcal{H} } ^ {k,H} (d _ {H,\\pi}) \\approx \\frac{1}{H} \\sum _ {t=1} ^ {H} \\log ( \\parallel s_t - s_t^{\\text{k-NN}} \\parallel _ 2) $$\nIn which we define intrinsic reward function as,\n$$ r_I (s_t) \\coloneqq \\log ( \\parallel s_t - s_t^{\\text{k-NN}} \\parallel _ 2) $$\nThis formulation enables us to deploy any RL algorithm to approximately optimize objective.\nSpecifically, in this work we use the policy gradient algorithm PPOو where at every time step $t$, the state $s_t^{\\text{k-NN}}$ is chosen from previous states $\\{ s_t \\} _ {t=1} ^ {t-1}$ of the same episode. To improve computational efficiency, instead of taking the full observation as the state (64 x 64 RGB image), we sub-sample the observation by applying average pooling of 3 x 3 to produce an image of size 21 x 21.\nWe found that agents trained for maximum entropy exploration exhibit a smaller generalization gap compared with the standard approach of training solely with extrinsic reward. The policies are equipped with a memory unit (GRU) to allow learning of deterministic policies that maximize the entropy.\nIn all three environments, we demonstrate a small generalization gap, as test performance on unseen levels closely follows the performance achieved during training.\nIn addition, we verify that the train results are near optimal by comparing with a hand designed approximately optimal exploration policy. For example, on Maze we use the well known maze exploring strategy wall follower, also known as the left/right-hand rule.\nExpGen Algorithm Our main insight is that, given the generalization property of the entropy maximization policy established above, an agent can apply this behavior in a test MDP and expect effective exploration at test time. We pair this insight with the epistemic POMDP idea, and propose to play the exploration policy when the agent faces epistemic uncertainty, hopefully driving the agent to a different state where the reward-seeking policy is more certain.\nOur framework comprises two parts: an entropy maximizing network and an ensemble of networks that maximize an extrinsic reward to evaluate epistemic uncertainty. The first step entails training a network equipped with a memory unit to obtain a maxEnt policy $\\pi_H$ that maximizes entropy. Next, we train an ensemble of memory-less policy networks $\\{ \\pi _ r ^ j \\} _ {j=1} ^ {m} $ to maximize extrinsic reward.\nHere is the ExpGen algorithm,\nWe consider domains with a finite action space, and say that the policy $\\pi _ r ^ i$ is certain at state $s$ if its action $a_i \\sim \\pi _ r ^ i (a|s)$ is in consensus with the ensemble: $a_i = a_j$ for the majority of $k$ out of $m$, where $k$ is a hyperparameter of our algorithm.\nSwitching between two policies may result in a case where the agent repeatedly toggles between two states (if, say, the maxEnt policy takes the agent from state $s_1$ to a state $s_2$, where the ensemble agrees on an action that again moves to state $s_1$.). To avoid such “meta-stable” behavior, we randomly choose the number of maxEnt steps $n_{\\pi_{\\mathcal{H}}}$ from a Geometric distribution, $n_{\\pi_{\\mathcal{H}}} \\sim Geom(\\alpha)$.\nExperiments Our experimental setup follows ProcGen\u0026rsquo;s easy configuration, wherein agents are trained on 200 levels for 25M steps and subsequently tested on random levels. All agents are implemented using the IMPALA (Importance Weighted Actor-Learner Architectures) convolutional architecture, and trained using PPO or IDAAC. For the maximum entropy agent $\\pi_H$ we incorporate a single GRU at the final embedding of the IMPALA convolutional architecture. For all games, we use the same parameter $\\alpha=0.5$ of the Geometric distribution and form an ensemble of 10 networks.\nFollowing figure is comparison across all ProcGen games, with 95% bootstrap CIs highlighted in color. Score distributions of ExpGen, PPO, PLR, UCB-DrAC, PPG and IDAAC.\nFollowing figure shows in each row, the probability of algorithm X outperforming algorithm Y. The comparison illustrates the superiority of ExpGen over the leading contender IDAAC with probability 0.6, as well as over other methods with even higher probability.\nSee Also PyTorch implementation of ExpGen @ GitHub Ev Zisselman Presentation @ NeurIPS 2023 ExpGen Rebuttal Process @ OpenReview ExpGen Poster for NeurIPS 2023 References [1] Zisselman, Ev, et al. \u0026ldquo;Explore to generalize in zero-shot rl.\u0026rdquo; Advances in Neural Information Processing Systems 36 (2024).\n[2] Sutton, Richard S., and Andrew G. Barto. \u0026ldquo;Reinforcement learning: An introduction.\u0026rdquo; MIT press, (2020).\n[3] Kirk, Robert, et al. \u0026ldquo;A survey of zero-shot generalisation in deep reinforcement learning.\u0026rdquo; Journal of Artificial Intelligence Research 76 (2023): 201-264.\n[4] Raileanu, Roberta, and Rob Fergus. \u0026ldquo;Decoupling value and policy for generalization in reinforcement learning.\u0026rdquo; International Conference on Machine Learning. PMLR, 2021.\n[5] Ghosh, Dibya, et al. \u0026ldquo;Why generalization in rl is difficult: Epistemic pomdps and implicit partial observability.\u0026rdquo; Advances in neural information processing systems 34 (2021): 25502-25515.\n[6] Espeholt, Lasse, et al. \u0026ldquo;Impala: Scalable distributed deep-rl with importance weighted actor-learner architectures.\u0026rdquo; International conference on machine learning. PMLR, 2018.\n","permalink":"http://localhost:1313/posts/explore-to-generalize-in-zero-shot-rl/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eThis paper studies the problem of zero-shot generalization in reinforcement learning and introduces an algorithm that trains an ensamble of maximum reward seeking agents and an maximum entropy agent. At test time, either the ensemble agrees on an action, and we generalize well, or we take exploratory actions with the help of maximum entropy agent and drive us to a novel part of the state space, where the ensemble may potentially agree again. This method combined with an invariance based approach achieves new state-of-the-art results on ProcGen.\u003c/p\u003e","title":"ExpGen: Explore to Generalize in Zero-Shot RL"},{"content":"Welcome to our blog! We are a team of enthusiastic professors and students from Sharif University of Technology who are eager to share our insights and passion for this fascinating field. Our aim is to share our knowledge and excitement about this cutting-edge field with you ❤️.\nProfessors Professor Mohammad Hossein Rohban Professor Ehsaneddin Asgari Students Arash Alikhani Alireza Nobakht Labs RIML Lab NLP \u0026amp; DH Lab We hope you enjoy our blog and find our content both informative and inspiring 🚀!\n","permalink":"http://localhost:1313/us/","summary":"\u003cp\u003eWelcome to our blog! We are a team of enthusiastic professors and students from Sharif University of Technology who are eager to share our insights and passion for this fascinating field. Our aim is to share our knowledge and excitement about this cutting-edge field with you ❤️.\u003c/p\u003e\n\u003ch2 id=\"professors\"\u003eProfessors\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eProfessor \u003ca href=\"https://www.linkedin.com/in/mohammad-hossein-rohban-75567677\"\u003eMohammad Hossein Rohban\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003eProfessor \u003ca href=\"https://www.linkedin.com/in/ehsaneddinasgari\"\u003eEhsaneddin Asgari\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"students\"\u003eStudents\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://www.linkedin.com/in/infinity2357\"\u003eArash Alikhani\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://www.linkedin.com/in/alireza-nobakht\"\u003eAlireza Nobakht\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"labs\"\u003eLabs\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/rohban-lab\"\u003eRIML Lab\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/language-ml\"\u003eNLP \u0026amp; DH Lab\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eWe hope you enjoy our blog and find our content both informative and inspiring 🚀!\u003c/p\u003e","title":""}]