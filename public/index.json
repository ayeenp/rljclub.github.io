[{"content":"Adversarial Attacks on Reinforcement Learning Policies Reinforcement Learning (RL) has powered breakthroughs in games, robotics, and autonomous systems. But just like image classifiers can be fooled by carefully crafted adversarial examples, RL agents are also vulnerable to adversarial attacks. These attacks can manipulate the agent‚Äôs perception of the environment or its decision process, leading to unsafe or suboptimal behavior.\nWhat Do Adversarial Attacks Mean in RL? In RL, an agent interacts with an environment, observes a state, takes an action, and receives a reward. An adversarial attack interferes with this process‚Äîoften by perturbing the input observations‚Äîso the agent chooses actions that look reasonable to it but are actually harmful.\nFor example:\nA self-driving car policy might mistake a stop sign for a speed-limit sign if subtle noise is added to the camera input. A robot trained to walk could be tripped by slight modifications to its sensor readings. Types of Attacks in Reinforcement Learning Researchers have identified several ways adversarial attacks can target RL policies. These attack vectors include:\nObservation Perturbation\nThe most widely studied form of attack: small, carefully crafted changes are made to the agent‚Äôs observations so it perceives the environment incorrectly.\nExample: Adding imperceptible noise to frames in an Atari game so the agent misjudges its next move.\nCommunication Perturbation\nIn multi-agent systems, communication messages can be perturbed, leading to miscoordination.\nExample: Two drones sharing location data receive slightly altered coordinates, causing a collision.\nMalicious Communications\nBeyond perturbations, adversaries may inject entirely fake or deceptive messages.\nExample: An attacker sends a false signal about enemy positions in a cooperative strategy game.\nWhy It Matters Adversarial attacks highlight the gap between high-performing RL policies in controlled benchmarks and their reliability in the real world. Understanding and mitigating these vulnerabilities is essential if we want RL to be trusted in safety-critical domains like autonomous driving, robotics, and healthcare.\nHow Vulnerable Are RL Policies to Adversarial Attacks? Reinforcement Learning (RL) has made huge strides in recent years, powering systems that can beat human champions in games and control robots with precision. But just like image classifiers can be fooled by imperceptible changes to inputs, RL policies are also highly vulnerable to adversarial attacks\nThe researchers show that even tiny perturbations‚Äîso small they are invisible to humans‚Äîcan cause RL agents to fail dramatically at test time. Using Atari games as a testbed, they evaluate three common deep RL algorithms: DQN, TRPO, and A3C. The results are clear: all three are susceptible, but DQN policies are especially vulnerable.\nWhite-Box Attacks with FGSM In white-box scenarios, where the adversary has full access to the policy and gradients, attacks are devastating. The team applies the Fast Gradient Sign Method (FGSM) to craft adversarial examples and finds:\nAn ‚Ñì‚àû-norm perturbation with Œµ = 0.001 can slash performance by over 50%. ‚Ñì1-norm attacks are even more powerful‚Äîby changing just a handful of pixels significantly, they can cripple the agent‚Äôs performance. Even policies trained with robust algorithms like TRPO and A3C experience sharp drops when faced with these attacks.\nBlack-Box Attacks and Transferability What if the adversary doesn‚Äôt have full access to the policy network? Surprisingly, attacks are still effective. In black-box settings, adversaries exploit the property of transferability:\nAdversarial examples created for one policy often transfer to another policy trained on the same task. Transferability also extends across algorithms‚Äîfor example, attacks generated against a DQN policy can still reduce the performance of an A3C policy. Effectiveness decreases with less knowledge, but performance still degrades significantly, especially with ‚Ñì1 attacks. Why This Matters The key takeaway is that adversarial examples are not just a computer vision problem. RL policies‚Äîdespite achieving high scores in training‚Äîcan be undermined by imperceptible perturbations. This fragility is dangerous for real-world applications like autonomous driving and robotics, where safety and reliability are non-negotiable.\nUniversal Adversarial Preservation (UAP) To ground these ideas, I re-implemented the 2017 Adversarial Attacks on Neural Network Policies setup (since there was no offical impelmentation) and trained a DQN agent on Atari Pong. After validating the baseline attacks, I implemented Universal Adversarial Perturbations (UAPs) and found that a single, fixed perturbation‚Äîcomputed once and then applied it to all observations, which was enough to consistently derail the policy across episodes and random seeds, without recomputing noise at every timestep. In other words, the attack generalized over time and trajectories, confirming that UAPs exploit stable perceptual quirks of the learned policy rather than moment-by-moment gradients. Practically, this feels much closer to a real-world threat model: an attacker only needs to tamper with the sensor once (think a sticker/overlay on the lens) instead of having high-bandwidth, per-step access to the system. Below you can see the plot of rewards vs Œµ bugdet and videos of different setups.\nbaselines\u0026rsquo; reward over different values of Œµ budget.\nClean ‚Äî original episode (no perturbation). Random uniform noise (Œµ = 2.0). FGSM (Œµ = 2.0) ‚Äî white-box attack. PGD (Œµ = 2.0) ‚Äî iterative, white-box attack. UAP (Œµ = 2.0) ‚Äî image-agnostic. Adversarial Policies: when weird opponents break strong RL TL;DR. Instead of adding pixel noise to an RL agent‚Äôs input, this paper shows you can train a policy that acts in the shared world to induce natural but adversarial observations for the victim‚Äîcausing robust, self-play‚Äìtrained agents to fail in zero-sum MuJoCo games. Fine-tuning helps‚Ä¶until a new adversary is learned.\nThe threat model: natural observations as the ‚Äúperturbation‚Äù In multi-agent settings, an attacker typically can‚Äôt flip pixels or edit state vectors. But it can choose actions that make the victim see carefully crafted, physically plausible observations. Hold the victim fixed and the two-player game becomes a single-agent MDP for the attacker, who learns a policy that elicits bad actions from the victim.\nQuick look:\nYou Shall Not Pass Kick \u0026amp; Defend Sumo (Human) Masked victim vs adversary Setup in a nutshell Victims: strong self-play policies (‚Äúagent zoo‚Äù) across four MuJoCo tasks: Kick \u0026amp; Defend, You Shall Not Pass, Sumo Humans, Sumo Ants. Attacker: trained with PPO for ~20M timesteps‚Äî\u0026lt; 3% of the 680‚Äì1360M timesteps used for the victims‚Äîyet reliably wins. Key idea: adversaries don‚Äôt become great players; they learn poses/motions that generate adversarial observations for the victim. Figures\nTasks used for evaluation.\nAdversary win rate rises quickly despite far fewer timesteps.\nWhat the learned adversary looks like (and why that matters) In Kick \u0026amp; Defend and YSNP, the adversary may never stand up‚Äîit finds contorted, stable poses that make the victim mis-act. In Sumo Humans, where falling loses immediately, it adopts a kneeling/stable stance that still provokes the victim to fall.\nQualitative behaviors: the ‚Äúpoint‚Äù is to confuse, not to excel at the nominal task.\nMasking test: evidence the attack is observational If wins come from manipulating what the victim sees, then hiding the adversary‚Äôs pose from the victim should help. That‚Äôs exactly what happens:\nAgainst normal opponents, the masked victim is (unsurprisingly) worse. Against the adversary, the masked victim becomes nearly immune (e.g., in YSNP: normal victim loses often; masked victim flips the outcome and wins almost always). Masking the adversary‚Äôs position removes the observation channel the attack exploits.\nDimensionality matters Victims are more vulnerable when more opponent DOFs are observed. The attack is stronger in Humanoid (higher-dimensional observed joints) than Ant (lower-dimensional). More controllable joints ‚Üí more ways to steer the victim off-distribution.\nHigher observed dimensionality correlates with higher adversary win rates.\nWhy it works: off-distribution activations Analyses of the victim‚Äôs network show adversarial opponents push internal activations farther from the training manifold than random or lifeless baselines.\nAdversarial policies drive ‚Äúweird‚Äù activations‚Äîmore off-distribution than simple OOD baselines.\nDefenses (and their limits) Fine-tuning the victim on the discovered adversary reduces that adversary‚Äôs success (often down to ~10% in YSNP), but:\nCatastrophic forgetting: performance vs normal opponents degrades (single-adversary fine-tune is worst; dual fine-tune helps but still regresses). Arms race: re-running the attack against the fine-tuned victim yields a new adversary that succeeds again‚Äîoften via a different failure mode (e.g., tripping rather than pure confusion). Before fine-tune After fine-tune (this adversary) Win-rate grid before/after fine-tuning against normal opponents and adversaries.\nTakeaways for practitioners Threat model upgrade: in multi-agent worlds, your attack surface includes other policies that craft natural observations‚Äîno pixel hacks needed. Exploitability check: training a targeted adversary lower-bounds your policy‚Äôs worst-case performance and reveals failure modes missed by self-play. Defense needs diversity: fine-tuning on a single adversary overfits. Prefer population-based or curriculum defenses that rotate diverse opponents and maintain competence vs normals. Robust Communicative Multi-Agent Reinforcement Learning with Active Defense By Yu et al., AAAI 2024\nüåê Why Communication Matters in Multi-Agent RL In multi-agent reinforcement learning (MARL), agents often face partial observability ‚Äî no single agent sees the full environment. To cooperate effectively, agents need to communicate, sharing information about what they see and what actions to take.\nThis communication has powered applications such as robot navigation and traffic light control.\nBut there‚Äôs a catch: in the real world, communication channels are noisy and vulnerable to adversarial attacks. If attackers tamper with even a few messages, the performance of MARL systems can collapse.\nüõ°Ô∏è Enter Active Defense: The Core Idea Yu et al. propose a new active defense strategy. Instead of blindly trusting all messages, agents:\nJudge the reliability of each incoming message using their own observations and history (hidden states). Adjust the influence of unreliable messages by reducing their weight in the decision process. üëâ Example: If one agent already searched location (1,1) and found nothing, but receives a message saying ‚ÄúTarget at (1,1)‚Äù, it can spot the inconsistency and downweight that message.\nüß© The ADMAC Framework The authors introduce Active Defense Multi-Agent Communication (ADMAC), which has two key components:\nReliability Estimator: A classifier that predicts whether a message is reliable (weight close to 1) or unreliable (weight close to 0). Decomposable Message Aggregation Policy Net: A structure that breaks down the influence of each message into an action preference vector, making it possible to scale its impact up or down. This allows agents to combine their own knowledge with weighted messages to make more robust decisions.\nThe figure above shows how an agent in the ADMAC framework generates its action distribution by combining its own observations with incoming messages from other agents:\nHidden state update: The agent maintains a hidden state (h·µ¢·µó‚Åª¬π), which is updated using the observation (o·µ¢·µó) through the GRU module f_HP. This captures past and current information. Base action preference: From the updated hidden state, the agent generates a base preference vector via f_BP, representing what it would do independently. Message influence: Each received message (m‚ÇÅ·µó, ‚Ä¶, m_N·µó) is processed with the observation through f_MP, producing a message-based action preference vector. Reliability estimation: A reliability estimator f_R evaluates each message, assigning it a weight w·µ¢(m‚±º·µó) that reflects how trustworthy it seems. Aggregation: The agent sums its base vector with all weighted message vectors to form a total action preference vector (v·µ¢·µó). Final decision: Applying a Softmax function converts this vector into a probability distribution over actions, from which the agent selects its next move. By downweighting unreliable messages, ADMAC enables agents to remain robust against malicious communication while still leveraging useful information from peers.\nReference: Yu, L., Qiu, Y., Yao, Q., Shen, Y., Zhang, X., \u0026amp; Wang, J. (2024). Robust Communicative Multi-Agent Reinforcement Learning with Active Defense. AAAI-24.\n","permalink":"http://localhost:1313/posts/adversarial-rl/","summary":"\u003ch1 id=\"adversarial-attacks-on-reinforcement-learning-policies\"\u003eAdversarial Attacks on Reinforcement Learning Policies\u003c/h1\u003e\n\u003cp\u003eReinforcement Learning (RL) has powered breakthroughs in games, robotics, and autonomous systems. But just like image classifiers can be fooled by carefully crafted adversarial examples, RL agents are also vulnerable to \u003cstrong\u003eadversarial attacks\u003c/strong\u003e. These attacks can manipulate the agent‚Äôs perception of the environment or its decision process, leading to unsafe or suboptimal behavior.\u003c/p\u003e\n\u003ch2 id=\"what-do-adversarial-attacks-mean-in-rl\"\u003eWhat Do Adversarial Attacks Mean in RL?\u003c/h2\u003e\n\u003cp\u003eIn RL, an agent interacts with an environment, observes a state, takes an action, and receives a reward. An adversarial attack interferes with this process‚Äîoften by perturbing the input observations‚Äîso the agent chooses actions that look reasonable to it but are actually harmful.\u003c/p\u003e","title":"Adversarial RL"},{"content":"Actor-Critic vs. Value-Based: Empirical Trade-offs Introduction In Reinforcement Learning (RL), one of the fundamental questions is:\nShould we focus on learning the value of states, or should we directly learn the optimal policy?\nBefore diving into this comparison, let‚Äôs briefly recall what RL is: a branch of machine learning in which an agent interacts with an environment to learn how to make decisions that maximize cumulative reward.\nTraditionally, RL algorithms fall into two main families:\nValue-Based methods : which aim to estimate the value of states or state‚Äìaction pairs. Policy-Based methods : which directly optimize a policy that maps states to actions. Actor-Critic algorithms combine the strengths of both worlds, simultaneously learning a value function and an explicit policy. This hybrid structure can often lead to more stable and efficient learning.\nValue-Based and Actor-Critic approaches represent fundamentally different perspectives: one focuses solely on learning state values, while the other integrates both value and policy learning. Comparing these two perspectives helps us better understand the impact of incorporating value or policy components in different environments.\nIn this project, we empirically evaluate these two families in both discrete and continuous action spaces. Four representative algorithms ( DQN, A2C, NAF, and SAC ) were implemented, along with a user-friendly graphical user interface (GUI) for training and evaluation.\nOur ultimate goal is to analyze trade-offs such as convergence speed, learning stability, and final performance across diverse scenarios.\nBackground The reinforcement learning algorithms used in this project fall into two main families:\nValue-Based\nIn this approach, the agent learns only a value function, such as $Q(s, a)$.\nThe policy is derived implicitly by selecting the action with the highest value:\n$\\pi(s) = \\arg\\max_a Q(s,a)$\nThis method is typically simpler, more stable, and computationally efficient.\nHowever, it faces limitations when dealing with continuous action spaces.\nExample algorithms: DQN, NAF.\nValue-Based methods are often well-suited for discrete action spaces with relatively small state‚Äìaction domains, where enumerating or approximating the value for each action is feasible.\nActor-Critic\nIn this framework, the agent consists of two components:\nActor : a parameterized policy that directly produces actions. Critic : a value function that evaluates the Actor‚Äôs performance and guides its updates. This combination can provide greater learning stability, improved performance in complex environments, and high flexibility in continuous action spaces.\nExample algorithms: A2C, SAC.\nActor-Critic methods are generally more suitable for continuous or high-dimensional action spaces, as the Actor can output actions directly without exhaustive value estimation.\nMethodology Project Design This project was designed to perform an empirical comparison between two major families of reinforcement learning algorithms: Value-Based and Actor-Critic.\nFour representative algorithms were selected and implemented in diverse discrete and continuous environments.\nTraining, evaluation, and comparison were carried out through a fully interactive, user-friendly graphical interface.\nImplemented Algorithms Representative algorithms from the two families were selected based on their reported performance in different environments according to the literature.\nFor each algorithm, the training procedure was reproduced in accordance with its original paper.\nThe overall structure of each algorithm is summarized below:\nAlgorithm Family Action Space Description Reference Deep Q-Network (DQN) Value-Based Discrete Uses experience replay and a fixed target network to stabilize learning. 1 Normalized Advantage Function (NAF) Value-Based Continuous Value-based method for continuous spaces using a specific Q-structure to simplify action selection. 2 Advantage Actor-Critic (A2C) Actor-Critic Discrete/Continuous Direct policy optimization guided by an advantage function. 3 Soft Actor-Critic (SAC) Actor-Critic Continuous Off-policy actor-critic method maximizing entropy for stability in complex environments. 4 Deep Q-Network (DQN) The pseudocode of DQN highlights the use of experience replay and a target network, which together reduce correlations between samples and stabilize training.\nNormalized Advantage Function (NAF) NAF handles continuous action spaces by constraining the Q-function into a quadratic form, which makes action selection computationally efficient.\nAdvantage Actor-Critic (A2C) A2C directly optimizes a parameterized policy (Actor) with guidance from the Critic, using advantage estimation to reduce gradient variance and improve learning stability.\nSoft Actor-Critic (SAC) SAC introduces entropy maximization in the objective, encouraging exploration and robustness in complex continuous environments.\nEnvironments There were many 2D and 3D environments available so that we could compare them.\nThe 12 famous environments are listed below:\nFour environments from the Gym library were selected to provide a diverse set of challenges that cover both discrete and continuous action spaces, as well as varying levels of complexity and dynamics:\nMountainCar-v0 (Discrete): A classic control problem where the agent must drive a car up a hill using discrete acceleration commands. This environment tests basic exploration and planning in a low-dimensional, discrete action space. Pendulum-v1 (Continuous): Requires applying continuous torque to keep a pendulum upright. This environment is ideal for evaluating continuous control algorithms and stabilizing dynamics. FrozenLake-v1 (Discrete): A gridworld task where the agent navigates an icy lake to reach a goal while avoiding holes. This environment emphasizes decision-making under uncertainty in a discrete setting. HalfCheetah-v4 (Continuous): A high-dimensional continuous control environment where the agent controls a bipedal cheetah to run efficiently. It challenges advanced continuous control and balance strategies. These environments were chosen to allow a comprehensive comparison of algorithms across different action types, state complexities, and control challenges.\nEnvironment Type Description MountainCar-v0 Discrete Drive a car up a hill by controlling acceleration in a discrete space. Pendulum-v1 Continuous Apply torque to keep a pendulum upright and stable. FrozenLake-v1 Discrete Navigate an icy grid to reach the goal without falling into holes. HalfCheetah-v4 Continuous Control the speed and balance of a simulated bipedal cheetah for fast running. Environment snapshots were recorded during training and appear as GIFs in the Results section.\nConfiguration and Evaluation Algorithms were run with optimized settings for each environment. Key hyperparameters (learning rate, Œ≥, batch size, buffer size) were tuned through trial-and-error, leveraging existing GitHub implementations for optimal performance. Comparisons were based on convergence speed, training stability, and final performance. Example configurations:\nAlgorithm Environment Œ≥ Learning Rate Batch Size Buffer Size DQN FrozenLake 0.93 6e-4 32 4,000 A2C MountainCar 0.96 1e-3 ‚Äì ‚Äì NAF Pendulum 0.99 3e-4 64 400,000 SAC HalfCheetah 0.99 3e-4 256 1,000,000 Graphical User Interface (GUI) A user-friendly GUI was developed to simplify training and comparing algorithms, enabling full project execution without direct coding.The code is available at: This Github Link The project structure is as follows:\nProject_Code/\r‚îú‚îÄ‚îÄ plots/\r‚îÇ ‚îî‚îÄ‚îÄ learning_curves/\r‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ Continuous/\r‚îÇ ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ Pendulum-v1/\r‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ SAC/\r‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ NAF/\r‚îÇ ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ HalfCheetah-v4/\r‚îÇ ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ SAC/\r‚îÇ ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ NAF/\r‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ Discrete/\r‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ FrozenLake-v1/\r‚îÇ ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ DQN/\r‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ 4x4\r‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ 8x8\r‚îÇ ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ A2C/\r‚îÇ ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ 4x4\r‚îÇ ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ 8x8\r‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ MountainCar-v0/\r‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ DQN/\r‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ A2C/\r‚îÇ ‚îú‚îÄ‚îÄ comparison_table.png\r‚îÇ ‚îî‚îÄ‚îÄ directory_structure.png\r‚îú‚îÄ‚îÄ requirements/\r‚îÇ ‚îú‚îÄ‚îÄ base.txt\r‚îÇ ‚îú‚îÄ‚îÄ dev.txt\r‚îÇ ‚îú‚îÄ‚îÄ env.txt\r‚îÇ ‚îî‚îÄ‚îÄ config.py\r‚îú‚îÄ‚îÄ src/\r‚îÇ ‚îú‚îÄ‚îÄ agents/\r‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ a2c.py\r‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ dqn.py\r‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ naf.py\r‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ sac.py\r‚îÇ ‚îú‚îÄ‚îÄ envs/\r‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ continuous_envs.py\r‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ discrete_envs.py\r‚îÇ ‚îú‚îÄ‚îÄ main.py\r‚îÇ ‚îú‚îÄ‚îÄ train.py\r‚îÇ ‚îî‚îÄ‚îÄ utils.py\r‚îú‚îÄ‚îÄ videos/\r‚îÇ ‚îú‚îÄ‚îÄ Continuous/\r‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ Pendulum-v1/\r‚îÇ ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ SAC/\r‚îÇ ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ NAF/\r‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ HalfCheetah-v4/\r‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ SAC/\r‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ NAF/\r‚îÇ ‚îî‚îÄ‚îÄ Discrete/\r‚îÇ ‚îú‚îÄ‚îÄ FrozenLake-v1/\r‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ DQN/\r‚îÇ ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ 4x4\r‚îÇ ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ 8x8\r‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ A2C/\r‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ 4x4\r‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ 8x8\r‚îÇ ‚îî‚îÄ‚îÄ MountainCar-v0/\r‚îÇ ‚îú‚îÄ‚îÄ DQN/\r‚îÇ ‚îî‚îÄ‚îÄ A2C/\r‚îú‚îÄ‚îÄ models/\r‚îÇ ‚îú‚îÄ‚îÄ Continuous/\r‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ Pendulum-v1/\r‚îÇ ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ SAC/\r‚îÇ ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ NAF/\r‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ HalfCheetah-v4/\r‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ SAC/\r‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ NAF/\r‚îÇ ‚îî‚îÄ‚îÄ Discrete/\r‚îÇ ‚îú‚îÄ‚îÄ FrozenLake-v1/\r‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ DQN/\r‚îÇ ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ 4x4\r‚îÇ ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ 8x8\r‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ A2C/\r‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ 4x4\r‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ 8x8\r‚îÇ ‚îî‚îÄ‚îÄ MountainCar-v0/\r‚îÇ ‚îú‚îÄ‚îÄ DQN/\r‚îÇ ‚îî‚îÄ‚îÄ A2C/\r‚îî‚îÄ‚îÄ README.md Main features:\nSelect algorithm, environment, action space type (discrete/continuous), and execution mode (train/test) with just a few clicks. Launch training with a Run button, and view results via Show Plots and Show Videos. Compare algorithms interactively using a dedicated Compare Algorithms window. Display key settings such as hyperparameters and project structure. Real-time console output for monitoring execution status and system messages. Interactive Mobile Application In addition to the desktop GUI, a mobile application for Android and iOS has been developed to provide interactive access to the project. By simply scanning the poster, users can explore various features, including:\nViewing videos of agent executions in different environments. Opening the project‚Äôs website for additional resources and documentation. Displaying the poster digitally for interactive exploration. Comparing learning curves and results across different algorithms. The images below showcase some sections of the app interface:\nResults This section presents the empirical evaluation of four reinforcement learning algorithms (DQN, NAF, A2C, and SAC) from the Value-Based and Actor-Critic families, across both discrete and continuous action space environments. The performance is analyzed based on final reward, convergence speed, and training stability, supported by quantitative metrics, qualitative visualizations (GIFs), and learning curves with a moving average (MA) applied to reduce noise. The experiments were conducted using the Gymnasium library, with optimized hyperparameters (see Methodology for details) and a fixed random seed for reproducibility. Training was performed on a single NVIDIA RTX 4050 GPU, with average runtimes of 1‚Äì4 hours per algorithm-environment pair.\nDiscrete Environments The discrete action space environments tested were FrozenLake-v1 (8x8 grid) and MountainCar-v0, which challenge the algorithms with stochastic transitions and sparse rewards, respectively. The table below summarizes the performance, followed by detailed analyses and visualizations.\nEnvironment Algorithm Final Reward (Avg ¬± Std) Convergence Speed (Episodes to 90% of Max) Stability (Std of Reward) FrozenLake-v1 DQN 0.98 ¬± 0.14 ~1,475 0.50 A2C 1.00 ¬± 0.00 ~1,209 0.48 MountainCar-v0 DQN -22.21 ¬± 79.32 ~2,000 81.50 A2C -27.87 ¬± 62.35 ~2,000 40.83 FrozenLake-v1 In FrozenLake-v1, a stochastic gridworld, A2C outperformed DQN in final reward (1.00 vs. 0.98 success rate) and converged faster (~1209 vs. ~1475 episodes to reach 90% of max reward). A2C‚Äôs advantage estimation provided greater stability, as evidenced by its lower standard deviation (0.48 vs. 0.50). The GIF below illustrates agent behaviors, showing A2C‚Äôs smoother navigation to the goal compared to DQN‚Äôs occasional missteps.\nReward comparison in FrozenLake-v1: A2C converges quickly and maintains stable performance compared to DQN.\nMountainCar-v0 In MountainCar-v0, a deterministic environment with sparse rewards, DQN achieved a better final reward (-22.21 vs. -27.87 timesteps to goal) but both algorithms converged at similar speeds (~2000 episodes). However, A2C exhibited greater stability (std of 40.83 vs. 81.50), avoiding large oscillations in learning. The GIF below shows DQN‚Äôs quicker ascent to the hilltop, while A2C maintains more consistent swings.\nReward comparison in MountainCar-v0: DQN converges faster and reaches higher rewards, while A2C shows greater stability.\nContinuous Environments The continuous action space environments tested were Pendulum-v1 and HalfCheetah-v4, which require precise control and balance in low- and high-dimensional settings, respectively. The table below summarizes the performance.\nEnvironment Algorithm Final Reward (Avg ¬± Std) Convergence Speed (Episodes to 90% of Max) Stability (Std of Reward) Pendulum-v1 NAF -141.17 ¬± 85.58 ~246 199.46 SAC 287.66 ¬± 62.38 ~152 113.23 HalfCheetah-v4 NAF 3,693.35 ¬± 575.60 ~862 1,077.01 SAC 10,247.42 ¬± 584.31 ~1,127 2,493.55 Pendulum-v1 In Pendulum-v1, SAC significantly outperformed NAF in final reward (287.66 vs. -141.17, higher is better) and converged faster (~152 vs. ~246 episodes). SAC‚Äôs entropy maximization ensured smoother learning, with a standard deviation of 113.23 compared to NAF‚Äôs 199.46. The GIF below highlights SAC‚Äôs ability to stabilize the pendulum upright, while NAF struggles with inconsistent torque.\nReward comparison in Pendulum-v1: SAC achieves higher rewards with smoother convergence compared to NAF.\nHalfCheetah-v4 In HalfCheetah-v4, a high-dimensional control task, SAC achieved a much higher final reward (10247.42 vs. 3693.35) but converged slightly slower (~1127 vs. ~862 episodes). SAC‚Äôs stability (std of 2493.55 vs. 1077.01) reflects its robustness in complex dynamics, though NAF shows lower variance. The GIF below shows SAC‚Äôs fluid running motion compared to NAF‚Äôs less coordinated movements.\nReward comparison in HalfCheetah-v4: SAC consistently outperforms NAF in both speed and stability.\nDiscussion \u0026amp; Conclusion This project examined the performance differences between Value-Based and Actor-Critic algorithms in both discrete and continuous environments.\nThe experimental results indicate that no single algorithm is universally superior; rather, the environment characteristics and action space type play a decisive role in determining performance.\nAnalysis \u0026amp; Interpretation Continuous Action Spaces SAC consistently outperformed NAF in both Pendulum-v1 and HalfCheetah-v4, thanks to its entropy maximization strategy, which promotes exploration and robustness. NAF‚Äôs fixed quadratic Q-function structure limited its flexibility in high-dimensional or complex tasks, leading to slower convergence and higher variance in rewards. SAC‚Äôs ability to directly optimize a stochastic policy made it particularly effective in continuous control scenarios.\nDiscrete Action Spaces In FrozenLake-v1, a stochastic environment, A2C‚Äôs stability (due to advantage estimation) gave it an edge over DQN, achieving higher success rates and faster convergence. In MountainCar-v0, a deterministic environment with a small action space, DQN‚Äôs value-based approach excelled in final reward and convergence speed, though A2C remained more stable. This highlights the suitability of Value-Based methods for simpler, deterministic settings and Actor-Critic methods for stochastic or complex environments.\nKey Findings: In simple discrete environments, such as FrozenLake, Value-Based algorithms (e.g., DQN) achieved competitive performance, but Actor-Critic algorithms (e.g., A2C) showed faster convergence and more stable learning. In continuous and more complex environments, Actor-Critic algorithms ‚Äî particularly SAC ‚Äî outperformed their Value-Based counterparts in terms of final reward and convergence speed. Observed Trade-offs: Aspect Value-Based Actor-Critic Simplicity of implementation Yes More complex Initial learning speed High in simple environments Depends on tuning Training stability More oscillations More stable Suitability for continuous spaces Not always Yes Overall, the choice between Value-Based and Actor-Critic methods should be guided by the nature of the task, the complexity of the environment, and the available computational budget.\nObservations Based on Environment Characteristics Our experimental results further reveal that the nature of the environment‚Äîin terms of action space, state space, and reward structure‚Äîsignificantly impacts algorithm performance:\nAction Space (Discrete vs. Continuous) Discrete Action Spaces: Value-Based algorithms like DQN tend to perform competitively, especially in small and low-dimensional discrete action spaces. They converge quickly and reliably, but may struggle when the action space grows larger or stochasticity increases. Actor-Critic methods such as A2C can still provide improved stability in these scenarios, especially under stochastic transitions. Continuous Action Spaces: Actor-Critic methods (e.g. SAC) dominate due to their ability to output continuous actions directly. Value-Based methods require specialized approximations (like NAF), which often limit flexibility and performance in high-dimensional or continuous control tasks. State Space (Low-dimensional vs. High-dimensional) Low-dimensional states (e.g., MountainCar, FrozenLake) generally favor Value-Based methods, which can efficiently enumerate or approximate Q-values. High-dimensional states (e.g., HalfCheetah) require the policy network of Actor-Critic methods to generalize across large state spaces. These methods better handle complex dynamics and correlations among state variables. Reward Structure (Sparse vs. Dense) Sparse Reward Environments (e.g., FrozenLake) challenge Value-Based methods to propagate value signals efficiently, potentially slowing convergence. Actor-Critic algorithms can leverage advantage estimation and policy gradients to maintain learning stability even with sparse rewards. Dense Reward Environments (e.g., HalfCheetah, Pendulum) allow both families to learn effectively, but Actor-Critic methods often achieve smoother and faster convergence due to direct policy optimization combined with value guidance. The interplay between action space type, state space complexity, and reward sparsity fundamentally shapes the suitability of each algorithm. In general:\nDiscrete + Low-dimensional + Dense reward ‚Üí Value-Based methods are competitive. Continuous + High-dimensional + Sparse or Dense reward ‚Üí Actor-Critic methods provide superior learning stability and higher final performance. These insights complement the empirical trade-offs already observed in our study, providing a more nuanced understanding of when and why certain RL algorithms excel under different environment characteristics.\nLimitations and Future Work Limitations The present project, which evaluates the performance of reinforcement learning algorithms (DQN, A2C, NAF, and SAC) in discrete (FrozenLake-v1 and MountainCar-v0) and continuous (Pendulum-v1 and HalfCheetah-v4) environments, provides valuable insights but is subject to several limitations, outlined below:\nLimited Number of Environments:\nThe study only examines four specific environments, which may not provide sufficient diversity to generalize results across all types of reinforcement learning environments. More complex environments with larger state or action spaces or different dynamics could yield different outcomes. Lack of Random Seed Variation:\nThe reported results are based on a single run or an average of a limited number of runs. Conducting multiple experiments with different random seeds could better demonstrate the robustness and reliability of the results. Focus on Specific Metrics:\nThe evaluation metrics (final reward average, convergence speed, and stability) cover only certain aspects of algorithm performance. Other metrics, such as computational efficiency, training time, or robustness to environmental noise, were not assessed. Future Work To build upon the findings of this project, which evaluated the performance of reinforcement learning algorithms (DQN, A2C, NAF, and SAC) in discrete (FrozenLake-v1, MountainCar-v0) and continuous (Pendulum-v1, HalfCheetah-v4) environments, several directions for future research and development can be pursued to address the limitations and extend the scope of the study:\nBroader Range of Environments:\nFuture work could include testing the algorithms on a wider variety of environments, such as those with larger state and action spaces, partially observable states (e.g., POMDPs), or real-world-inspired tasks. This would help validate the generalizability of the observed performance trends. Incorporating Random Seed Variations:\nConducting multiple runs with different random seeds would improve the robustness of results and allow for statistical analysis of performance variability, ensuring that conclusions are not biased by specific initial conditions. Evaluation of Additional Metrics:\nFuture studies could incorporate metrics such as computational efficiency, memory usage, training time, and robustness to environmental perturbations (e.g., noise or dynamic changes). This would provide a more holistic view of algorithm suitability for practical applications. Real-World Application: Robotics The insights gained from this project have significant potential for real-world applications, particularly in robotics, where reinforcement learning can enable autonomous systems to perform complex tasks. The following outlines how the evaluated algorithms could be applied and extended in robotics contexts:\nRobotic Manipulation:\nAlgorithms like SAC, which performed well in continuous control tasks (e.g., Pendulum-v1, HalfCheetah-v4), could be applied to robotic arms for tasks such as grasping, object manipulation, or assembly. SAC‚Äôs ability to handle continuous action spaces makes it suitable for precise control in high-dimensional settings. Autonomous Navigation:\nDiscrete action space algorithms like DQN and A2C, tested in environments like FrozenLake-v1, could be adapted for robot navigation in grid-like or structured environments (e.g., warehouse robots). A2C‚Äôs stability in stochastic settings could be particularly useful for navigating dynamic or uncertain environments. Locomotion and Mobility:\nThe success of SAC in HalfCheetah-v4 suggests its potential for controlling legged robots or humanoid robots for locomotion tasks. Future work could involve applying SAC to real-world robotic platforms to achieve robust and efficient walking or running behaviors. These future research directions and real-world applications highlight the potential to extend the current study‚Äôs findings to more diverse and practical scenarios. By addressing the identified limitations and applying the algorithms to robotics, this work can contribute to the development of more robust, efficient, and adaptable autonomous systems.\nReferences A2C (Advantage Actor-Critic):\nMnih, V., et al. (2016). \u0026ldquo;Asynchronous Methods for Deep Reinforcement Learning.\u0026rdquo; ICML 2016. Paper Stable-Baselines3 A2C Implementation: GitHub SAC (Soft Actor-Critic):\nHaarnoja, T., et al. (2018). \u0026ldquo;Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor.\u0026rdquo; ICML 2018. Paper Stable-Baselines3 SAC Implementation: GitHub DQN (Deep Q-Network):\nMnih, V., et al. (2015). \u0026ldquo;Human-level control through deep reinforcement learning.\u0026rdquo; Nature, 518(7540), 529-533. Paper Stable-Baselines3 DQN Implementation: GitHub NAF (Normalized Advantage Function):\nGu, S., et al. (2016). \u0026ldquo;Continuous Deep Q-Learning with Model-based Acceleration.\u0026rdquo; ICML 2016. Paper Gymnasium:\nOfficial Gymnasium Documentation: Gymnasium ","permalink":"http://localhost:1313/posts/actor-critic-vs-value-based-empirical-trade-offs/","summary":"This study compares Value-Based (DQN, NAF) and Actor-Critic (A2C, SAC) reinforcement learning algorithms in diverse environments (MountainCar-v0, Pendulum-v1, FrozenLake-v1, HalfCheetah-v4). Through empirical evaluation, we analyze trade-offs in convergence speed, learning stability, and final performance, supported by a user-friendly GUI and mobile application for interactive training and visualization.","title":"Actor-Critic vs. Value-Based: Empirical Trade-offs"},{"content":"Introduction This paper critically examines the assumptions commonly accepted in modeling reinforcement learning problems, suggesting that these assumptions may impede progress in the field. The authors refer to these assumptions as \u0026ldquo;dogmas.\u0026rdquo;\nDogma: A fixed, especially religious, belief or set of beliefs that people are expected to accept without any doubts. ‚ÄîFrom the Cambridge Advanced Learner\u0026rsquo;s Dictionary \u0026amp; Thesaurus\nThe paper introduces three central dogmas:\nThe Environment Spotlight Learning as Finding a Solution The Reward Hypothesis (although not exactly a dogma) The author argues the true reinforcement learning landscape is actualy like this,\nIn the words of Rich Sutton:\nRL can be viewed as a microcosm of the whole AI problem.\nHowever, today\u0026rsquo;s RL landscape is overly simplified,\nThese three dogmas are responsible for narrowing the potential of RL,\nThe authors propose that we consider moving beyond these dogmas,\nTo reclaim the true landscape of RL,\nBackground The authors reference Thomas Kuhn\u0026rsquo;s book, \u0026ldquo;The Structure of Scientific Revolutions\u0026rdquo;,\nKuhn distinguishes between two phases of scientific activity,\nNormal Science: Resembling puzzle-solving. Revolutionary Phase: Involving a fundamental rethinking of the values, methods, and commitments of science, which Kuhn calls a \u0026ldquo;paradigm.\u0026rdquo; Here\u0026rsquo;s an example of a previous paradigm shift in science:\nThe authors explore the paradigm shift needed in RL:\nDogma One: The Environment Spotlight The first dogma we call the environment spotlight, which refers to our collective focus on modeling environments and environment-centric concepts rather than agents.\nWhat do we mean when we say that we focus on environments? We suggest that it is easy to answer only one of the following two questions:\nWhat is at least one canonical mathematical model of an environment in RL?\nMDP and its variants! And we define everything in terms of it. By embracing the MDP, we are allowed to import a variety of fundamental results and algorithms that define much of our primary research objectives and pathways. For example, we know every MDP has at least one deterministic, optimal, stationary policy, and that dynamic programming can be used to identify this policy. What is at least one canonical mathematical model of an agent in RL?\nIn contrast, this question has no clear answer! The author suggests it is important to define, model, and analyse agents in addition to environments. We should build toward a canonical mathematical model of an agent that can open us to the possibility of discovering general laws governing agents (if they exist).\nDogma Two: Learning as Finding a Solution The second dogma is embedded in the way we treat the concept of learning. We tend to view learning as a finite process involving the search for‚Äîand eventual discovery of‚Äîa solution to a given task.\nWe tend to implicitly assume that the learning agents we design will eventually find a solution to the task at hand, at which point learning can cease. Such agents can be understood as searching through a space of representable functions that captures the possible action-selection strategies available to an agent, similar to the Problem Space Hypothesis, and, critically, this space contains at least one function‚Äîsuch as the optimal policy of an MDP‚Äîthat is of sufficient quality to consider the task of interested solved. Often, we are then interested in designing learning agents that are guaranteed to converge to such an endpoint, at which point the agent can stop its search (and thus, stop its learning).\nThe author suggests to embrace the view that learning can also be treated as adaptation. As a consequence, our focus will drift away from optimality and toward a version of the RL problem in which agents continually improve, rather than focus on agents that are trying to solve a specific problem.\nWhen we move away from optimality,\nHow do we think about evaluation? How, precisely, can we define this form of learning, and differentiate it from others? What are the basic algorithmic building blocks that carry out this form of learning, and how are they different from the algorithms we use today? Do our standard analysis tools such as regret and sample complexity still apply? These questions are important, and require reorienting around this alternate view of learning.\nThe authors introduce the book \u0026ldquo;Finite and Infinite Games\u0026rdquo;,\nAnd the concept of Finite and Infinite Games is summarized in the following quote,\nThere are at least two kinds of games, One could be called finite; the other infinite. A finite game is played for the purpose of winning, an infinite game for the purpose of continuing the play.\nAnd argues alignment is an infinite game.\nDogma Three: The Reward Hypothesis The third dogma is the reward hypothesis, which states \u0026ldquo;All of what we mean by goals and purposes can be well thought of as maximization of the expected value of the cumulative sum of a received scalar signal (reward).\u0026rdquo;\nThe authors argue that the reward hypothesis is not truly a dogma. Nevertheless, it is crucial to understand its nuances as we continue to design intelligent agents.\nThe reward hypothesis basically says,\nIn recent analysis by [2] fully characterizes the implicit conditions required for the hypothesis to be true. These conditions come in two forms. First, [2] provide a pair of interpretative assumptions that clarify what it would mean for the reward hypothesis to be true or false‚Äîroughly, these amount to saying two things (brwon doors).\nFirst, that \u0026ldquo;goals and purposes\u0026rdquo; can be understood in terms of a preference relation on possible outcomes. Second, that a reward function captures these preferences if the ordering over agents induced by value functions matches that of the ordering induced by preference on agent outcomes. This leads to the following conjecture,\nThen, under this interpretation, a Markov reward function exists to capture a preference relation if and only if the preference relation satisfies the four von Neumann-Morgenstern axioms, and a fifth Bowling et al. call $\\gamma$-Temporal Indifference.\nAxiom 1: Completeness \u0026gt; You have a preference between every outcome pair.\nYou can always compare any two choices. Axiom 2: Transitivity \u0026gt; No preference cycles.\nIf you like chocolate more than vanilla, and vanilla more than strawberry, you must like chocolate more than strawberry. Axiom 3: Independence \u0026gt; Independent alternatives can\u0026rsquo;t change your preference.\nIf you like pizza more than salad, and you have to choose between a lottery of pizza or ice cream and a lottery of salad or ice cream, you should still prefer the pizza lottery over the salad lottery. Axiom 4: Continuity \u0026gt; There is always a break even chance.\nImagine you like a 100 dollar bill more than a 50 dollar bill, and a 50 dollar bill more than a 1 dollar bill. There should be a scenario where getting a chance at 100 dollar and 1 dollar, with certain probabilities, is equally good as getting the 50 dollar for sure. These 4 axioms are called the von Neumann-Morgenstern axioms.\nAxiom 5: Temporal $\\boldsymbol{\\gamma}$-Indifference \u0026gt; Discounting is consistent throughout time. Temporal $\\gamma$-indifference says that if you are indifferent between receiving a reward at time $t$ and receiving the same reward at time $t+1$, then your preference should not change if we move both time points by the same amount. For instance, if you don\u0026rsquo;t care whether you get a candy today or tomorrow, then you should also not care whether you get the candy next week or the week after. Taking these axioms into account, the reward conjecture becomes the reward theorem,\nIt is essential to consider that people do not always conform to these axioms, and human preferences can vary.\nIt is important that we are aware of the implicit restrictions we are placing on the viable goals and purposes under consideration when we represent a goal or purpose through a reward signal. We should become familiar with the requirements imposed by the five axioms, and be aware of what specifically we might be giving up when we choose to write down a reward function.\nSee Also David Abel Presentation @ ICML 2023 David Abel Personal Website Mark Ho Personal Website Anna Harutyunyan Personal Website References [1] Abel, David, Mark K. Ho, and Anna Harutyunyan. \u0026ldquo;Three Dogmas of Reinforcement Learning.\u0026rdquo; arXiv preprint arXiv:2407.10583 (2024).\n[2] Bowling, Michael, et al. \u0026ldquo;Settling the reward hypothesis.\u0026rdquo; International Conference on Machine Learning. PMLR, 2023.\n","permalink":"http://localhost:1313/posts/three-dogmas-of-reinforcement-learning/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eThis paper critically examines the assumptions commonly accepted in modeling reinforcement learning problems, suggesting that these assumptions may impede progress in the field. The authors refer to these assumptions as \u0026ldquo;dogmas.\u0026rdquo;\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003eDogma: A fixed, especially religious, belief or set of beliefs that people are expected to accept without any doubts.\u003c/strong\u003e \u003cem\u003e‚ÄîFrom the \u003ca href=\"https://dictionary.cambridge.org/dictionary/english\"\u003eCambridge Advanced Learner\u0026rsquo;s Dictionary \u0026amp; Thesaurus\u003c/a\u003e\u003c/em\u003e\u003c/p\u003e\u003c/blockquote\u003e\n\u003cp\u003eThe paper introduces three central dogmas:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eThe Environment Spotlight\u003c/li\u003e\n\u003cli\u003eLearning as Finding a Solution\u003c/li\u003e\n\u003cli\u003eThe Reward Hypothesis (although not exactly a dogma)\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eThe author argues the true reinforcement learning landscape is actualy like this,\u003c/p\u003e","title":"Three Dogmas of Reinforcement Learning"},{"content":"Introduction This paper studies the problem of zero-shot generalization in reinforcement learning and introduces an algorithm that trains an ensamble of maximum reward seeking agents and an maximum entropy agent. At test time, either the ensemble agrees on an action, and we generalize well, or we take exploratory actions with the help of maximum entropy agent and drive us to a novel part of the state space, where the ensemble may potentially agree again. This method combined with an invariance based approach achieves new state-of-the-art results on ProcGen.\nBackground I know it\u0026rsquo;s a lot to take in! You may be wondering:\nWhat is reinforcement learning? ü•∫ What generalization means for RL? ü•≤ What is zero-shot generalization? ü•π What are max reward and max entropy agents?! ‚òπÔ∏è What is an ensamble of them?!! üòü What is an invariance based approach? üòì And what the heck is ProcGen?!!! üò† Don\u0026rsquo;t worry! We are going to cover all of that and more! And you are going to fully understand this paper and finish reading this article with an smile üôÇ!\nWhat is reinforcement learning? Reinforcement learning is learning what to do‚Äîhow to map situations to actions‚Äîso as to maximize a numerical reward signal. Imagine yourself right now, in reinforcement learning terms, you are an agent and everything that is not you, is your environment. You perceive the world through your senses (e.g., eyes, ears, etc.) and what you perceive turns into electrical signals that your brain processes to form an understanding of your surroundings (state). Based on this understanding, you make decisions (actions) with the goal of achieving the best possible outcome for yourself (reward).\nIn a more formal sense, reinforcement learning involves the following components:\nAgent: The learner or decision maker (e.g., you). Environment: Everything the agent interacts with (e.g., the world around you). State ($s$): A representation of the current situation of the agent within the environment (e.g., what you see, hear, and feel at any given moment). Actions ($a$): The set of all possible moves the agent can make (e.g., moving your hand, walking, speaking). Reward ($r$): The feedback received from the environment in response to the agent‚Äôs action (e.g., pleasure from eating food, pain from touching a hot surface). Policy ($\\pi$): A strategy used by the agent to decide which actions to take based on the current state (e.g., your habits and decision-making processes). Value Function ($V$): A function that estimates the expected cumulative reward of being in a certain state and following a particular policy (e.g., your prediction of future happiness based on current actions). The objective of the agent is to develop a policy that maximizes the total cumulative reward over time. This is typically achieved through a process of exploration (trying out new actions to discover their effects) and exploitation (using known actions that yield high rewards).\nIn mathematical terms, the goal is to find a policy $\\pi$ that maximizes the expected return $G_t$, which is the cumulative sum of discounted rewards:\n$$ G_t = r_{t+1} + \\gamma r_{t+2} + \\gamma^2 r_{t+3} + \\cdots $$\nwhere $\\gamma$ (0 ‚â§ $\\gamma$ \u0026lt; 1) is the discount factor that determines the importance of future rewards.\nFor a more thorough and in depth explanation of reinforcement learning please refer to Lilian Weng excelent blog post A (Long) Peek into Reinforcement Learning.\nWhat generalization means for RL? In reinforcement learning, generalization involves an agent\u0026rsquo;s ability to apply learned policies or value functions to new states or environments that it has not encountered during training. This is essential because it is often impractical or impossible to train an agent on every possible state it might encounter.\nThere are bunch of methods that researchers have used to tackle the problem of generalization in reinforcement learning that are summarized in the following diagram.\nThis paper focuses on RL-Specific solutions and specifically exploration technique to tackle the problem of generalization. If you\u0026rsquo;re interested to learn more about the generalization problem in reinforcement learning please refer to reference [3].\nWhat is zero-shot generalization? Zero-shot generalization in RL refers to the ability of an agent to perform well in entirely new environments or tasks without any prior specific training or fine-tuning on those environments or tasks. This is a significant challenge because it requires the agent to leverage its learned representations and policies in a highly flexible and adaptive manner.\nIn order to define the objective of zero-shot generalization we first have to define what MDPs and POMDPs are.\nMarkov Decision Process (MDP):\nMDP is a mathematical framework used to describe an environment in reinforcement learning where the outcome is partly random and partly under the control of a decision-maker (agent). A MDP is a tuple $M = (S, A, P_{init}, P, r, \\gamma)$ where,\nStates ($S \\in \\mathbb{R}^{|S|}$): A finite set of states that describe all possible situations in which the agent can be. Actions ($A \\in \\mathbb{R}^{|A|}$): A finite set of actions available to the agent. Initial State Distribution ($P_{init}$): A distribution of starting state $(s_0 \\sim P_{init})$. Transition Probability ($P$): A function $P(s_{t+1}, s_t, a_t)$ representing the probability of transitioning from state $s_t$ to state $s_{t+1}$ after taking action $a_t$ $(s_{t+1} \\sim P(.|s_t,a_t))$. Reward ($r: S \\times A \\rightarrow \\mathbb{R}$): A function $r(s_t, a_t)$ representing the immediate reward $r_t$ received after transitioning from state $s_t$ due to action $a_t$ $(r_t = r(s_t, a_t))$. Discount Factor ($\\gamma$): $(0 \\leq \\gamma \u0026lt; 1)$ is a constant that determines the importance of future rewards. Partially Observable Markov Decision Process (POMDP):\nPOMDP extends MDPs to situations where the agent does not have complete information about the current state. Instead, the agent must make decisions based on partial observations. A POMDP is a tuple $M = (S, A, O, P_{init}, P, \\Sigma, r, \\gamma)$ where other that above definitions for MDP,\nObservation Space ($O$): A finite set of observations the agent can receive about the state. Observation Function ($\\Sigma$): A function that given current state $s_t$ and current action $a_t$ gives us the current observation $o_t$ $(o_t = \\Sigma(s_t, a_t) \\in O)$. If we set $O = S$ and $\\Sigma(s,a) = s$, the POMDP turns into regular MDP.\nLet the history at time $t$ be,\n$$ h_t = \\{ o_0, a_0, r_0, o_1, a_1, r_1, \\dots, o_t \\} $$\nThe agent‚Äôs next action is outlined by a policy $\\pi$, which is a stochastic mapping from the history to an action probability,\n$$ \\pi(a|h_t) = P(a_t=a|h_t) $$\nIn this formulation, a history-dependent policy (and not a Markov policy) is required both due to partially observed states, epistemic uncertainty, and also for optimal maxEnt exploration.\nWe assume a prior distribution over POMDPs $P(M)$, defined over some space of POMDPs. For a given POMDP, an optimal policy maximizes the expected discounted return,\n$$ \\mathbb{E}_{\\pi,M} \\left[ \\sum _{t=0}^{\\infty} \\gamma^t r(s_t,a_t) \\right] $$\nWhere the expectation is taken over the policy $\\pi(h_t)$, and the state transition probability $s_t \\sim P$ of POMDP $M$.\nOur generalization objective is to maximize the discounted cumulative reward taken in expectation over the POMDP prior,\n$$ \\mathcal{R}_ {pop}(\\pi) = \\mathbb{E}_ {M \\sim P(M)} {\\left[ \\mathbb{E}_{\\pi,M} {\\left[ \\sum _{t=0}^{\\infty} \\gamma^t r(s_t,a_t) \\right]} \\right]} $$\nSeeking a policy that performs well in expectation over any POMDP from the prior corresponds to zero-shot generalization.\nAnd as you may have guessed we don\u0026rsquo;t have access to true prior distribution of POMDPs so we have to estimate it with $N$ training POMDPs $M_1, M_2, \\dots, M_N$ sampled from the true prior distribution $P(M)$. So we are going to maximize empirical discounted cumulative reward,\n$$ \\begin{align*} \\mathcal{R}_ {emp}(\\pi) \u0026amp;= \\frac{1}{N} \\sum _ {i=1}^{N} \\mathbb{E} _ {\\pi,M_i} \\left[ \\sum _ {t=0}^{\\infty} \\gamma^t r(s_t,a_t) \\right] \\\\ \u0026amp;= \\mathbb{E}_ {M \\sim \\hat{P}(M)} {\\left[ \\mathbb{E}_{\\pi,M} {\\left[ \\sum _{t=0}^{\\infty} \\gamma^t r(s_t,a_t) \\right]} \\right]} \\end{align*} $$\nWhere the empirical POMDP distribution $\\hat{P}(M)$ can be different from the true distribution, i.e. $\\hat{P}(M) \\neq P(M)$. In general, a policy that optimizes the empirical reward may perform poorly on the population reward and this is known as overfitting in statistical learning theory.\nWhat are max reward and max entropy agents?! As we have seen, the goal of agents in reinforcement learning is to find a policy $\\pi$ that maximizes the expected discounted return by focusing on actions that lead to the greatest immediate or future rewards. We call these common RL agents \u0026ldquo;max reward agents\u0026rdquo; in this paper. On the other hand, \u0026ldquo;max entropy agents\u0026rdquo; aim to maximize the entropy of the policy for visiting different states. Maximizing entropy encourages the agent to explore a wider range of actions that lead the agent to visit new states even when they don\u0026rsquo;t contribute any reward.\nThis type of agent will help us to make decisions when we have epistemic uncertainty about what to do at test time. Epistemic uncertainty basically means the uncertainty that we have because of our lack of knowledge and can be improved by gathering more information about the situation.\nThe insight of the authors of this paper is that learning a policy that effectively explores the domain is harder to memorize than a policy that maximizes reward for a specific task, and therefore they expect such learned behavior to generalize well.\nWhat is an ensamble of them?!! Ensembling in reinforcement learning involves combining the policies of multiple individual agents to make more accurate and robust decisions. The idea is that by aggregating the outputs of different agents, we can leverage their diverse perspectives and expertise to improve overall performance.\nThis paper uses an ensamble of max reward agents to help the agent decide on the overal epistemic uncetainty that we have at test time.\nWhat is an invariance based approach? Invariance based algorithms in reinforcement learning focus on developing policies that are robust to changes and variations in the environment. These algorithms aim to identify and leverage invariant features or patterns that remain consistent across different environments or tasks. The goal is to ensure that the learned policy performs well not just in the training environment but also in new, unseen environments.\nThis paper uses IDAAC algorithm which is a special kind of DAAC algorithm as its invariance based algorithm.\nDAAC (Decoupled Advantage Actor-Critic) uses two separate networks, one for learning the policy and advantage, and one for learning the value. The value estimates are used to compute the advantage targets.\nIDAAC (Invariant Decoupled Advantage Actor-Critic) adds an additional regularizer to the DAAC policy encoder to ensure that it does not contain episode-specific information. The encoder is trained adversarially with a discriminator so that it cannot classify which observation from a given pair $(s_i, s_j)$ was first in a trajectory.\nFor more information about IDAAC algorithm please refer to reference [4].\nWhat the heck is ProcGen?!!! Procgen is a benchmark suite for evaluating the generalization capabilities of reinforcement learning agents. It was developed by OpenAI and consists of a collection of procedurally generated environments that vary in terms of visual appearance, dynamics, and difficulty. The goal of Procgen is to provide a standardized and challenging set of environments that can be used to assess the ability of RL algorithms to generalize to unseen scenarios.\nIf you are new to reinforcement learning, I know these explanations are a lot to take in! If you find yourself lost, I recommend to check out the following courses at your leisure:\nDeep Reinforcement Learning (by Hugging Face ü§ó) Reinforcement Learning (by Mutual Information) Introduction to Reinforcement Learning (by David Silver) Reinforcement Learning (by Michael Littman \u0026amp; Charles Isbell) Reinforcement Learning (by Emma Brunskill) Deep Reinforcement Learning Bootcamp 2017 Foundations of Deep RL (by Pieter Abbeel) Deep Reinforcement Learning \u0026amp; Control (by Katerina Fragkiadaki) Deep Reinforcement Learning (by Sergey Levine) After reviewing all this we can focus on the rest of the paper!\nHidden Maze Experiment One of the key observation of the authors of this paper is that invariance is not enough for zero-shot generalization of reinforcemen learning algorithm. They designed the hidden maze experiment too demonstrate that. Imagine Maze, but with the walls and goal hidden in the observation. Arguably, this is the most task-invariant observation possible, such that a solution can still be obtained in a reasonable time.\nAn agent with memory can be trained to optimally solve all training tasks: figuring out wall positions by trying to move ahead and observing the resulting motion, and identifying based on its movement history in which training maze it is currently in. Obviously, such a strategy will not generalize to test mazes. Performance in Maze, where the strategy for solving any particular training task must be indicative of that task, has largely not improved by methods based on invariance\nThe following figure shows PPO performance on the hidden maze task, indicating severe overfitting.\nAs described by [5], an agent can overcome test-time errors in its policy by treating the perfect policy as an unobserved variable. The resulting decision making problem, termed the epistemic POMDP, may require some exploration at test time to resolve uncertainty. The article further proposed the LEEP algorithm based on this principle, which trains an ensemble of agents and essentially chooses randomly between the members when the ensemble does not agree, and was the first method to present substantial generalization improvement on Maze.\nIn this paper authors extend this idea and asked, How to improve exploration at test time?, and their approach is based on a novel discovery, when they train an agent to explore the training domains using a maximum entropy objective, they observe that the learned exploration behavior generalizes surprisingly well (much better than the generalization attained when training the agent to maximize reward).\nIn the following section we gonna dig deep into internals of maximum entropy policy.\nMaxEnt Policy For simplicity the authors discuss this part for the MDP case. A policy $\\pi$, through its interaction with an MDP, induces a t-step state distribution over the state space $S$,\n$$ d _ {t,\\pi} (s) = p(s_t=s | \\pi) $$\nThe objective of maximum entropy exploration is given by:\n$$ \\mathcal{H}(d(.)) = -\\mathbb{E} _ {s \\sim d} \\left[ \\log{d(s)} \\right] $$\nWhere $d$ can be regarded as either,\nStationary state distribution (infinite horizon): $d _ {\\pi} = \\lim _ {t \\rightarrow \\infty} d _ {t,\\pi} (s)$ Discounted state distribution (infinite horizon): $d _ {\\gamma, \\pi} = (1-\\gamma) \\sum _ {t=0} ^ {\\infty} \\gamma^t d _ {t,\\pi} (s)$ Marginal state distribution (finite horizon): $d _ {T, \\pi} = \\frac{1}{T} \\sum _ {t=0} ^ {T} d _ {t,\\pi} (s)$ In this work they focus on the finite horizon setting and adapt the marginal state distribution $d _ {T, \\pi}$ in which $T$ equals the episode horizon $H$, so we seek to maximize the objective:\n$$ \\begin{align*} \\mathcal{R} _ {\\mathcal{H}} (\\pi) \u0026amp;= \\mathbb{E} _ {M \\sim \\hat{P}(M)} \\left[ \\mathcal{H}(d _ {H,\\pi}) \\right] \\\\ \u0026amp;=\\mathbb{E} _ {M \\sim \\hat{P}(M)} \\left[ \\mathcal{H}(\\frac{1}{H} \\sum _ {t=0} ^ {H} d _ {t,\\pi} (s)) \\right] \\end{align*} $$\nwhich yields a policy that \u0026ldquo;equally\u0026rdquo; visits all states during the episode.\nTo maximize this objective we can estimating the density of the agent\u0026rsquo;s state visitation distribution, but in this paper the authors adapt the non-parametric entropy estimation approach; we estimate the entropy using the particle based k-nearest neighbor (k-NN estimator).\nTo estimate the distribution $d _ {H,\\pi}$ over the states $S$, we consider each trajectory as $H$ samples of states $\\{ s_t \\} _ {t=1} ^ {H}$ and take $s _ t ^ {\\text{k-NN}}$ to be the k-NN of the state $s_t$ within the trajectory,\n$$ \\hat{ \\mathcal{H} } ^ {k,H} (d _ {H,\\pi}) \\approx \\frac{1}{H} \\sum _ {t=1} ^ {H} \\log ( \\parallel s_t - s_t^{\\text{k-NN}} \\parallel _ 2) $$\nIn which we define intrinsic reward function as,\n$$ r_I (s_t) \\coloneqq \\log ( \\parallel s_t - s_t^{\\text{k-NN}} \\parallel _ 2) $$\nThis formulation enables us to deploy any RL algorithm to approximately optimize objective.\nSpecifically, in this work we use the policy gradient algorithm PPOŸà where at every time step $t$, the state $s_t^{\\text{k-NN}}$ is chosen from previous states $\\{ s_t \\} _ {t=1} ^ {t-1}$ of the same episode. To improve computational efficiency, instead of taking the full observation as the state (64 x 64 RGB image), we sub-sample the observation by applying average pooling of 3 x 3 to produce an image of size 21 x 21.\nWe found that agents trained for maximum entropy exploration exhibit a smaller generalization gap compared with the standard approach of training solely with extrinsic reward. The policies are equipped with a memory unit (GRU) to allow learning of deterministic policies that maximize the entropy.\nIn all three environments, we demonstrate a small generalization gap, as test performance on unseen levels closely follows the performance achieved during training.\nIn addition, we verify that the train results are near optimal by comparing with a hand designed approximately optimal exploration policy. For example, on Maze we use the well known maze exploring strategy wall follower, also known as the left/right-hand rule.\nExpGen Algorithm Our main insight is that, given the generalization property of the entropy maximization policy established above, an agent can apply this behavior in a test MDP and expect effective exploration at test time. We pair this insight with the epistemic POMDP idea, and propose to play the exploration policy when the agent faces epistemic uncertainty, hopefully driving the agent to a different state where the reward-seeking policy is more certain.\nOur framework comprises two parts: an entropy maximizing network and an ensemble of networks that maximize an extrinsic reward to evaluate epistemic uncertainty. The first step entails training a network equipped with a memory unit to obtain a maxEnt policy $\\pi_H$ that maximizes entropy. Next, we train an ensemble of memory-less policy networks $\\{ \\pi _ r ^ j \\} _ {j=1} ^ {m} $ to maximize extrinsic reward.\nHere is the ExpGen algorithm,\nWe consider domains with a finite action space, and say that the policy $\\pi _ r ^ i$ is certain at state $s$ if its action $a_i \\sim \\pi _ r ^ i (a|s)$ is in consensus with the ensemble: $a_i = a_j$ for the majority of $k$ out of $m$, where $k$ is a hyperparameter of our algorithm.\nSwitching between two policies may result in a case where the agent repeatedly toggles between two states (if, say, the maxEnt policy takes the agent from state $s_1$ to a state $s_2$, where the ensemble agrees on an action that again moves to state $s_1$.). To avoid such ‚Äúmeta-stable‚Äù behavior, we randomly choose the number of maxEnt steps $n_{\\pi_{\\mathcal{H}}}$ from a Geometric distribution, $n_{\\pi_{\\mathcal{H}}} \\sim Geom(\\alpha)$.\nExperiments Our experimental setup follows ProcGen\u0026rsquo;s easy configuration, wherein agents are trained on 200 levels for 25M steps and subsequently tested on random levels. All agents are implemented using the IMPALA (Importance Weighted Actor-Learner Architectures) convolutional architecture, and trained using PPO or IDAAC. For the maximum entropy agent $\\pi_H$ we incorporate a single GRU at the final embedding of the IMPALA convolutional architecture. For all games, we use the same parameter $\\alpha=0.5$ of the Geometric distribution and form an ensemble of 10 networks.\nFollowing figure is comparison across all ProcGen games, with 95% bootstrap CIs highlighted in color. Score distributions of ExpGen, PPO, PLR, UCB-DrAC, PPG and IDAAC.\nFollowing figure shows in each row, the probability of algorithm X outperforming algorithm Y. The comparison illustrates the superiority of ExpGen over the leading contender IDAAC with probability 0.6, as well as over other methods with even higher probability.\nSee Also PyTorch implementation of ExpGen @ GitHub Ev Zisselman Presentation @ NeurIPS 2023 ExpGen Rebuttal Process @ OpenReview ExpGen Poster for NeurIPS 2023 References [1] Zisselman, Ev, et al. \u0026ldquo;Explore to generalize in zero-shot rl.\u0026rdquo; Advances in Neural Information Processing Systems 36 (2024).\n[2] Sutton, Richard S., and Andrew G. Barto. \u0026ldquo;Reinforcement learning: An introduction.\u0026rdquo; MIT press, (2020).\n[3] Kirk, Robert, et al. \u0026ldquo;A survey of zero-shot generalisation in deep reinforcement learning.\u0026rdquo; Journal of Artificial Intelligence Research 76 (2023): 201-264.\n[4] Raileanu, Roberta, and Rob Fergus. \u0026ldquo;Decoupling value and policy for generalization in reinforcement learning.\u0026rdquo; International Conference on Machine Learning. PMLR, 2021.\n[5] Ghosh, Dibya, et al. \u0026ldquo;Why generalization in rl is difficult: Epistemic pomdps and implicit partial observability.\u0026rdquo; Advances in neural information processing systems 34 (2021): 25502-25515.\n[6] Espeholt, Lasse, et al. \u0026ldquo;Impala: Scalable distributed deep-rl with importance weighted actor-learner architectures.\u0026rdquo; International conference on machine learning. PMLR, 2018.\n","permalink":"http://localhost:1313/posts/explore-to-generalize-in-zero-shot-rl/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eThis paper studies the problem of zero-shot generalization in reinforcement learning and introduces an algorithm that trains an ensamble of maximum reward seeking agents and an maximum entropy agent. At test time, either the ensemble agrees on an action, and we generalize well, or we take exploratory actions with the help of maximum entropy agent and drive us to a novel part of the state space, where the ensemble may potentially agree again. This method combined with an invariance based approach achieves new state-of-the-art results on ProcGen.\u003c/p\u003e","title":"ExpGen: Explore to Generalize in Zero-Shot RL"},{"content":"Welcome to our blog! We are a team of enthusiastic professors and students from Sharif University of Technology who are eager to share our insights and passion for this fascinating field. Our aim is to share our knowledge and excitement about this cutting-edge field with you ‚ù§Ô∏è.\nProfessors Professor Mohammad Hossein Rohban Professor Ehsaneddin Asgari Students Arash Alikhani Alireza Nobakht Labs RIML Lab NLP \u0026amp; DH Lab We hope you enjoy our blog and find our content both informative and inspiring üöÄ!\n","permalink":"http://localhost:1313/us/","summary":"\u003cp\u003eWelcome to our blog! We are a team of enthusiastic professors and students from Sharif University of Technology who are eager to share our insights and passion for this fascinating field. Our aim is to share our knowledge and excitement about this cutting-edge field with you ‚ù§Ô∏è.\u003c/p\u003e\n\u003ch2 id=\"professors\"\u003eProfessors\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eProfessor \u003ca href=\"https://www.linkedin.com/in/mohammad-hossein-rohban-75567677\"\u003eMohammad Hossein Rohban\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003eProfessor \u003ca href=\"https://www.linkedin.com/in/ehsaneddinasgari\"\u003eEhsaneddin Asgari\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"students\"\u003eStudents\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://www.linkedin.com/in/infinity2357\"\u003eArash Alikhani\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://www.linkedin.com/in/alireza-nobakht\"\u003eAlireza Nobakht\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"labs\"\u003eLabs\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/rohban-lab\"\u003eRIML Lab\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/language-ml\"\u003eNLP \u0026amp; DH Lab\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eWe hope you enjoy our blog and find our content both informative and inspiring üöÄ!\u003c/p\u003e","title":""},{"content":"Adversarial Attacks on Reinforcement Learning Policies Reinforcement Learning (RL) has powered breakthroughs in games, robotics, and autonomous systems. But just like image classifiers can be fooled by carefully crafted adversarial examples, RL agents are also vulnerable to adversarial attacks. These attacks can manipulate the agent‚Äôs perception of the environment or its decision process, leading to unsafe or suboptimal behavior.\nWhat Do Adversarial Attacks Mean in RL? In RL, an agent interacts with an environment, observes a state, takes an action, and receives a reward. An adversarial attack interferes with this process‚Äîoften by perturbing the input observations‚Äîso the agent chooses actions that look reasonable to it but are actually harmful.\nFor example:\nA self-driving car policy might mistake a stop sign for a speed-limit sign if subtle noise is added to the camera input. A robot trained to walk could be tripped by slight modifications to its sensor readings. Types of Attacks in Reinforcement Learning Researchers have identified several ways adversarial attacks can target RL policies. These attack vectors include:\nObservation Perturbation\nThe most widely studied form of attack: small, carefully crafted changes are made to the agent‚Äôs observations so it perceives the environment incorrectly.\nExample: Adding imperceptible noise to frames in an Atari game so the agent misjudges its next move.\nCommunication Perturbation\nIn multi-agent systems, communication messages can be perturbed, leading to miscoordination.\nExample: Two drones sharing location data receive slightly altered coordinates, causing a collision.\nMalicious Communications\nBeyond perturbations, adversaries may inject entirely fake or deceptive messages.\nExample: An attacker sends a false signal about enemy positions in a cooperative strategy game.\nWhy It Matters Adversarial attacks highlight the gap between high-performing RL policies in controlled benchmarks and their reliability in the real world. Understanding and mitigating these vulnerabilities is essential if we want RL to be trusted in safety-critical domains like autonomous driving, robotics, and healthcare.\nHow Vulnerable Are RL Policies to Adversarial Attacks? Reinforcement Learning (RL) has made huge strides in recent years, powering systems that can beat human champions in games and control robots with precision. But just like image classifiers can be fooled by imperceptible changes to inputs, RL policies are also highly vulnerable to adversarial attacks\nThe researchers show that even tiny perturbations‚Äîso small they are invisible to humans‚Äîcan cause RL agents to fail dramatically at test time. Using Atari games as a testbed, they evaluate three common deep RL algorithms: DQN, TRPO, and A3C. The results are clear: all three are susceptible, but DQN policies are especially vulnerable.\nWhite-Box Attacks with FGSM In white-box scenarios, where the adversary has full access to the policy and gradients, attacks are devastating. The team applies the Fast Gradient Sign Method (FGSM) to craft adversarial examples and finds:\nAn ‚Ñì‚àû-norm perturbation with Œµ = 0.001 can slash performance by over 50%. ‚Ñì1-norm attacks are even more powerful‚Äîby changing just a handful of pixels significantly, they can cripple the agent‚Äôs performance. Even policies trained with robust algorithms like TRPO and A3C experience sharp drops when faced with these attacks.\nBlack-Box Attacks and Transferability What if the adversary doesn‚Äôt have full access to the policy network? Surprisingly, attacks are still effective. In black-box settings, adversaries exploit the property of transferability:\nAdversarial examples created for one policy often transfer to another policy trained on the same task. Transferability also extends across algorithms‚Äîfor example, attacks generated against a DQN policy can still reduce the performance of an A3C policy. Effectiveness decreases with less knowledge, but performance still degrades significantly, especially with ‚Ñì1 attacks. Why This Matters The key takeaway is that adversarial examples are not just a computer vision problem. RL policies‚Äîdespite achieving high scores in training‚Äîcan be undermined by imperceptible perturbations. This fragility is dangerous for real-world applications like autonomous driving and robotics, where safety and reliability are non-negotiable.\nUniversal Adversarial Preservation (UAP) To ground these ideas, I re-implemented the 2017 Adversarial Attacks on Neural Network Policies setup (since there was no offical impelmentation) and trained a DQN agent on Atari Pong. After validating the baseline attacks, I implemented Universal Adversarial Perturbations (UAPs) and found that a single, fixed perturbation‚Äîcomputed once and then applied it to all observations, which was enough to consistently derail the policy across episodes and random seeds, without recomputing noise at every timestep. In other words, the attack generalized over time and trajectories, confirming that UAPs exploit stable perceptual quirks of the learned policy rather than moment-by-moment gradients. Practically, this feels much closer to a real-world threat model: an attacker only needs to tamper with the sensor once (think a sticker/overlay on the lens) instead of having high-bandwidth, per-step access to the system. Below you can see the plot of rewards vs Œµ bugdet and videos of different setups.\nbaselines\u0026rsquo; reward over different values of Œµ budget.\nClean ‚Äî original episode (no perturbation). Random uniform noise (Œµ = 2.0). FGSM (Œµ = 2.0) ‚Äî white-box attack. PGD (Œµ = 2.0) ‚Äî iterative, white-box attack. UAP (Œµ = 2.0) ‚Äî image-agnostic. Adversarial Policies: when weird opponents break strong RL TL;DR. Instead of adding pixel noise to an RL agent‚Äôs input, this paper shows you can train a policy that acts in the shared world to induce natural but adversarial observations for the victim‚Äîcausing robust, self-play‚Äìtrained agents to fail in zero-sum MuJoCo games. Fine-tuning helps‚Ä¶until a new adversary is learned.\nThe threat model: natural observations as the ‚Äúperturbation‚Äù In multi-agent settings, an attacker typically can‚Äôt flip pixels or edit state vectors. But it can choose actions that make the victim see carefully crafted, physically plausible observations. Hold the victim fixed and the two-player game becomes a single-agent MDP for the attacker, who learns a policy that elicits bad actions from the victim.\nQuick look:\nYou Shall Not Pass Kick \u0026amp; Defend Sumo (Human) Masked victim vs adversary Setup in a nutshell Victims: strong self-play policies (‚Äúagent zoo‚Äù) across four MuJoCo tasks: Kick \u0026amp; Defend, You Shall Not Pass, Sumo Humans, Sumo Ants. Attacker: trained with PPO for ~20M timesteps‚Äî\u0026lt; 3% of the 680‚Äì1360M timesteps used for the victims‚Äîyet reliably wins. Key idea: adversaries don‚Äôt become great players; they learn poses/motions that generate adversarial observations for the victim. Figures\nTasks used for evaluation.\nAdversary win rate rises quickly despite far fewer timesteps.\nWhat the learned adversary looks like (and why that matters) In Kick \u0026amp; Defend and YSNP, the adversary may never stand up‚Äîit finds contorted, stable poses that make the victim mis-act. In Sumo Humans, where falling loses immediately, it adopts a kneeling/stable stance that still provokes the victim to fall.\nQualitative behaviors: the ‚Äúpoint‚Äù is to confuse, not to excel at the nominal task.\nMasking test: evidence the attack is observational If wins come from manipulating what the victim sees, then hiding the adversary‚Äôs pose from the victim should help. That‚Äôs exactly what happens:\nAgainst normal opponents, the masked victim is (unsurprisingly) worse. Against the adversary, the masked victim becomes nearly immune (e.g., in YSNP: normal victim loses often; masked victim flips the outcome and wins almost always). Masking the adversary‚Äôs position removes the observation channel the attack exploits.\nDimensionality matters Victims are more vulnerable when more opponent DOFs are observed. The attack is stronger in Humanoid (higher-dimensional observed joints) than Ant (lower-dimensional). More controllable joints ‚Üí more ways to steer the victim off-distribution.\nHigher observed dimensionality correlates with higher adversary win rates.\nWhy it works: off-distribution activations Analyses of the victim‚Äôs network show adversarial opponents push internal activations farther from the training manifold than random or lifeless baselines.\nAdversarial policies drive ‚Äúweird‚Äù activations‚Äîmore off-distribution than simple OOD baselines.\nDefenses (and their limits) Fine-tuning the victim on the discovered adversary reduces that adversary‚Äôs success (often down to ~10% in YSNP), but:\nCatastrophic forgetting: performance vs normal opponents degrades (single-adversary fine-tune is worst; dual fine-tune helps but still regresses). Arms race: re-running the attack against the fine-tuned victim yields a new adversary that succeeds again‚Äîoften via a different failure mode (e.g., tripping rather than pure confusion). Before fine-tune After fine-tune (this adversary) Win-rate grid before/after fine-tuning against normal opponents and adversaries.\nTakeaways for practitioners Threat model upgrade: in multi-agent worlds, your attack surface includes other policies that craft natural observations‚Äîno pixel hacks needed. Exploitability check: training a targeted adversary lower-bounds your policy‚Äôs worst-case performance and reveals failure modes missed by self-play. Defense needs diversity: fine-tuning on a single adversary overfits. Prefer population-based or curriculum defenses that rotate diverse opponents and maintain competence vs normals. Robust Communicative Multi-Agent Reinforcement Learning with Active Defense By Yu et al., AAAI 2024\nüåê Why Communication Matters in Multi-Agent RL In multi-agent reinforcement learning (MARL), agents often face partial observability ‚Äî no single agent sees the full environment. To cooperate effectively, agents need to communicate, sharing information about what they see and what actions to take.\nThis communication has powered applications such as robot navigation and traffic light control.\nBut there‚Äôs a catch: in the real world, communication channels are noisy and vulnerable to adversarial attacks. If attackers tamper with even a few messages, the performance of MARL systems can collapse.\nüõ°Ô∏è Enter Active Defense: The Core Idea Yu et al. propose a new active defense strategy. Instead of blindly trusting all messages, agents:\nJudge the reliability of each incoming message using their own observations and history (hidden states). Adjust the influence of unreliable messages by reducing their weight in the decision process. üëâ Example: If one agent already searched location (1,1) and found nothing, but receives a message saying ‚ÄúTarget at (1,1)‚Äù, it can spot the inconsistency and downweight that message.\nüß© The ADMAC Framework The authors introduce Active Defense Multi-Agent Communication (ADMAC), which has two key components:\nReliability Estimator: A classifier that predicts whether a message is reliable (weight close to 1) or unreliable (weight close to 0). Decomposable Message Aggregation Policy Net: A structure that breaks down the influence of each message into an action preference vector, making it possible to scale its impact up or down. This allows agents to combine their own knowledge with weighted messages to make more robust decisions.\nThe figure above shows how an agent in the ADMAC framework generates its action distribution by combining its own observations with incoming messages from other agents:\nHidden state update: The agent maintains a hidden state (h·µ¢·µó‚Åª¬π), which is updated using the observation (o·µ¢·µó) through the GRU module f_HP. This captures past and current information. Base action preference: From the updated hidden state, the agent generates a base preference vector via f_BP, representing what it would do independently. Message influence: Each received message (m‚ÇÅ·µó, ‚Ä¶, m_N·µó) is processed with the observation through f_MP, producing a message-based action preference vector. Reliability estimation: A reliability estimator f_R evaluates each message, assigning it a weight w·µ¢(m‚±º·µó) that reflects how trustworthy it seems. Aggregation: The agent sums its base vector with all weighted message vectors to form a total action preference vector (v·µ¢·µó). Final decision: Applying a Softmax function converts this vector into a probability distribution over actions, from which the agent selects its next move. By downweighting unreliable messages, ADMAC enables agents to remain robust against malicious communication while still leveraging useful information from peers.\nReference: Yu, L., Qiu, Y., Yao, Q., Shen, Y., Zhang, X., \u0026amp; Wang, J. (2024). Robust Communicative Multi-Agent Reinforcement Learning with Active Defense. AAAI-24.\n","permalink":"http://localhost:1313/posts/adversarial-rl/","summary":"\u003ch1 id=\"adversarial-attacks-on-reinforcement-learning-policies\"\u003eAdversarial Attacks on Reinforcement Learning Policies\u003c/h1\u003e\n\u003cp\u003eReinforcement Learning (RL) has powered breakthroughs in games, robotics, and autonomous systems. But just like image classifiers can be fooled by carefully crafted adversarial examples, RL agents are also vulnerable to \u003cstrong\u003eadversarial attacks\u003c/strong\u003e. These attacks can manipulate the agent‚Äôs perception of the environment or its decision process, leading to unsafe or suboptimal behavior.\u003c/p\u003e\n\u003ch2 id=\"what-do-adversarial-attacks-mean-in-rl\"\u003eWhat Do Adversarial Attacks Mean in RL?\u003c/h2\u003e\n\u003cp\u003eIn RL, an agent interacts with an environment, observes a state, takes an action, and receives a reward. An adversarial attack interferes with this process‚Äîoften by perturbing the input observations‚Äîso the agent chooses actions that look reasonable to it but are actually harmful.\u003c/p\u003e","title":"Adversarial RL"},{"content":"Actor-Critic vs. Value-Based: Empirical Trade-offs Introduction In Reinforcement Learning (RL), one of the fundamental questions is:\nShould we focus on learning the value of states, or should we directly learn the optimal policy?\nBefore diving into this comparison, let‚Äôs briefly recall what RL is: a branch of machine learning in which an agent interacts with an environment to learn how to make decisions that maximize cumulative reward.\nTraditionally, RL algorithms fall into two main families:\nValue-Based methods : which aim to estimate the value of states or state‚Äìaction pairs. Policy-Based methods : which directly optimize a policy that maps states to actions. Actor-Critic algorithms combine the strengths of both worlds, simultaneously learning a value function and an explicit policy. This hybrid structure can often lead to more stable and efficient learning.\nValue-Based and Actor-Critic approaches represent fundamentally different perspectives: one focuses solely on learning state values, while the other integrates both value and policy learning. Comparing these two perspectives helps us better understand the impact of incorporating value or policy components in different environments.\nIn this project, we empirically evaluate these two families in both discrete and continuous action spaces. Four representative algorithms ( DQN, A2C, NAF, and SAC ) were implemented, along with a user-friendly graphical user interface (GUI) for training and evaluation.\nOur ultimate goal is to analyze trade-offs such as convergence speed, learning stability, and final performance across diverse scenarios.\nBackground The reinforcement learning algorithms used in this project fall into two main families:\nValue-Based\nIn this approach, the agent learns only a value function, such as $Q(s, a)$.\nThe policy is derived implicitly by selecting the action with the highest value:\n$\\pi(s) = \\arg\\max_a Q(s,a)$\nThis method is typically simpler, more stable, and computationally efficient.\nHowever, it faces limitations when dealing with continuous action spaces.\nExample algorithms: DQN, NAF.\nValue-Based methods are often well-suited for discrete action spaces with relatively small state‚Äìaction domains, where enumerating or approximating the value for each action is feasible.\nActor-Critic\nIn this framework, the agent consists of two components:\nActor : a parameterized policy that directly produces actions. Critic : a value function that evaluates the Actor‚Äôs performance and guides its updates. This combination can provide greater learning stability, improved performance in complex environments, and high flexibility in continuous action spaces.\nExample algorithms: A2C, SAC.\nActor-Critic methods are generally more suitable for continuous or high-dimensional action spaces, as the Actor can output actions directly without exhaustive value estimation.\nMethodology Project Design This project was designed to perform an empirical comparison between two major families of reinforcement learning algorithms: Value-Based and Actor-Critic.\nFour representative algorithms were selected and implemented in diverse discrete and continuous environments.\nTraining, evaluation, and comparison were carried out through a fully interactive, user-friendly graphical interface.\nImplemented Algorithms Representative algorithms from the two families were selected based on their reported performance in different environments according to the literature.\nFor each algorithm, the training procedure was reproduced in accordance with its original paper.\nThe overall structure of each algorithm is summarized below:\nAlgorithm Family Action Space Description Reference Deep Q-Network (DQN) Value-Based Discrete Uses experience replay and a fixed target network to stabilize learning. 1 Normalized Advantage Function (NAF) Value-Based Continuous Value-based method for continuous spaces using a specific Q-structure to simplify action selection. 2 Advantage Actor-Critic (A2C) Actor-Critic Discrete/Continuous Direct policy optimization guided by an advantage function. 3 Soft Actor-Critic (SAC) Actor-Critic Continuous Off-policy actor-critic method maximizing entropy for stability in complex environments. 4 Deep Q-Network (DQN) The pseudocode of DQN highlights the use of experience replay and a target network, which together reduce correlations between samples and stabilize training.\nNormalized Advantage Function (NAF) NAF handles continuous action spaces by constraining the Q-function into a quadratic form, which makes action selection computationally efficient.\nAdvantage Actor-Critic (A2C) A2C directly optimizes a parameterized policy (Actor) with guidance from the Critic, using advantage estimation to reduce gradient variance and improve learning stability.\nSoft Actor-Critic (SAC) SAC introduces entropy maximization in the objective, encouraging exploration and robustness in complex continuous environments.\nEnvironments There were many 2D and 3D environments available so that we could compare them.\nThe 12 famous environments are listed below:\nFour environments from the Gym library were selected to provide a diverse set of challenges that cover both discrete and continuous action spaces, as well as varying levels of complexity and dynamics:\nMountainCar-v0 (Discrete): A classic control problem where the agent must drive a car up a hill using discrete acceleration commands. This environment tests basic exploration and planning in a low-dimensional, discrete action space. Pendulum-v1 (Continuous): Requires applying continuous torque to keep a pendulum upright. This environment is ideal for evaluating continuous control algorithms and stabilizing dynamics. FrozenLake-v1 (Discrete): A gridworld task where the agent navigates an icy lake to reach a goal while avoiding holes. This environment emphasizes decision-making under uncertainty in a discrete setting. HalfCheetah-v4 (Continuous): A high-dimensional continuous control environment where the agent controls a bipedal cheetah to run efficiently. It challenges advanced continuous control and balance strategies. These environments were chosen to allow a comprehensive comparison of algorithms across different action types, state complexities, and control challenges.\nEnvironment Type Description MountainCar-v0 Discrete Drive a car up a hill by controlling acceleration in a discrete space. Pendulum-v1 Continuous Apply torque to keep a pendulum upright and stable. FrozenLake-v1 Discrete Navigate an icy grid to reach the goal without falling into holes. HalfCheetah-v4 Continuous Control the speed and balance of a simulated bipedal cheetah for fast running. Environment snapshots were recorded during training and appear as GIFs in the Results section.\nConfiguration and Evaluation Algorithms were run with optimized settings for each environment. Key hyperparameters (learning rate, Œ≥, batch size, buffer size) were tuned through trial-and-error, leveraging existing GitHub implementations for optimal performance. Comparisons were based on convergence speed, training stability, and final performance. Example configurations:\nAlgorithm Environment Œ≥ Learning Rate Batch Size Buffer Size DQN FrozenLake 0.93 6e-4 32 4,000 A2C MountainCar 0.96 1e-3 ‚Äì ‚Äì NAF Pendulum 0.99 3e-4 64 400,000 SAC HalfCheetah 0.99 3e-4 256 1,000,000 Graphical User Interface (GUI) A user-friendly GUI was developed to simplify training and comparing algorithms, enabling full project execution without direct coding.The code is available at: This Github Link The project structure is as follows:\nProject_Code/\r‚îú‚îÄ‚îÄ plots/\r‚îÇ ‚îî‚îÄ‚îÄ learning_curves/\r‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ Continuous/\r‚îÇ ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ Pendulum-v1/\r‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ SAC/\r‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ NAF/\r‚îÇ ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ HalfCheetah-v4/\r‚îÇ ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ SAC/\r‚îÇ ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ NAF/\r‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ Discrete/\r‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ FrozenLake-v1/\r‚îÇ ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ DQN/\r‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ 4x4\r‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ 8x8\r‚îÇ ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ A2C/\r‚îÇ ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ 4x4\r‚îÇ ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ 8x8\r‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ MountainCar-v0/\r‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ DQN/\r‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ A2C/\r‚îÇ ‚îú‚îÄ‚îÄ comparison_table.png\r‚îÇ ‚îî‚îÄ‚îÄ directory_structure.png\r‚îú‚îÄ‚îÄ requirements/\r‚îÇ ‚îú‚îÄ‚îÄ base.txt\r‚îÇ ‚îú‚îÄ‚îÄ dev.txt\r‚îÇ ‚îú‚îÄ‚îÄ env.txt\r‚îÇ ‚îî‚îÄ‚îÄ config.py\r‚îú‚îÄ‚îÄ src/\r‚îÇ ‚îú‚îÄ‚îÄ agents/\r‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ a2c.py\r‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ dqn.py\r‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ naf.py\r‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ sac.py\r‚îÇ ‚îú‚îÄ‚îÄ envs/\r‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ continuous_envs.py\r‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ discrete_envs.py\r‚îÇ ‚îú‚îÄ‚îÄ main.py\r‚îÇ ‚îú‚îÄ‚îÄ train.py\r‚îÇ ‚îî‚îÄ‚îÄ utils.py\r‚îú‚îÄ‚îÄ videos/\r‚îÇ ‚îú‚îÄ‚îÄ Continuous/\r‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ Pendulum-v1/\r‚îÇ ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ SAC/\r‚îÇ ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ NAF/\r‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ HalfCheetah-v4/\r‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ SAC/\r‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ NAF/\r‚îÇ ‚îî‚îÄ‚îÄ Discrete/\r‚îÇ ‚îú‚îÄ‚îÄ FrozenLake-v1/\r‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ DQN/\r‚îÇ ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ 4x4\r‚îÇ ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ 8x8\r‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ A2C/\r‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ 4x4\r‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ 8x8\r‚îÇ ‚îî‚îÄ‚îÄ MountainCar-v0/\r‚îÇ ‚îú‚îÄ‚îÄ DQN/\r‚îÇ ‚îî‚îÄ‚îÄ A2C/\r‚îú‚îÄ‚îÄ models/\r‚îÇ ‚îú‚îÄ‚îÄ Continuous/\r‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ Pendulum-v1/\r‚îÇ ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ SAC/\r‚îÇ ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ NAF/\r‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ HalfCheetah-v4/\r‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ SAC/\r‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ NAF/\r‚îÇ ‚îî‚îÄ‚îÄ Discrete/\r‚îÇ ‚îú‚îÄ‚îÄ FrozenLake-v1/\r‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ DQN/\r‚îÇ ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ 4x4\r‚îÇ ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ 8x8\r‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ A2C/\r‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ 4x4\r‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ 8x8\r‚îÇ ‚îî‚îÄ‚îÄ MountainCar-v0/\r‚îÇ ‚îú‚îÄ‚îÄ DQN/\r‚îÇ ‚îî‚îÄ‚îÄ A2C/\r‚îî‚îÄ‚îÄ README.md Main features:\nSelect algorithm, environment, action space type (discrete/continuous), and execution mode (train/test) with just a few clicks. Launch training with a Run button, and view results via Show Plots and Show Videos. Compare algorithms interactively using a dedicated Compare Algorithms window. Display key settings such as hyperparameters and project structure. Real-time console output for monitoring execution status and system messages. Interactive Mobile Application In addition to the desktop GUI, a mobile application for Android and iOS has been developed to provide interactive access to the project. By simply scanning the poster, users can explore various features, including:\nViewing videos of agent executions in different environments. Opening the project‚Äôs website for additional resources and documentation. Displaying the poster digitally for interactive exploration. Comparing learning curves and results across different algorithms. The images below showcase some sections of the app interface:\nResults This section presents the empirical evaluation of four reinforcement learning algorithms (DQN, NAF, A2C, and SAC) from the Value-Based and Actor-Critic families, across both discrete and continuous action space environments. The performance is analyzed based on final reward, convergence speed, and training stability, supported by quantitative metrics, qualitative visualizations (GIFs), and learning curves with a moving average (MA) applied to reduce noise. The experiments were conducted using the Gymnasium library, with optimized hyperparameters (see Methodology for details) and a fixed random seed for reproducibility. Training was performed on a single NVIDIA RTX 4050 GPU, with average runtimes of 1‚Äì4 hours per algorithm-environment pair.\nDiscrete Environments The discrete action space environments tested were FrozenLake-v1 (8x8 grid) and MountainCar-v0, which challenge the algorithms with stochastic transitions and sparse rewards, respectively. The table below summarizes the performance, followed by detailed analyses and visualizations.\nEnvironment Algorithm Final Reward (Avg ¬± Std) Convergence Speed (Episodes to 90% of Max) Stability (Std of Reward) FrozenLake-v1 DQN 0.98 ¬± 0.14 ~1,475 0.50 A2C 1.00 ¬± 0.00 ~1,209 0.48 MountainCar-v0 DQN -22.21 ¬± 79.32 ~2,000 81.50 A2C -27.87 ¬± 62.35 ~2,000 40.83 FrozenLake-v1 In FrozenLake-v1, a stochastic gridworld, A2C outperformed DQN in final reward (1.00 vs. 0.98 success rate) and converged faster (~1209 vs. ~1475 episodes to reach 90% of max reward). A2C‚Äôs advantage estimation provided greater stability, as evidenced by its lower standard deviation (0.48 vs. 0.50). The GIF below illustrates agent behaviors, showing A2C‚Äôs smoother navigation to the goal compared to DQN‚Äôs occasional missteps.\nReward comparison in FrozenLake-v1: A2C converges quickly and maintains stable performance compared to DQN.\nMountainCar-v0 In MountainCar-v0, a deterministic environment with sparse rewards, DQN achieved a better final reward (-22.21 vs. -27.87 timesteps to goal) but both algorithms converged at similar speeds (~2000 episodes). However, A2C exhibited greater stability (std of 40.83 vs. 81.50), avoiding large oscillations in learning. The GIF below shows DQN‚Äôs quicker ascent to the hilltop, while A2C maintains more consistent swings.\nReward comparison in MountainCar-v0: DQN converges faster and reaches higher rewards, while A2C shows greater stability.\nContinuous Environments The continuous action space environments tested were Pendulum-v1 and HalfCheetah-v4, which require precise control and balance in low- and high-dimensional settings, respectively. The table below summarizes the performance.\nEnvironment Algorithm Final Reward (Avg ¬± Std) Convergence Speed (Episodes to 90% of Max) Stability (Std of Reward) Pendulum-v1 NAF -141.17 ¬± 85.58 ~246 199.46 SAC 287.66 ¬± 62.38 ~152 113.23 HalfCheetah-v4 NAF 3,693.35 ¬± 575.60 ~862 1,077.01 SAC 10,247.42 ¬± 584.31 ~1,127 2,493.55 Pendulum-v1 In Pendulum-v1, SAC significantly outperformed NAF in final reward (287.66 vs. -141.17, higher is better) and converged faster (~152 vs. ~246 episodes). SAC‚Äôs entropy maximization ensured smoother learning, with a standard deviation of 113.23 compared to NAF‚Äôs 199.46. The GIF below highlights SAC‚Äôs ability to stabilize the pendulum upright, while NAF struggles with inconsistent torque.\nReward comparison in Pendulum-v1: SAC achieves higher rewards with smoother convergence compared to NAF.\nHalfCheetah-v4 In HalfCheetah-v4, a high-dimensional control task, SAC achieved a much higher final reward (10247.42 vs. 3693.35) but converged slightly slower (~1127 vs. ~862 episodes). SAC‚Äôs stability (std of 2493.55 vs. 1077.01) reflects its robustness in complex dynamics, though NAF shows lower variance. The GIF below shows SAC‚Äôs fluid running motion compared to NAF‚Äôs less coordinated movements.\nReward comparison in HalfCheetah-v4: SAC consistently outperforms NAF in both speed and stability.\nDiscussion \u0026amp; Conclusion This project examined the performance differences between Value-Based and Actor-Critic algorithms in both discrete and continuous environments.\nThe experimental results indicate that no single algorithm is universally superior; rather, the environment characteristics and action space type play a decisive role in determining performance.\nAnalysis \u0026amp; Interpretation Continuous Action Spaces SAC consistently outperformed NAF in both Pendulum-v1 and HalfCheetah-v4, thanks to its entropy maximization strategy, which promotes exploration and robustness. NAF‚Äôs fixed quadratic Q-function structure limited its flexibility in high-dimensional or complex tasks, leading to slower convergence and higher variance in rewards. SAC‚Äôs ability to directly optimize a stochastic policy made it particularly effective in continuous control scenarios.\nDiscrete Action Spaces In FrozenLake-v1, a stochastic environment, A2C‚Äôs stability (due to advantage estimation) gave it an edge over DQN, achieving higher success rates and faster convergence. In MountainCar-v0, a deterministic environment with a small action space, DQN‚Äôs value-based approach excelled in final reward and convergence speed, though A2C remained more stable. This highlights the suitability of Value-Based methods for simpler, deterministic settings and Actor-Critic methods for stochastic or complex environments.\nKey Findings: In simple discrete environments, such as FrozenLake, Value-Based algorithms (e.g., DQN) achieved competitive performance, but Actor-Critic algorithms (e.g., A2C) showed faster convergence and more stable learning. In continuous and more complex environments, Actor-Critic algorithms ‚Äî particularly SAC ‚Äî outperformed their Value-Based counterparts in terms of final reward and convergence speed. Observed Trade-offs: Aspect Value-Based Actor-Critic Simplicity of implementation Yes More complex Initial learning speed High in simple environments Depends on tuning Training stability More oscillations More stable Suitability for continuous spaces Not always Yes Overall, the choice between Value-Based and Actor-Critic methods should be guided by the nature of the task, the complexity of the environment, and the available computational budget.\nObservations Based on Environment Characteristics Our experimental results further reveal that the nature of the environment‚Äîin terms of action space, state space, and reward structure‚Äîsignificantly impacts algorithm performance:\nAction Space (Discrete vs. Continuous) Discrete Action Spaces: Value-Based algorithms like DQN tend to perform competitively, especially in small and low-dimensional discrete action spaces. They converge quickly and reliably, but may struggle when the action space grows larger or stochasticity increases. Actor-Critic methods such as A2C can still provide improved stability in these scenarios, especially under stochastic transitions. Continuous Action Spaces: Actor-Critic methods (e.g. SAC) dominate due to their ability to output continuous actions directly. Value-Based methods require specialized approximations (like NAF), which often limit flexibility and performance in high-dimensional or continuous control tasks. State Space (Low-dimensional vs. High-dimensional) Low-dimensional states (e.g., MountainCar, FrozenLake) generally favor Value-Based methods, which can efficiently enumerate or approximate Q-values. High-dimensional states (e.g., HalfCheetah) require the policy network of Actor-Critic methods to generalize across large state spaces. These methods better handle complex dynamics and correlations among state variables. Reward Structure (Sparse vs. Dense) Sparse Reward Environments (e.g., FrozenLake) challenge Value-Based methods to propagate value signals efficiently, potentially slowing convergence. Actor-Critic algorithms can leverage advantage estimation and policy gradients to maintain learning stability even with sparse rewards. Dense Reward Environments (e.g., HalfCheetah, Pendulum) allow both families to learn effectively, but Actor-Critic methods often achieve smoother and faster convergence due to direct policy optimization combined with value guidance. The interplay between action space type, state space complexity, and reward sparsity fundamentally shapes the suitability of each algorithm. In general:\nDiscrete + Low-dimensional + Dense reward ‚Üí Value-Based methods are competitive. Continuous + High-dimensional + Sparse or Dense reward ‚Üí Actor-Critic methods provide superior learning stability and higher final performance. These insights complement the empirical trade-offs already observed in our study, providing a more nuanced understanding of when and why certain RL algorithms excel under different environment characteristics.\nLimitations and Future Work Limitations The present project, which evaluates the performance of reinforcement learning algorithms (DQN, A2C, NAF, and SAC) in discrete (FrozenLake-v1 and MountainCar-v0) and continuous (Pendulum-v1 and HalfCheetah-v4) environments, provides valuable insights but is subject to several limitations, outlined below:\nLimited Number of Environments:\nThe study only examines four specific environments, which may not provide sufficient diversity to generalize results across all types of reinforcement learning environments. More complex environments with larger state or action spaces or different dynamics could yield different outcomes. Lack of Random Seed Variation:\nThe reported results are based on a single run or an average of a limited number of runs. Conducting multiple experiments with different random seeds could better demonstrate the robustness and reliability of the results. Focus on Specific Metrics:\nThe evaluation metrics (final reward average, convergence speed, and stability) cover only certain aspects of algorithm performance. Other metrics, such as computational efficiency, training time, or robustness to environmental noise, were not assessed. Future Work To build upon the findings of this project, which evaluated the performance of reinforcement learning algorithms (DQN, A2C, NAF, and SAC) in discrete (FrozenLake-v1, MountainCar-v0) and continuous (Pendulum-v1, HalfCheetah-v4) environments, several directions for future research and development can be pursued to address the limitations and extend the scope of the study:\nBroader Range of Environments:\nFuture work could include testing the algorithms on a wider variety of environments, such as those with larger state and action spaces, partially observable states (e.g., POMDPs), or real-world-inspired tasks. This would help validate the generalizability of the observed performance trends. Incorporating Random Seed Variations:\nConducting multiple runs with different random seeds would improve the robustness of results and allow for statistical analysis of performance variability, ensuring that conclusions are not biased by specific initial conditions. Evaluation of Additional Metrics:\nFuture studies could incorporate metrics such as computational efficiency, memory usage, training time, and robustness to environmental perturbations (e.g., noise or dynamic changes). This would provide a more holistic view of algorithm suitability for practical applications. Real-World Application: Robotics The insights gained from this project have significant potential for real-world applications, particularly in robotics, where reinforcement learning can enable autonomous systems to perform complex tasks. The following outlines how the evaluated algorithms could be applied and extended in robotics contexts:\nRobotic Manipulation:\nAlgorithms like SAC, which performed well in continuous control tasks (e.g., Pendulum-v1, HalfCheetah-v4), could be applied to robotic arms for tasks such as grasping, object manipulation, or assembly. SAC‚Äôs ability to handle continuous action spaces makes it suitable for precise control in high-dimensional settings. Autonomous Navigation:\nDiscrete action space algorithms like DQN and A2C, tested in environments like FrozenLake-v1, could be adapted for robot navigation in grid-like or structured environments (e.g., warehouse robots). A2C‚Äôs stability in stochastic settings could be particularly useful for navigating dynamic or uncertain environments. Locomotion and Mobility:\nThe success of SAC in HalfCheetah-v4 suggests its potential for controlling legged robots or humanoid robots for locomotion tasks. Future work could involve applying SAC to real-world robotic platforms to achieve robust and efficient walking or running behaviors. These future research directions and real-world applications highlight the potential to extend the current study‚Äôs findings to more diverse and practical scenarios. By addressing the identified limitations and applying the algorithms to robotics, this work can contribute to the development of more robust, efficient, and adaptable autonomous systems.\nReferences A2C (Advantage Actor-Critic):\nMnih, V., et al. (2016). \u0026ldquo;Asynchronous Methods for Deep Reinforcement Learning.\u0026rdquo; ICML 2016. Paper Stable-Baselines3 A2C Implementation: GitHub SAC (Soft Actor-Critic):\nHaarnoja, T., et al. (2018). \u0026ldquo;Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor.\u0026rdquo; ICML 2018. Paper Stable-Baselines3 SAC Implementation: GitHub DQN (Deep Q-Network):\nMnih, V., et al. (2015). \u0026ldquo;Human-level control through deep reinforcement learning.\u0026rdquo; Nature, 518(7540), 529-533. Paper Stable-Baselines3 DQN Implementation: GitHub NAF (Normalized Advantage Function):\nGu, S., et al. (2016). \u0026ldquo;Continuous Deep Q-Learning with Model-based Acceleration.\u0026rdquo; ICML 2016. Paper Gymnasium:\nOfficial Gymnasium Documentation: Gymnasium ","permalink":"http://localhost:1313/posts/actor-critic-vs-value-based-empirical-trade-offs/","summary":"This study compares Value-Based (DQN, NAF) and Actor-Critic (A2C, SAC) reinforcement learning algorithms in diverse environments (MountainCar-v0, Pendulum-v1, FrozenLake-v1, HalfCheetah-v4). Through empirical evaluation, we analyze trade-offs in convergence speed, learning stability, and final performance, supported by a user-friendly GUI and mobile application for interactive training and visualization.","title":"Actor-Critic vs. Value-Based: Empirical Trade-offs"},{"content":"Introduction This paper critically examines the assumptions commonly accepted in modeling reinforcement learning problems, suggesting that these assumptions may impede progress in the field. The authors refer to these assumptions as \u0026ldquo;dogmas.\u0026rdquo;\nDogma: A fixed, especially religious, belief or set of beliefs that people are expected to accept without any doubts. ‚ÄîFrom the Cambridge Advanced Learner\u0026rsquo;s Dictionary \u0026amp; Thesaurus\nThe paper introduces three central dogmas:\nThe Environment Spotlight Learning as Finding a Solution The Reward Hypothesis (although not exactly a dogma) The author argues the true reinforcement learning landscape is actualy like this,\nIn the words of Rich Sutton:\nRL can be viewed as a microcosm of the whole AI problem.\nHowever, today\u0026rsquo;s RL landscape is overly simplified,\nThese three dogmas are responsible for narrowing the potential of RL,\nThe authors propose that we consider moving beyond these dogmas,\nTo reclaim the true landscape of RL,\nBackground The authors reference Thomas Kuhn\u0026rsquo;s book, \u0026ldquo;The Structure of Scientific Revolutions\u0026rdquo;,\nKuhn distinguishes between two phases of scientific activity,\nNormal Science: Resembling puzzle-solving. Revolutionary Phase: Involving a fundamental rethinking of the values, methods, and commitments of science, which Kuhn calls a \u0026ldquo;paradigm.\u0026rdquo; Here\u0026rsquo;s an example of a previous paradigm shift in science:\nThe authors explore the paradigm shift needed in RL:\nDogma One: The Environment Spotlight The first dogma we call the environment spotlight, which refers to our collective focus on modeling environments and environment-centric concepts rather than agents.\nWhat do we mean when we say that we focus on environments? We suggest that it is easy to answer only one of the following two questions:\nWhat is at least one canonical mathematical model of an environment in RL?\nMDP and its variants! And we define everything in terms of it. By embracing the MDP, we are allowed to import a variety of fundamental results and algorithms that define much of our primary research objectives and pathways. For example, we know every MDP has at least one deterministic, optimal, stationary policy, and that dynamic programming can be used to identify this policy. What is at least one canonical mathematical model of an agent in RL?\nIn contrast, this question has no clear answer! The author suggests it is important to define, model, and analyse agents in addition to environments. We should build toward a canonical mathematical model of an agent that can open us to the possibility of discovering general laws governing agents (if they exist).\nDogma Two: Learning as Finding a Solution The second dogma is embedded in the way we treat the concept of learning. We tend to view learning as a finite process involving the search for‚Äîand eventual discovery of‚Äîa solution to a given task.\nWe tend to implicitly assume that the learning agents we design will eventually find a solution to the task at hand, at which point learning can cease. Such agents can be understood as searching through a space of representable functions that captures the possible action-selection strategies available to an agent, similar to the Problem Space Hypothesis, and, critically, this space contains at least one function‚Äîsuch as the optimal policy of an MDP‚Äîthat is of sufficient quality to consider the task of interested solved. Often, we are then interested in designing learning agents that are guaranteed to converge to such an endpoint, at which point the agent can stop its search (and thus, stop its learning).\nThe author suggests to embrace the view that learning can also be treated as adaptation. As a consequence, our focus will drift away from optimality and toward a version of the RL problem in which agents continually improve, rather than focus on agents that are trying to solve a specific problem.\nWhen we move away from optimality,\nHow do we think about evaluation? How, precisely, can we define this form of learning, and differentiate it from others? What are the basic algorithmic building blocks that carry out this form of learning, and how are they different from the algorithms we use today? Do our standard analysis tools such as regret and sample complexity still apply? These questions are important, and require reorienting around this alternate view of learning.\nThe authors introduce the book \u0026ldquo;Finite and Infinite Games\u0026rdquo;,\nAnd the concept of Finite and Infinite Games is summarized in the following quote,\nThere are at least two kinds of games, One could be called finite; the other infinite. A finite game is played for the purpose of winning, an infinite game for the purpose of continuing the play.\nAnd argues alignment is an infinite game.\nDogma Three: The Reward Hypothesis The third dogma is the reward hypothesis, which states \u0026ldquo;All of what we mean by goals and purposes can be well thought of as maximization of the expected value of the cumulative sum of a received scalar signal (reward).\u0026rdquo;\nThe authors argue that the reward hypothesis is not truly a dogma. Nevertheless, it is crucial to understand its nuances as we continue to design intelligent agents.\nThe reward hypothesis basically says,\nIn recent analysis by [2] fully characterizes the implicit conditions required for the hypothesis to be true. These conditions come in two forms. First, [2] provide a pair of interpretative assumptions that clarify what it would mean for the reward hypothesis to be true or false‚Äîroughly, these amount to saying two things (brwon doors).\nFirst, that \u0026ldquo;goals and purposes\u0026rdquo; can be understood in terms of a preference relation on possible outcomes. Second, that a reward function captures these preferences if the ordering over agents induced by value functions matches that of the ordering induced by preference on agent outcomes. This leads to the following conjecture,\nThen, under this interpretation, a Markov reward function exists to capture a preference relation if and only if the preference relation satisfies the four von Neumann-Morgenstern axioms, and a fifth Bowling et al. call $\\gamma$-Temporal Indifference.\nAxiom 1: Completeness \u0026gt; You have a preference between every outcome pair.\nYou can always compare any two choices. Axiom 2: Transitivity \u0026gt; No preference cycles.\nIf you like chocolate more than vanilla, and vanilla more than strawberry, you must like chocolate more than strawberry. Axiom 3: Independence \u0026gt; Independent alternatives can\u0026rsquo;t change your preference.\nIf you like pizza more than salad, and you have to choose between a lottery of pizza or ice cream and a lottery of salad or ice cream, you should still prefer the pizza lottery over the salad lottery. Axiom 4: Continuity \u0026gt; There is always a break even chance.\nImagine you like a 100 dollar bill more than a 50 dollar bill, and a 50 dollar bill more than a 1 dollar bill. There should be a scenario where getting a chance at 100 dollar and 1 dollar, with certain probabilities, is equally good as getting the 50 dollar for sure. These 4 axioms are called the von Neumann-Morgenstern axioms.\nAxiom 5: Temporal $\\boldsymbol{\\gamma}$-Indifference \u0026gt; Discounting is consistent throughout time. Temporal $\\gamma$-indifference says that if you are indifferent between receiving a reward at time $t$ and receiving the same reward at time $t+1$, then your preference should not change if we move both time points by the same amount. For instance, if you don\u0026rsquo;t care whether you get a candy today or tomorrow, then you should also not care whether you get the candy next week or the week after. Taking these axioms into account, the reward conjecture becomes the reward theorem,\nIt is essential to consider that people do not always conform to these axioms, and human preferences can vary.\nIt is important that we are aware of the implicit restrictions we are placing on the viable goals and purposes under consideration when we represent a goal or purpose through a reward signal. We should become familiar with the requirements imposed by the five axioms, and be aware of what specifically we might be giving up when we choose to write down a reward function.\nSee Also David Abel Presentation @ ICML 2023 David Abel Personal Website Mark Ho Personal Website Anna Harutyunyan Personal Website References [1] Abel, David, Mark K. Ho, and Anna Harutyunyan. \u0026ldquo;Three Dogmas of Reinforcement Learning.\u0026rdquo; arXiv preprint arXiv:2407.10583 (2024).\n[2] Bowling, Michael, et al. \u0026ldquo;Settling the reward hypothesis.\u0026rdquo; International Conference on Machine Learning. PMLR, 2023.\n","permalink":"http://localhost:1313/posts/three-dogmas-of-reinforcement-learning/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eThis paper critically examines the assumptions commonly accepted in modeling reinforcement learning problems, suggesting that these assumptions may impede progress in the field. The authors refer to these assumptions as \u0026ldquo;dogmas.\u0026rdquo;\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003eDogma: A fixed, especially religious, belief or set of beliefs that people are expected to accept without any doubts.\u003c/strong\u003e \u003cem\u003e‚ÄîFrom the \u003ca href=\"https://dictionary.cambridge.org/dictionary/english\"\u003eCambridge Advanced Learner\u0026rsquo;s Dictionary \u0026amp; Thesaurus\u003c/a\u003e\u003c/em\u003e\u003c/p\u003e\u003c/blockquote\u003e\n\u003cp\u003eThe paper introduces three central dogmas:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eThe Environment Spotlight\u003c/li\u003e\n\u003cli\u003eLearning as Finding a Solution\u003c/li\u003e\n\u003cli\u003eThe Reward Hypothesis (although not exactly a dogma)\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eThe author argues the true reinforcement learning landscape is actualy like this,\u003c/p\u003e","title":"Three Dogmas of Reinforcement Learning"},{"content":"Introduction This paper studies the problem of zero-shot generalization in reinforcement learning and introduces an algorithm that trains an ensamble of maximum reward seeking agents and an maximum entropy agent. At test time, either the ensemble agrees on an action, and we generalize well, or we take exploratory actions with the help of maximum entropy agent and drive us to a novel part of the state space, where the ensemble may potentially agree again. This method combined with an invariance based approach achieves new state-of-the-art results on ProcGen.\nBackground I know it\u0026rsquo;s a lot to take in! You may be wondering:\nWhat is reinforcement learning? ü•∫ What generalization means for RL? ü•≤ What is zero-shot generalization? ü•π What are max reward and max entropy agents?! ‚òπÔ∏è What is an ensamble of them?!! üòü What is an invariance based approach? üòì And what the heck is ProcGen?!!! üò† Don\u0026rsquo;t worry! We are going to cover all of that and more! And you are going to fully understand this paper and finish reading this article with an smile üôÇ!\nWhat is reinforcement learning? Reinforcement learning is learning what to do‚Äîhow to map situations to actions‚Äîso as to maximize a numerical reward signal. Imagine yourself right now, in reinforcement learning terms, you are an agent and everything that is not you, is your environment. You perceive the world through your senses (e.g., eyes, ears, etc.) and what you perceive turns into electrical signals that your brain processes to form an understanding of your surroundings (state). Based on this understanding, you make decisions (actions) with the goal of achieving the best possible outcome for yourself (reward).\nIn a more formal sense, reinforcement learning involves the following components:\nAgent: The learner or decision maker (e.g., you). Environment: Everything the agent interacts with (e.g., the world around you). State ($s$): A representation of the current situation of the agent within the environment (e.g., what you see, hear, and feel at any given moment). Actions ($a$): The set of all possible moves the agent can make (e.g., moving your hand, walking, speaking). Reward ($r$): The feedback received from the environment in response to the agent‚Äôs action (e.g., pleasure from eating food, pain from touching a hot surface). Policy ($\\pi$): A strategy used by the agent to decide which actions to take based on the current state (e.g., your habits and decision-making processes). Value Function ($V$): A function that estimates the expected cumulative reward of being in a certain state and following a particular policy (e.g., your prediction of future happiness based on current actions). The objective of the agent is to develop a policy that maximizes the total cumulative reward over time. This is typically achieved through a process of exploration (trying out new actions to discover their effects) and exploitation (using known actions that yield high rewards).\nIn mathematical terms, the goal is to find a policy $\\pi$ that maximizes the expected return $G_t$, which is the cumulative sum of discounted rewards:\n$$ G_t = r_{t+1} + \\gamma r_{t+2} + \\gamma^2 r_{t+3} + \\cdots $$\nwhere $\\gamma$ (0 ‚â§ $\\gamma$ \u0026lt; 1) is the discount factor that determines the importance of future rewards.\nFor a more thorough and in depth explanation of reinforcement learning please refer to Lilian Weng excelent blog post A (Long) Peek into Reinforcement Learning.\nWhat generalization means for RL? In reinforcement learning, generalization involves an agent\u0026rsquo;s ability to apply learned policies or value functions to new states or environments that it has not encountered during training. This is essential because it is often impractical or impossible to train an agent on every possible state it might encounter.\nThere are bunch of methods that researchers have used to tackle the problem of generalization in reinforcement learning that are summarized in the following diagram.\nThis paper focuses on RL-Specific solutions and specifically exploration technique to tackle the problem of generalization. If you\u0026rsquo;re interested to learn more about the generalization problem in reinforcement learning please refer to reference [3].\nWhat is zero-shot generalization? Zero-shot generalization in RL refers to the ability of an agent to perform well in entirely new environments or tasks without any prior specific training or fine-tuning on those environments or tasks. This is a significant challenge because it requires the agent to leverage its learned representations and policies in a highly flexible and adaptive manner.\nIn order to define the objective of zero-shot generalization we first have to define what MDPs and POMDPs are.\nMarkov Decision Process (MDP):\nMDP is a mathematical framework used to describe an environment in reinforcement learning where the outcome is partly random and partly under the control of a decision-maker (agent). A MDP is a tuple $M = (S, A, P_{init}, P, r, \\gamma)$ where,\nStates ($S \\in \\mathbb{R}^{|S|}$): A finite set of states that describe all possible situations in which the agent can be. Actions ($A \\in \\mathbb{R}^{|A|}$): A finite set of actions available to the agent. Initial State Distribution ($P_{init}$): A distribution of starting state $(s_0 \\sim P_{init})$. Transition Probability ($P$): A function $P(s_{t+1}, s_t, a_t)$ representing the probability of transitioning from state $s_t$ to state $s_{t+1}$ after taking action $a_t$ $(s_{t+1} \\sim P(.|s_t,a_t))$. Reward ($r: S \\times A \\rightarrow \\mathbb{R}$): A function $r(s_t, a_t)$ representing the immediate reward $r_t$ received after transitioning from state $s_t$ due to action $a_t$ $(r_t = r(s_t, a_t))$. Discount Factor ($\\gamma$): $(0 \\leq \\gamma \u0026lt; 1)$ is a constant that determines the importance of future rewards. Partially Observable Markov Decision Process (POMDP):\nPOMDP extends MDPs to situations where the agent does not have complete information about the current state. Instead, the agent must make decisions based on partial observations. A POMDP is a tuple $M = (S, A, O, P_{init}, P, \\Sigma, r, \\gamma)$ where other that above definitions for MDP,\nObservation Space ($O$): A finite set of observations the agent can receive about the state. Observation Function ($\\Sigma$): A function that given current state $s_t$ and current action $a_t$ gives us the current observation $o_t$ $(o_t = \\Sigma(s_t, a_t) \\in O)$. If we set $O = S$ and $\\Sigma(s,a) = s$, the POMDP turns into regular MDP.\nLet the history at time $t$ be,\n$$ h_t = \\{ o_0, a_0, r_0, o_1, a_1, r_1, \\dots, o_t \\} $$\nThe agent‚Äôs next action is outlined by a policy $\\pi$, which is a stochastic mapping from the history to an action probability,\n$$ \\pi(a|h_t) = P(a_t=a|h_t) $$\nIn this formulation, a history-dependent policy (and not a Markov policy) is required both due to partially observed states, epistemic uncertainty, and also for optimal maxEnt exploration.\nWe assume a prior distribution over POMDPs $P(M)$, defined over some space of POMDPs. For a given POMDP, an optimal policy maximizes the expected discounted return,\n$$ \\mathbb{E}_{\\pi,M} \\left[ \\sum _{t=0}^{\\infty} \\gamma^t r(s_t,a_t) \\right] $$\nWhere the expectation is taken over the policy $\\pi(h_t)$, and the state transition probability $s_t \\sim P$ of POMDP $M$.\nOur generalization objective is to maximize the discounted cumulative reward taken in expectation over the POMDP prior,\n$$ \\mathcal{R}_ {pop}(\\pi) = \\mathbb{E}_ {M \\sim P(M)} {\\left[ \\mathbb{E}_{\\pi,M} {\\left[ \\sum _{t=0}^{\\infty} \\gamma^t r(s_t,a_t) \\right]} \\right]} $$\nSeeking a policy that performs well in expectation over any POMDP from the prior corresponds to zero-shot generalization.\nAnd as you may have guessed we don\u0026rsquo;t have access to true prior distribution of POMDPs so we have to estimate it with $N$ training POMDPs $M_1, M_2, \\dots, M_N$ sampled from the true prior distribution $P(M)$. So we are going to maximize empirical discounted cumulative reward,\n$$ \\begin{align*} \\mathcal{R}_ {emp}(\\pi) \u0026amp;= \\frac{1}{N} \\sum _ {i=1}^{N} \\mathbb{E} _ {\\pi,M_i} \\left[ \\sum _ {t=0}^{\\infty} \\gamma^t r(s_t,a_t) \\right] \\\\ \u0026amp;= \\mathbb{E}_ {M \\sim \\hat{P}(M)} {\\left[ \\mathbb{E}_{\\pi,M} {\\left[ \\sum _{t=0}^{\\infty} \\gamma^t r(s_t,a_t) \\right]} \\right]} \\end{align*} $$\nWhere the empirical POMDP distribution $\\hat{P}(M)$ can be different from the true distribution, i.e. $\\hat{P}(M) \\neq P(M)$. In general, a policy that optimizes the empirical reward may perform poorly on the population reward and this is known as overfitting in statistical learning theory.\nWhat are max reward and max entropy agents?! As we have seen, the goal of agents in reinforcement learning is to find a policy $\\pi$ that maximizes the expected discounted return by focusing on actions that lead to the greatest immediate or future rewards. We call these common RL agents \u0026ldquo;max reward agents\u0026rdquo; in this paper. On the other hand, \u0026ldquo;max entropy agents\u0026rdquo; aim to maximize the entropy of the policy for visiting different states. Maximizing entropy encourages the agent to explore a wider range of actions that lead the agent to visit new states even when they don\u0026rsquo;t contribute any reward.\nThis type of agent will help us to make decisions when we have epistemic uncertainty about what to do at test time. Epistemic uncertainty basically means the uncertainty that we have because of our lack of knowledge and can be improved by gathering more information about the situation.\nThe insight of the authors of this paper is that learning a policy that effectively explores the domain is harder to memorize than a policy that maximizes reward for a specific task, and therefore they expect such learned behavior to generalize well.\nWhat is an ensamble of them?!! Ensembling in reinforcement learning involves combining the policies of multiple individual agents to make more accurate and robust decisions. The idea is that by aggregating the outputs of different agents, we can leverage their diverse perspectives and expertise to improve overall performance.\nThis paper uses an ensamble of max reward agents to help the agent decide on the overal epistemic uncetainty that we have at test time.\nWhat is an invariance based approach? Invariance based algorithms in reinforcement learning focus on developing policies that are robust to changes and variations in the environment. These algorithms aim to identify and leverage invariant features or patterns that remain consistent across different environments or tasks. The goal is to ensure that the learned policy performs well not just in the training environment but also in new, unseen environments.\nThis paper uses IDAAC algorithm which is a special kind of DAAC algorithm as its invariance based algorithm.\nDAAC (Decoupled Advantage Actor-Critic) uses two separate networks, one for learning the policy and advantage, and one for learning the value. The value estimates are used to compute the advantage targets.\nIDAAC (Invariant Decoupled Advantage Actor-Critic) adds an additional regularizer to the DAAC policy encoder to ensure that it does not contain episode-specific information. The encoder is trained adversarially with a discriminator so that it cannot classify which observation from a given pair $(s_i, s_j)$ was first in a trajectory.\nFor more information about IDAAC algorithm please refer to reference [4].\nWhat the heck is ProcGen?!!! Procgen is a benchmark suite for evaluating the generalization capabilities of reinforcement learning agents. It was developed by OpenAI and consists of a collection of procedurally generated environments that vary in terms of visual appearance, dynamics, and difficulty. The goal of Procgen is to provide a standardized and challenging set of environments that can be used to assess the ability of RL algorithms to generalize to unseen scenarios.\nIf you are new to reinforcement learning, I know these explanations are a lot to take in! If you find yourself lost, I recommend to check out the following courses at your leisure:\nDeep Reinforcement Learning (by Hugging Face ü§ó) Reinforcement Learning (by Mutual Information) Introduction to Reinforcement Learning (by David Silver) Reinforcement Learning (by Michael Littman \u0026amp; Charles Isbell) Reinforcement Learning (by Emma Brunskill) Deep Reinforcement Learning Bootcamp 2017 Foundations of Deep RL (by Pieter Abbeel) Deep Reinforcement Learning \u0026amp; Control (by Katerina Fragkiadaki) Deep Reinforcement Learning (by Sergey Levine) After reviewing all this we can focus on the rest of the paper!\nHidden Maze Experiment One of the key observation of the authors of this paper is that invariance is not enough for zero-shot generalization of reinforcemen learning algorithm. They designed the hidden maze experiment too demonstrate that. Imagine Maze, but with the walls and goal hidden in the observation. Arguably, this is the most task-invariant observation possible, such that a solution can still be obtained in a reasonable time.\nAn agent with memory can be trained to optimally solve all training tasks: figuring out wall positions by trying to move ahead and observing the resulting motion, and identifying based on its movement history in which training maze it is currently in. Obviously, such a strategy will not generalize to test mazes. Performance in Maze, where the strategy for solving any particular training task must be indicative of that task, has largely not improved by methods based on invariance\nThe following figure shows PPO performance on the hidden maze task, indicating severe overfitting.\nAs described by [5], an agent can overcome test-time errors in its policy by treating the perfect policy as an unobserved variable. The resulting decision making problem, termed the epistemic POMDP, may require some exploration at test time to resolve uncertainty. The article further proposed the LEEP algorithm based on this principle, which trains an ensemble of agents and essentially chooses randomly between the members when the ensemble does not agree, and was the first method to present substantial generalization improvement on Maze.\nIn this paper authors extend this idea and asked, How to improve exploration at test time?, and their approach is based on a novel discovery, when they train an agent to explore the training domains using a maximum entropy objective, they observe that the learned exploration behavior generalizes surprisingly well (much better than the generalization attained when training the agent to maximize reward).\nIn the following section we gonna dig deep into internals of maximum entropy policy.\nMaxEnt Policy For simplicity the authors discuss this part for the MDP case. A policy $\\pi$, through its interaction with an MDP, induces a t-step state distribution over the state space $S$,\n$$ d _ {t,\\pi} (s) = p(s_t=s | \\pi) $$\nThe objective of maximum entropy exploration is given by:\n$$ \\mathcal{H}(d(.)) = -\\mathbb{E} _ {s \\sim d} \\left[ \\log{d(s)} \\right] $$\nWhere $d$ can be regarded as either,\nStationary state distribution (infinite horizon): $d _ {\\pi} = \\lim _ {t \\rightarrow \\infty} d _ {t,\\pi} (s)$ Discounted state distribution (infinite horizon): $d _ {\\gamma, \\pi} = (1-\\gamma) \\sum _ {t=0} ^ {\\infty} \\gamma^t d _ {t,\\pi} (s)$ Marginal state distribution (finite horizon): $d _ {T, \\pi} = \\frac{1}{T} \\sum _ {t=0} ^ {T} d _ {t,\\pi} (s)$ In this work they focus on the finite horizon setting and adapt the marginal state distribution $d _ {T, \\pi}$ in which $T$ equals the episode horizon $H$, so we seek to maximize the objective:\n$$ \\begin{align*} \\mathcal{R} _ {\\mathcal{H}} (\\pi) \u0026amp;= \\mathbb{E} _ {M \\sim \\hat{P}(M)} \\left[ \\mathcal{H}(d _ {H,\\pi}) \\right] \\\\ \u0026amp;=\\mathbb{E} _ {M \\sim \\hat{P}(M)} \\left[ \\mathcal{H}(\\frac{1}{H} \\sum _ {t=0} ^ {H} d _ {t,\\pi} (s)) \\right] \\end{align*} $$\nwhich yields a policy that \u0026ldquo;equally\u0026rdquo; visits all states during the episode.\nTo maximize this objective we can estimating the density of the agent\u0026rsquo;s state visitation distribution, but in this paper the authors adapt the non-parametric entropy estimation approach; we estimate the entropy using the particle based k-nearest neighbor (k-NN estimator).\nTo estimate the distribution $d _ {H,\\pi}$ over the states $S$, we consider each trajectory as $H$ samples of states $\\{ s_t \\} _ {t=1} ^ {H}$ and take $s _ t ^ {\\text{k-NN}}$ to be the k-NN of the state $s_t$ within the trajectory,\n$$ \\hat{ \\mathcal{H} } ^ {k,H} (d _ {H,\\pi}) \\approx \\frac{1}{H} \\sum _ {t=1} ^ {H} \\log ( \\parallel s_t - s_t^{\\text{k-NN}} \\parallel _ 2) $$\nIn which we define intrinsic reward function as,\n$$ r_I (s_t) \\coloneqq \\log ( \\parallel s_t - s_t^{\\text{k-NN}} \\parallel _ 2) $$\nThis formulation enables us to deploy any RL algorithm to approximately optimize objective.\nSpecifically, in this work we use the policy gradient algorithm PPOŸà where at every time step $t$, the state $s_t^{\\text{k-NN}}$ is chosen from previous states $\\{ s_t \\} _ {t=1} ^ {t-1}$ of the same episode. To improve computational efficiency, instead of taking the full observation as the state (64 x 64 RGB image), we sub-sample the observation by applying average pooling of 3 x 3 to produce an image of size 21 x 21.\nWe found that agents trained for maximum entropy exploration exhibit a smaller generalization gap compared with the standard approach of training solely with extrinsic reward. The policies are equipped with a memory unit (GRU) to allow learning of deterministic policies that maximize the entropy.\nIn all three environments, we demonstrate a small generalization gap, as test performance on unseen levels closely follows the performance achieved during training.\nIn addition, we verify that the train results are near optimal by comparing with a hand designed approximately optimal exploration policy. For example, on Maze we use the well known maze exploring strategy wall follower, also known as the left/right-hand rule.\nExpGen Algorithm Our main insight is that, given the generalization property of the entropy maximization policy established above, an agent can apply this behavior in a test MDP and expect effective exploration at test time. We pair this insight with the epistemic POMDP idea, and propose to play the exploration policy when the agent faces epistemic uncertainty, hopefully driving the agent to a different state where the reward-seeking policy is more certain.\nOur framework comprises two parts: an entropy maximizing network and an ensemble of networks that maximize an extrinsic reward to evaluate epistemic uncertainty. The first step entails training a network equipped with a memory unit to obtain a maxEnt policy $\\pi_H$ that maximizes entropy. Next, we train an ensemble of memory-less policy networks $\\{ \\pi _ r ^ j \\} _ {j=1} ^ {m} $ to maximize extrinsic reward.\nHere is the ExpGen algorithm,\nWe consider domains with a finite action space, and say that the policy $\\pi _ r ^ i$ is certain at state $s$ if its action $a_i \\sim \\pi _ r ^ i (a|s)$ is in consensus with the ensemble: $a_i = a_j$ for the majority of $k$ out of $m$, where $k$ is a hyperparameter of our algorithm.\nSwitching between two policies may result in a case where the agent repeatedly toggles between two states (if, say, the maxEnt policy takes the agent from state $s_1$ to a state $s_2$, where the ensemble agrees on an action that again moves to state $s_1$.). To avoid such ‚Äúmeta-stable‚Äù behavior, we randomly choose the number of maxEnt steps $n_{\\pi_{\\mathcal{H}}}$ from a Geometric distribution, $n_{\\pi_{\\mathcal{H}}} \\sim Geom(\\alpha)$.\nExperiments Our experimental setup follows ProcGen\u0026rsquo;s easy configuration, wherein agents are trained on 200 levels for 25M steps and subsequently tested on random levels. All agents are implemented using the IMPALA (Importance Weighted Actor-Learner Architectures) convolutional architecture, and trained using PPO or IDAAC. For the maximum entropy agent $\\pi_H$ we incorporate a single GRU at the final embedding of the IMPALA convolutional architecture. For all games, we use the same parameter $\\alpha=0.5$ of the Geometric distribution and form an ensemble of 10 networks.\nFollowing figure is comparison across all ProcGen games, with 95% bootstrap CIs highlighted in color. Score distributions of ExpGen, PPO, PLR, UCB-DrAC, PPG and IDAAC.\nFollowing figure shows in each row, the probability of algorithm X outperforming algorithm Y. The comparison illustrates the superiority of ExpGen over the leading contender IDAAC with probability 0.6, as well as over other methods with even higher probability.\nSee Also PyTorch implementation of ExpGen @ GitHub Ev Zisselman Presentation @ NeurIPS 2023 ExpGen Rebuttal Process @ OpenReview ExpGen Poster for NeurIPS 2023 References [1] Zisselman, Ev, et al. \u0026ldquo;Explore to generalize in zero-shot rl.\u0026rdquo; Advances in Neural Information Processing Systems 36 (2024).\n[2] Sutton, Richard S., and Andrew G. Barto. \u0026ldquo;Reinforcement learning: An introduction.\u0026rdquo; MIT press, (2020).\n[3] Kirk, Robert, et al. \u0026ldquo;A survey of zero-shot generalisation in deep reinforcement learning.\u0026rdquo; Journal of Artificial Intelligence Research 76 (2023): 201-264.\n[4] Raileanu, Roberta, and Rob Fergus. \u0026ldquo;Decoupling value and policy for generalization in reinforcement learning.\u0026rdquo; International Conference on Machine Learning. PMLR, 2021.\n[5] Ghosh, Dibya, et al. \u0026ldquo;Why generalization in rl is difficult: Epistemic pomdps and implicit partial observability.\u0026rdquo; Advances in neural information processing systems 34 (2021): 25502-25515.\n[6] Espeholt, Lasse, et al. \u0026ldquo;Impala: Scalable distributed deep-rl with importance weighted actor-learner architectures.\u0026rdquo; International conference on machine learning. PMLR, 2018.\n","permalink":"http://localhost:1313/posts/explore-to-generalize-in-zero-shot-rl/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eThis paper studies the problem of zero-shot generalization in reinforcement learning and introduces an algorithm that trains an ensamble of maximum reward seeking agents and an maximum entropy agent. At test time, either the ensemble agrees on an action, and we generalize well, or we take exploratory actions with the help of maximum entropy agent and drive us to a novel part of the state space, where the ensemble may potentially agree again. This method combined with an invariance based approach achieves new state-of-the-art results on ProcGen.\u003c/p\u003e","title":"ExpGen: Explore to Generalize in Zero-Shot RL"},{"content":"Welcome to our blog! We are a team of enthusiastic professors and students from Sharif University of Technology who are eager to share our insights and passion for this fascinating field. Our aim is to share our knowledge and excitement about this cutting-edge field with you ‚ù§Ô∏è.\nProfessors Professor Mohammad Hossein Rohban Professor Ehsaneddin Asgari Students Arash Alikhani Alireza Nobakht Labs RIML Lab NLP \u0026amp; DH Lab We hope you enjoy our blog and find our content both informative and inspiring üöÄ!\n","permalink":"http://localhost:1313/us/","summary":"\u003cp\u003eWelcome to our blog! We are a team of enthusiastic professors and students from Sharif University of Technology who are eager to share our insights and passion for this fascinating field. Our aim is to share our knowledge and excitement about this cutting-edge field with you ‚ù§Ô∏è.\u003c/p\u003e\n\u003ch2 id=\"professors\"\u003eProfessors\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eProfessor \u003ca href=\"https://www.linkedin.com/in/mohammad-hossein-rohban-75567677\"\u003eMohammad Hossein Rohban\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003eProfessor \u003ca href=\"https://www.linkedin.com/in/ehsaneddinasgari\"\u003eEhsaneddin Asgari\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"students\"\u003eStudents\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://www.linkedin.com/in/infinity2357\"\u003eArash Alikhani\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://www.linkedin.com/in/alireza-nobakht\"\u003eAlireza Nobakht\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"labs\"\u003eLabs\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/rohban-lab\"\u003eRIML Lab\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/language-ml\"\u003eNLP \u0026amp; DH Lab\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eWe hope you enjoy our blog and find our content both informative and inspiring üöÄ!\u003c/p\u003e","title":""},{"content":"Adversarial Attacks on Reinforcement Learning Policies Reinforcement Learning (RL) has powered breakthroughs in games, robotics, and autonomous systems. But just like image classifiers can be fooled by carefully crafted adversarial examples, RL agents are also vulnerable to adversarial attacks. These attacks can manipulate the agent‚Äôs perception of the environment or its decision process, leading to unsafe or suboptimal behavior.\nWhat Do Adversarial Attacks Mean in RL? In RL, an agent interacts with an environment, observes a state, takes an action, and receives a reward. An adversarial attack interferes with this process‚Äîoften by perturbing the input observations‚Äîso the agent chooses actions that look reasonable to it but are actually harmful.\nFor example:\nA self-driving car policy might mistake a stop sign for a speed-limit sign if subtle noise is added to the camera input. A robot trained to walk could be tripped by slight modifications to its sensor readings. Types of Attacks in Reinforcement Learning Researchers have identified several ways adversarial attacks can target RL policies. These attack vectors include:\nObservation Perturbation\nThe most widely studied form of attack: small, carefully crafted changes are made to the agent‚Äôs observations so it perceives the environment incorrectly.\nExample: Adding imperceptible noise to frames in an Atari game so the agent misjudges its next move.\nCommunication Perturbation\nIn multi-agent systems, communication messages can be perturbed, leading to miscoordination.\nExample: Two drones sharing location data receive slightly altered coordinates, causing a collision.\nMalicious Communications\nBeyond perturbations, adversaries may inject entirely fake or deceptive messages.\nExample: An attacker sends a false signal about enemy positions in a cooperative strategy game.\nWhy It Matters Adversarial attacks highlight the gap between high-performing RL policies in controlled benchmarks and their reliability in the real world. Understanding and mitigating these vulnerabilities is essential if we want RL to be trusted in safety-critical domains like autonomous driving, robotics, and healthcare.\nHow Vulnerable Are RL Policies to Adversarial Attacks? Reinforcement Learning (RL) has made huge strides in recent years, powering systems that can beat human champions in games and control robots with precision. But just like image classifiers can be fooled by imperceptible changes to inputs, RL policies are also highly vulnerable to adversarial attacks\nThe researchers show that even tiny perturbations‚Äîso small they are invisible to humans‚Äîcan cause RL agents to fail dramatically at test time. Using Atari games as a testbed, they evaluate three common deep RL algorithms: DQN, TRPO, and A3C. The results are clear: all three are susceptible, but DQN policies are especially vulnerable.\nWhite-Box Attacks with FGSM In white-box scenarios, where the adversary has full access to the policy and gradients, attacks are devastating. The team applies the Fast Gradient Sign Method (FGSM) to craft adversarial examples and finds:\nAn ‚Ñì‚àû-norm perturbation with Œµ = 0.001 can slash performance by over 50%. ‚Ñì1-norm attacks are even more powerful‚Äîby changing just a handful of pixels significantly, they can cripple the agent‚Äôs performance. Even policies trained with robust algorithms like TRPO and A3C experience sharp drops when faced with these attacks.\nBlack-Box Attacks and Transferability What if the adversary doesn‚Äôt have full access to the policy network? Surprisingly, attacks are still effective. In black-box settings, adversaries exploit the property of transferability:\nAdversarial examples created for one policy often transfer to another policy trained on the same task. Transferability also extends across algorithms‚Äîfor example, attacks generated against a DQN policy can still reduce the performance of an A3C policy. Effectiveness decreases with less knowledge, but performance still degrades significantly, especially with ‚Ñì1 attacks. Why This Matters The key takeaway is that adversarial examples are not just a computer vision problem. RL policies‚Äîdespite achieving high scores in training‚Äîcan be undermined by imperceptible perturbations. This fragility is dangerous for real-world applications like autonomous driving and robotics, where safety and reliability are non-negotiable.\nUniversal Adversarial Preservation (UAP) To ground these ideas, I re-implemented the 2017 Adversarial Attacks on Neural Network Policies setup (since there was no offical impelmentation) and trained a DQN agent on Atari Pong. After validating the baseline attacks, I implemented Universal Adversarial Perturbations (UAPs) and found that a single, fixed perturbation‚Äîcomputed once and then applied it to all observations, which was enough to consistently derail the policy across episodes and random seeds, without recomputing noise at every timestep. In other words, the attack generalized over time and trajectories, confirming that UAPs exploit stable perceptual quirks of the learned policy rather than moment-by-moment gradients. Practically, this feels much closer to a real-world threat model: an attacker only needs to tamper with the sensor once (think a sticker/overlay on the lens) instead of having high-bandwidth, per-step access to the system. Below you can see the plot of rewards vs Œµ bugdet and videos of different setups.\nbaselines\u0026rsquo; reward over different values of Œµ budget.\nClean ‚Äî original episode (no perturbation). Random uniform noise (Œµ = 2.0). FGSM (Œµ = 2.0) ‚Äî white-box attack. PGD (Œµ = 2.0) ‚Äî iterative, white-box attack. UAP (Œµ = 2.0) ‚Äî image-agnostic. Adversarial Policies: when weird opponents break strong RL TL;DR. Instead of adding pixel noise to an RL agent‚Äôs input, this paper shows you can train a policy that acts in the shared world to induce natural but adversarial observations for the victim‚Äîcausing robust, self-play‚Äìtrained agents to fail in zero-sum MuJoCo games. Fine-tuning helps‚Ä¶until a new adversary is learned.\nThe threat model: natural observations as the ‚Äúperturbation‚Äù In multi-agent settings, an attacker typically can‚Äôt flip pixels or edit state vectors. But it can choose actions that make the victim see carefully crafted, physically plausible observations. Hold the victim fixed and the two-player game becomes a single-agent MDP for the attacker, who learns a policy that elicits bad actions from the victim.\nQuick look:\nYou Shall Not Pass Kick \u0026amp; Defend Sumo (Human) Masked victim vs adversary Setup in a nutshell Victims: strong self-play policies (‚Äúagent zoo‚Äù) across four MuJoCo tasks: Kick \u0026amp; Defend, You Shall Not Pass, Sumo Humans, Sumo Ants. Attacker: trained with PPO for ~20M timesteps‚Äî\u0026lt; 3% of the 680‚Äì1360M timesteps used for the victims‚Äîyet reliably wins. Key idea: adversaries don‚Äôt become great players; they learn poses/motions that generate adversarial observations for the victim. Figures\nTasks used for evaluation.\nAdversary win rate rises quickly despite far fewer timesteps.\nWhat the learned adversary looks like (and why that matters) In Kick \u0026amp; Defend and YSNP, the adversary may never stand up‚Äîit finds contorted, stable poses that make the victim mis-act. In Sumo Humans, where falling loses immediately, it adopts a kneeling/stable stance that still provokes the victim to fall.\nQualitative behaviors: the ‚Äúpoint‚Äù is to confuse, not to excel at the nominal task.\nMasking test: evidence the attack is observational If wins come from manipulating what the victim sees, then hiding the adversary‚Äôs pose from the victim should help. That‚Äôs exactly what happens:\nAgainst normal opponents, the masked victim is (unsurprisingly) worse. Against the adversary, the masked victim becomes nearly immune (e.g., in YSNP: normal victim loses often; masked victim flips the outcome and wins almost always). Masking the adversary‚Äôs position removes the observation channel the attack exploits.\nDimensionality matters Victims are more vulnerable when more opponent DOFs are observed. The attack is stronger in Humanoid (higher-dimensional observed joints) than Ant (lower-dimensional). More controllable joints ‚Üí more ways to steer the victim off-distribution.\nHigher observed dimensionality correlates with higher adversary win rates.\nWhy it works: off-distribution activations Analyses of the victim‚Äôs network show adversarial opponents push internal activations farther from the training manifold than random or lifeless baselines.\nAdversarial policies drive ‚Äúweird‚Äù activations‚Äîmore off-distribution than simple OOD baselines.\nDefenses (and their limits) Fine-tuning the victim on the discovered adversary reduces that adversary‚Äôs success (often down to ~10% in YSNP), but:\nCatastrophic forgetting: performance vs normal opponents degrades (single-adversary fine-tune is worst; dual fine-tune helps but still regresses). Arms race: re-running the attack against the fine-tuned victim yields a new adversary that succeeds again‚Äîoften via a different failure mode (e.g., tripping rather than pure confusion). Before fine-tune After fine-tune (this adversary) Win-rate grid before/after fine-tuning against normal opponents and adversaries.\nTakeaways for practitioners Threat model upgrade: in multi-agent worlds, your attack surface includes other policies that craft natural observations‚Äîno pixel hacks needed. Exploitability check: training a targeted adversary lower-bounds your policy‚Äôs worst-case performance and reveals failure modes missed by self-play. Defense needs diversity: fine-tuning on a single adversary overfits. Prefer population-based or curriculum defenses that rotate diverse opponents and maintain competence vs normals. Robust Communicative Multi-Agent Reinforcement Learning with Active Defense By Yu et al., AAAI 2024\nüåê Why Communication Matters in Multi-Agent RL In multi-agent reinforcement learning (MARL), agents often face partial observability ‚Äî no single agent sees the full environment. To cooperate effectively, agents need to communicate, sharing information about what they see and what actions to take.\nThis communication has powered applications such as robot navigation and traffic light control.\nBut there‚Äôs a catch: in the real world, communication channels are noisy and vulnerable to adversarial attacks. If attackers tamper with even a few messages, the performance of MARL systems can collapse.\nüõ°Ô∏è Enter Active Defense: The Core Idea Yu et al. propose a new active defense strategy. Instead of blindly trusting all messages, agents:\nJudge the reliability of each incoming message using their own observations and history (hidden states). Adjust the influence of unreliable messages by reducing their weight in the decision process. üëâ Example: If one agent already searched location (1,1) and found nothing, but receives a message saying ‚ÄúTarget at (1,1)‚Äù, it can spot the inconsistency and downweight that message.\nüß© The ADMAC Framework The authors introduce Active Defense Multi-Agent Communication (ADMAC), which has two key components:\nReliability Estimator: A classifier that predicts whether a message is reliable (weight close to 1) or unreliable (weight close to 0). Decomposable Message Aggregation Policy Net: A structure that breaks down the influence of each message into an action preference vector, making it possible to scale its impact up or down. This allows agents to combine their own knowledge with weighted messages to make more robust decisions.\nThe figure above shows how an agent in the ADMAC framework generates its action distribution by combining its own observations with incoming messages from other agents:\nHidden state update: The agent maintains a hidden state (h·µ¢·µó‚Åª¬π), which is updated using the observation (o·µ¢·µó) through the GRU module f_HP. This captures past and current information. Base action preference: From the updated hidden state, the agent generates a base preference vector via f_BP, representing what it would do independently. Message influence: Each received message (m‚ÇÅ·µó, ‚Ä¶, m_N·µó) is processed with the observation through f_MP, producing a message-based action preference vector. Reliability estimation: A reliability estimator f_R evaluates each message, assigning it a weight w·µ¢(m‚±º·µó) that reflects how trustworthy it seems. Aggregation: The agent sums its base vector with all weighted message vectors to form a total action preference vector (v·µ¢·µó). Final decision: Applying a Softmax function converts this vector into a probability distribution over actions, from which the agent selects its next move. By downweighting unreliable messages, ADMAC enables agents to remain robust against malicious communication while still leveraging useful information from peers.\nReference: Yu, L., Qiu, Y., Yao, Q., Shen, Y., Zhang, X., \u0026amp; Wang, J. (2024). Robust Communicative Multi-Agent Reinforcement Learning with Active Defense. AAAI-24.\n","permalink":"http://localhost:1313/posts/adversarial-rl/","summary":"\u003ch1 id=\"adversarial-attacks-on-reinforcement-learning-policies\"\u003eAdversarial Attacks on Reinforcement Learning Policies\u003c/h1\u003e\n\u003cp\u003eReinforcement Learning (RL) has powered breakthroughs in games, robotics, and autonomous systems. But just like image classifiers can be fooled by carefully crafted adversarial examples, RL agents are also vulnerable to \u003cstrong\u003eadversarial attacks\u003c/strong\u003e. These attacks can manipulate the agent‚Äôs perception of the environment or its decision process, leading to unsafe or suboptimal behavior.\u003c/p\u003e\n\u003ch2 id=\"what-do-adversarial-attacks-mean-in-rl\"\u003eWhat Do Adversarial Attacks Mean in RL?\u003c/h2\u003e\n\u003cp\u003eIn RL, an agent interacts with an environment, observes a state, takes an action, and receives a reward. An adversarial attack interferes with this process‚Äîoften by perturbing the input observations‚Äîso the agent chooses actions that look reasonable to it but are actually harmful.\u003c/p\u003e","title":"Adversarial RL"},{"content":"Actor-Critic vs. Value-Based: Empirical Trade-offs Introduction In Reinforcement Learning (RL), one of the fundamental questions is:\nShould we focus on learning the value of states, or should we directly learn the optimal policy?\nBefore diving into this comparison, let‚Äôs briefly recall what RL is: a branch of machine learning in which an agent interacts with an environment to learn how to make decisions that maximize cumulative reward.\nTraditionally, RL algorithms fall into two main families:\nValue-Based methods : which aim to estimate the value of states or state‚Äìaction pairs. Policy-Based methods : which directly optimize a policy that maps states to actions. Actor-Critic algorithms combine the strengths of both worlds, simultaneously learning a value function and an explicit policy. This hybrid structure can often lead to more stable and efficient learning.\nValue-Based and Actor-Critic approaches represent fundamentally different perspectives: one focuses solely on learning state values, while the other integrates both value and policy learning. Comparing these two perspectives helps us better understand the impact of incorporating value or policy components in different environments.\nIn this project, we empirically evaluate these two families in both discrete and continuous action spaces. Four representative algorithms ( DQN, A2C, NAF, and SAC ) were implemented, along with a user-friendly graphical user interface (GUI) for training and evaluation.\nOur ultimate goal is to analyze trade-offs such as convergence speed, learning stability, and final performance across diverse scenarios.\nBackground The reinforcement learning algorithms used in this project fall into two main families:\nValue-Based\nIn this approach, the agent learns only a value function, such as $Q(s, a)$.\nThe policy is derived implicitly by selecting the action with the highest value:\n$\\pi(s) = \\arg\\max_a Q(s,a)$\nThis method is typically simpler, more stable, and computationally efficient.\nHowever, it faces limitations when dealing with continuous action spaces.\nExample algorithms: DQN, NAF.\nValue-Based methods are often well-suited for discrete action spaces with relatively small state‚Äìaction domains, where enumerating or approximating the value for each action is feasible.\nActor-Critic\nIn this framework, the agent consists of two components:\nActor : a parameterized policy that directly produces actions. Critic : a value function that evaluates the Actor‚Äôs performance and guides its updates. This combination can provide greater learning stability, improved performance in complex environments, and high flexibility in continuous action spaces.\nExample algorithms: A2C, SAC.\nActor-Critic methods are generally more suitable for continuous or high-dimensional action spaces, as the Actor can output actions directly without exhaustive value estimation.\nMethodology Project Design This project was designed to perform an empirical comparison between two major families of reinforcement learning algorithms: Value-Based and Actor-Critic.\nFour representative algorithms were selected and implemented in diverse discrete and continuous environments.\nTraining, evaluation, and comparison were carried out through a fully interactive, user-friendly graphical interface.\nImplemented Algorithms Representative algorithms from the two families were selected based on their reported performance in different environments according to the literature.\nFor each algorithm, the training procedure was reproduced in accordance with its original paper.\nThe overall structure of each algorithm is summarized below:\nAlgorithm Family Action Space Description Reference Deep Q-Network (DQN) Value-Based Discrete Uses experience replay and a fixed target network to stabilize learning. 1 Normalized Advantage Function (NAF) Value-Based Continuous Value-based method for continuous spaces using a specific Q-structure to simplify action selection. 2 Advantage Actor-Critic (A2C) Actor-Critic Discrete/Continuous Direct policy optimization guided by an advantage function. 3 Soft Actor-Critic (SAC) Actor-Critic Continuous Off-policy actor-critic method maximizing entropy for stability in complex environments. 4 Deep Q-Network (DQN) The pseudocode of DQN highlights the use of experience replay and a target network, which together reduce correlations between samples and stabilize training.\nNormalized Advantage Function (NAF) NAF handles continuous action spaces by constraining the Q-function into a quadratic form, which makes action selection computationally efficient.\nAdvantage Actor-Critic (A2C) A2C directly optimizes a parameterized policy (Actor) with guidance from the Critic, using advantage estimation to reduce gradient variance and improve learning stability.\nSoft Actor-Critic (SAC) SAC introduces entropy maximization in the objective, encouraging exploration and robustness in complex continuous environments.\nEnvironments There were many 2D and 3D environments available so that we could compare them.\nThe 12 famous environments are listed below:\nFour environments from the Gym library were selected to provide a diverse set of challenges that cover both discrete and continuous action spaces, as well as varying levels of complexity and dynamics:\nMountainCar-v0 (Discrete): A classic control problem where the agent must drive a car up a hill using discrete acceleration commands. This environment tests basic exploration and planning in a low-dimensional, discrete action space. Pendulum-v1 (Continuous): Requires applying continuous torque to keep a pendulum upright. This environment is ideal for evaluating continuous control algorithms and stabilizing dynamics. FrozenLake-v1 (Discrete): A gridworld task where the agent navigates an icy lake to reach a goal while avoiding holes. This environment emphasizes decision-making under uncertainty in a discrete setting. HalfCheetah-v4 (Continuous): A high-dimensional continuous control environment where the agent controls a bipedal cheetah to run efficiently. It challenges advanced continuous control and balance strategies. These environments were chosen to allow a comprehensive comparison of algorithms across different action types, state complexities, and control challenges.\nEnvironment Type Description MountainCar-v0 Discrete Drive a car up a hill by controlling acceleration in a discrete space. Pendulum-v1 Continuous Apply torque to keep a pendulum upright and stable. FrozenLake-v1 Discrete Navigate an icy grid to reach the goal without falling into holes. HalfCheetah-v4 Continuous Control the speed and balance of a simulated bipedal cheetah for fast running. Environment snapshots were recorded during training and appear as GIFs in the Results section.\nConfiguration and Evaluation Algorithms were run with optimized settings for each environment. Key hyperparameters (learning rate, Œ≥, batch size, buffer size) were tuned through trial-and-error, leveraging existing GitHub implementations for optimal performance. Comparisons were based on convergence speed, training stability, and final performance. Example configurations:\nAlgorithm Environment Œ≥ Learning Rate Batch Size Buffer Size DQN FrozenLake 0.93 6e-4 32 4,000 A2C MountainCar 0.96 1e-3 ‚Äì ‚Äì NAF Pendulum 0.99 3e-4 64 400,000 SAC HalfCheetah 0.99 3e-4 256 1,000,000 Graphical User Interface (GUI) A user-friendly GUI was developed to simplify training and comparing algorithms, enabling full project execution without direct coding.The code is available at: This Github Link The project structure is as follows:\nProject_Code/\r‚îú‚îÄ‚îÄ plots/\r‚îÇ ‚îî‚îÄ‚îÄ learning_curves/\r‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ Continuous/\r‚îÇ ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ Pendulum-v1/\r‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ SAC/\r‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ NAF/\r‚îÇ ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ HalfCheetah-v4/\r‚îÇ ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ SAC/\r‚îÇ ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ NAF/\r‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ Discrete/\r‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ FrozenLake-v1/\r‚îÇ ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ DQN/\r‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ 4x4\r‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ 8x8\r‚îÇ ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ A2C/\r‚îÇ ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ 4x4\r‚îÇ ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ 8x8\r‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ MountainCar-v0/\r‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ DQN/\r‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ A2C/\r‚îÇ ‚îú‚îÄ‚îÄ comparison_table.png\r‚îÇ ‚îî‚îÄ‚îÄ directory_structure.png\r‚îú‚îÄ‚îÄ requirements/\r‚îÇ ‚îú‚îÄ‚îÄ base.txt\r‚îÇ ‚îú‚îÄ‚îÄ dev.txt\r‚îÇ ‚îú‚îÄ‚îÄ env.txt\r‚îÇ ‚îî‚îÄ‚îÄ config.py\r‚îú‚îÄ‚îÄ src/\r‚îÇ ‚îú‚îÄ‚îÄ agents/\r‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ a2c.py\r‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ dqn.py\r‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ naf.py\r‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ sac.py\r‚îÇ ‚îú‚îÄ‚îÄ envs/\r‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ continuous_envs.py\r‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ discrete_envs.py\r‚îÇ ‚îú‚îÄ‚îÄ main.py\r‚îÇ ‚îú‚îÄ‚îÄ train.py\r‚îÇ ‚îî‚îÄ‚îÄ utils.py\r‚îú‚îÄ‚îÄ videos/\r‚îÇ ‚îú‚îÄ‚îÄ Continuous/\r‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ Pendulum-v1/\r‚îÇ ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ SAC/\r‚îÇ ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ NAF/\r‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ HalfCheetah-v4/\r‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ SAC/\r‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ NAF/\r‚îÇ ‚îî‚îÄ‚îÄ Discrete/\r‚îÇ ‚îú‚îÄ‚îÄ FrozenLake-v1/\r‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ DQN/\r‚îÇ ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ 4x4\r‚îÇ ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ 8x8\r‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ A2C/\r‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ 4x4\r‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ 8x8\r‚îÇ ‚îî‚îÄ‚îÄ MountainCar-v0/\r‚îÇ ‚îú‚îÄ‚îÄ DQN/\r‚îÇ ‚îî‚îÄ‚îÄ A2C/\r‚îú‚îÄ‚îÄ models/\r‚îÇ ‚îú‚îÄ‚îÄ Continuous/\r‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ Pendulum-v1/\r‚îÇ ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ SAC/\r‚îÇ ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ NAF/\r‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ HalfCheetah-v4/\r‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ SAC/\r‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ NAF/\r‚îÇ ‚îî‚îÄ‚îÄ Discrete/\r‚îÇ ‚îú‚îÄ‚îÄ FrozenLake-v1/\r‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ DQN/\r‚îÇ ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ 4x4\r‚îÇ ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ 8x8\r‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ A2C/\r‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ 4x4\r‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ 8x8\r‚îÇ ‚îî‚îÄ‚îÄ MountainCar-v0/\r‚îÇ ‚îú‚îÄ‚îÄ DQN/\r‚îÇ ‚îî‚îÄ‚îÄ A2C/\r‚îî‚îÄ‚îÄ README.md Main features:\nSelect algorithm, environment, action space type (discrete/continuous), and execution mode (train/test) with just a few clicks. Launch training with a Run button, and view results via Show Plots and Show Videos. Compare algorithms interactively using a dedicated Compare Algorithms window. Display key settings such as hyperparameters and project structure. Real-time console output for monitoring execution status and system messages. Interactive Mobile Application In addition to the desktop GUI, a mobile application for Android and iOS has been developed to provide interactive access to the project. By simply scanning the poster, users can explore various features, including:\nViewing videos of agent executions in different environments. Opening the project‚Äôs website for additional resources and documentation. Displaying the poster digitally for interactive exploration. Comparing learning curves and results across different algorithms. The images below showcase some sections of the app interface:\nResults This section presents the empirical evaluation of four reinforcement learning algorithms (DQN, NAF, A2C, and SAC) from the Value-Based and Actor-Critic families, across both discrete and continuous action space environments. The performance is analyzed based on final reward, convergence speed, and training stability, supported by quantitative metrics, qualitative visualizations (GIFs), and learning curves with a moving average (MA) applied to reduce noise. The experiments were conducted using the Gymnasium library, with optimized hyperparameters (see Methodology for details) and a fixed random seed for reproducibility. Training was performed on a single NVIDIA RTX 4050 GPU, with average runtimes of 1‚Äì4 hours per algorithm-environment pair.\nDiscrete Environments The discrete action space environments tested were FrozenLake-v1 (8x8 grid) and MountainCar-v0, which challenge the algorithms with stochastic transitions and sparse rewards, respectively. The table below summarizes the performance, followed by detailed analyses and visualizations.\nEnvironment Algorithm Final Reward (Avg ¬± Std) Convergence Speed (Episodes to 90% of Max) Stability (Std of Reward) FrozenLake-v1 DQN 0.98 ¬± 0.14 ~1,475 0.50 A2C 1.00 ¬± 0.00 ~1,209 0.48 MountainCar-v0 DQN -22.21 ¬± 79.32 ~2,000 81.50 A2C -27.87 ¬± 62.35 ~2,000 40.83 FrozenLake-v1 In FrozenLake-v1, a stochastic gridworld, A2C outperformed DQN in final reward (1.00 vs. 0.98 success rate) and converged faster (~1209 vs. ~1475 episodes to reach 90% of max reward). A2C‚Äôs advantage estimation provided greater stability, as evidenced by its lower standard deviation (0.48 vs. 0.50). The GIF below illustrates agent behaviors, showing A2C‚Äôs smoother navigation to the goal compared to DQN‚Äôs occasional missteps.\nReward comparison in FrozenLake-v1: A2C converges quickly and maintains stable performance compared to DQN.\nMountainCar-v0 In MountainCar-v0, a deterministic environment with sparse rewards, DQN achieved a better final reward (-22.21 vs. -27.87 timesteps to goal) but both algorithms converged at similar speeds (~2000 episodes). However, A2C exhibited greater stability (std of 40.83 vs. 81.50), avoiding large oscillations in learning. The GIF below shows DQN‚Äôs quicker ascent to the hilltop, while A2C maintains more consistent swings.\nReward comparison in MountainCar-v0: DQN converges faster and reaches higher rewards, while A2C shows greater stability.\nContinuous Environments The continuous action space environments tested were Pendulum-v1 and HalfCheetah-v4, which require precise control and balance in low- and high-dimensional settings, respectively. The table below summarizes the performance.\nEnvironment Algorithm Final Reward (Avg ¬± Std) Convergence Speed (Episodes to 90% of Max) Stability (Std of Reward) Pendulum-v1 NAF -141.17 ¬± 85.58 ~246 199.46 SAC 287.66 ¬± 62.38 ~152 113.23 HalfCheetah-v4 NAF 3,693.35 ¬± 575.60 ~862 1,077.01 SAC 10,247.42 ¬± 584.31 ~1,127 2,493.55 Pendulum-v1 In Pendulum-v1, SAC significantly outperformed NAF in final reward (287.66 vs. -141.17, higher is better) and converged faster (~152 vs. ~246 episodes). SAC‚Äôs entropy maximization ensured smoother learning, with a standard deviation of 113.23 compared to NAF‚Äôs 199.46. The GIF below highlights SAC‚Äôs ability to stabilize the pendulum upright, while NAF struggles with inconsistent torque.\nReward comparison in Pendulum-v1: SAC achieves higher rewards with smoother convergence compared to NAF.\nHalfCheetah-v4 In HalfCheetah-v4, a high-dimensional control task, SAC achieved a much higher final reward (10247.42 vs. 3693.35) but converged slightly slower (~1127 vs. ~862 episodes). SAC‚Äôs stability (std of 2493.55 vs. 1077.01) reflects its robustness in complex dynamics, though NAF shows lower variance. The GIF below shows SAC‚Äôs fluid running motion compared to NAF‚Äôs less coordinated movements.\nReward comparison in HalfCheetah-v4: SAC consistently outperforms NAF in both speed and stability.\nDiscussion \u0026amp; Conclusion This project examined the performance differences between Value-Based and Actor-Critic algorithms in both discrete and continuous environments.\nThe experimental results indicate that no single algorithm is universally superior; rather, the environment characteristics and action space type play a decisive role in determining performance.\nAnalysis \u0026amp; Interpretation Continuous Action Spaces SAC consistently outperformed NAF in both Pendulum-v1 and HalfCheetah-v4, thanks to its entropy maximization strategy, which promotes exploration and robustness. NAF‚Äôs fixed quadratic Q-function structure limited its flexibility in high-dimensional or complex tasks, leading to slower convergence and higher variance in rewards. SAC‚Äôs ability to directly optimize a stochastic policy made it particularly effective in continuous control scenarios.\nDiscrete Action Spaces In FrozenLake-v1, a stochastic environment, A2C‚Äôs stability (due to advantage estimation) gave it an edge over DQN, achieving higher success rates and faster convergence. In MountainCar-v0, a deterministic environment with a small action space, DQN‚Äôs value-based approach excelled in final reward and convergence speed, though A2C remained more stable. This highlights the suitability of Value-Based methods for simpler, deterministic settings and Actor-Critic methods for stochastic or complex environments.\nKey Findings: In simple discrete environments, such as FrozenLake, Value-Based algorithms (e.g., DQN) achieved competitive performance, but Actor-Critic algorithms (e.g., A2C) showed faster convergence and more stable learning. In continuous and more complex environments, Actor-Critic algorithms ‚Äî particularly SAC ‚Äî outperformed their Value-Based counterparts in terms of final reward and convergence speed. Observed Trade-offs: Aspect Value-Based Actor-Critic Simplicity of implementation Yes More complex Initial learning speed High in simple environments Depends on tuning Training stability More oscillations More stable Suitability for continuous spaces Not always Yes Overall, the choice between Value-Based and Actor-Critic methods should be guided by the nature of the task, the complexity of the environment, and the available computational budget.\nObservations Based on Environment Characteristics Our experimental results further reveal that the nature of the environment‚Äîin terms of action space, state space, and reward structure‚Äîsignificantly impacts algorithm performance:\nAction Space (Discrete vs. Continuous) Discrete Action Spaces: Value-Based algorithms like DQN tend to perform competitively, especially in small and low-dimensional discrete action spaces. They converge quickly and reliably, but may struggle when the action space grows larger or stochasticity increases. Actor-Critic methods such as A2C can still provide improved stability in these scenarios, especially under stochastic transitions. Continuous Action Spaces: Actor-Critic methods (e.g. SAC) dominate due to their ability to output continuous actions directly. Value-Based methods require specialized approximations (like NAF), which often limit flexibility and performance in high-dimensional or continuous control tasks. State Space (Low-dimensional vs. High-dimensional) Low-dimensional states (e.g., MountainCar, FrozenLake) generally favor Value-Based methods, which can efficiently enumerate or approximate Q-values. High-dimensional states (e.g., HalfCheetah) require the policy network of Actor-Critic methods to generalize across large state spaces. These methods better handle complex dynamics and correlations among state variables. Reward Structure (Sparse vs. Dense) Sparse Reward Environments (e.g., FrozenLake) challenge Value-Based methods to propagate value signals efficiently, potentially slowing convergence. Actor-Critic algorithms can leverage advantage estimation and policy gradients to maintain learning stability even with sparse rewards. Dense Reward Environments (e.g., HalfCheetah, Pendulum) allow both families to learn effectively, but Actor-Critic methods often achieve smoother and faster convergence due to direct policy optimization combined with value guidance. The interplay between action space type, state space complexity, and reward sparsity fundamentally shapes the suitability of each algorithm. In general:\nDiscrete + Low-dimensional + Dense reward ‚Üí Value-Based methods are competitive. Continuous + High-dimensional + Sparse or Dense reward ‚Üí Actor-Critic methods provide superior learning stability and higher final performance. These insights complement the empirical trade-offs already observed in our study, providing a more nuanced understanding of when and why certain RL algorithms excel under different environment characteristics.\nLimitations and Future Work Limitations The present project, which evaluates the performance of reinforcement learning algorithms (DQN, A2C, NAF, and SAC) in discrete (FrozenLake-v1 and MountainCar-v0) and continuous (Pendulum-v1 and HalfCheetah-v4) environments, provides valuable insights but is subject to several limitations, outlined below:\nLimited Number of Environments:\nThe study only examines four specific environments, which may not provide sufficient diversity to generalize results across all types of reinforcement learning environments. More complex environments with larger state or action spaces or different dynamics could yield different outcomes. Lack of Random Seed Variation:\nThe reported results are based on a single run or an average of a limited number of runs. Conducting multiple experiments with different random seeds could better demonstrate the robustness and reliability of the results. Focus on Specific Metrics:\nThe evaluation metrics (final reward average, convergence speed, and stability) cover only certain aspects of algorithm performance. Other metrics, such as computational efficiency, training time, or robustness to environmental noise, were not assessed. Future Work To build upon the findings of this project, which evaluated the performance of reinforcement learning algorithms (DQN, A2C, NAF, and SAC) in discrete (FrozenLake-v1, MountainCar-v0) and continuous (Pendulum-v1, HalfCheetah-v4) environments, several directions for future research and development can be pursued to address the limitations and extend the scope of the study:\nBroader Range of Environments:\nFuture work could include testing the algorithms on a wider variety of environments, such as those with larger state and action spaces, partially observable states (e.g., POMDPs), or real-world-inspired tasks. This would help validate the generalizability of the observed performance trends. Incorporating Random Seed Variations:\nConducting multiple runs with different random seeds would improve the robustness of results and allow for statistical analysis of performance variability, ensuring that conclusions are not biased by specific initial conditions. Evaluation of Additional Metrics:\nFuture studies could incorporate metrics such as computational efficiency, memory usage, training time, and robustness to environmental perturbations (e.g., noise or dynamic changes). This would provide a more holistic view of algorithm suitability for practical applications. Real-World Application: Robotics The insights gained from this project have significant potential for real-world applications, particularly in robotics, where reinforcement learning can enable autonomous systems to perform complex tasks. The following outlines how the evaluated algorithms could be applied and extended in robotics contexts:\nRobotic Manipulation:\nAlgorithms like SAC, which performed well in continuous control tasks (e.g., Pendulum-v1, HalfCheetah-v4), could be applied to robotic arms for tasks such as grasping, object manipulation, or assembly. SAC‚Äôs ability to handle continuous action spaces makes it suitable for precise control in high-dimensional settings. Autonomous Navigation:\nDiscrete action space algorithms like DQN and A2C, tested in environments like FrozenLake-v1, could be adapted for robot navigation in grid-like or structured environments (e.g., warehouse robots). A2C‚Äôs stability in stochastic settings could be particularly useful for navigating dynamic or uncertain environments. Locomotion and Mobility:\nThe success of SAC in HalfCheetah-v4 suggests its potential for controlling legged robots or humanoid robots for locomotion tasks. Future work could involve applying SAC to real-world robotic platforms to achieve robust and efficient walking or running behaviors. These future research directions and real-world applications highlight the potential to extend the current study‚Äôs findings to more diverse and practical scenarios. By addressing the identified limitations and applying the algorithms to robotics, this work can contribute to the development of more robust, efficient, and adaptable autonomous systems.\nReferences A2C (Advantage Actor-Critic):\nMnih, V., et al. (2016). \u0026ldquo;Asynchronous Methods for Deep Reinforcement Learning.\u0026rdquo; ICML 2016. Paper Stable-Baselines3 A2C Implementation: GitHub SAC (Soft Actor-Critic):\nHaarnoja, T., et al. (2018). \u0026ldquo;Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor.\u0026rdquo; ICML 2018. Paper Stable-Baselines3 SAC Implementation: GitHub DQN (Deep Q-Network):\nMnih, V., et al. (2015). \u0026ldquo;Human-level control through deep reinforcement learning.\u0026rdquo; Nature, 518(7540), 529-533. Paper Stable-Baselines3 DQN Implementation: GitHub NAF (Normalized Advantage Function):\nGu, S., et al. (2016). \u0026ldquo;Continuous Deep Q-Learning with Model-based Acceleration.\u0026rdquo; ICML 2016. Paper Gymnasium:\nOfficial Gymnasium Documentation: Gymnasium ","permalink":"http://localhost:1313/posts/actor-critic-vs-value-based-empirical-trade-offs/","summary":"This study compares Value-Based (DQN, NAF) and Actor-Critic (A2C, SAC) reinforcement learning algorithms in diverse environments (MountainCar-v0, Pendulum-v1, FrozenLake-v1, HalfCheetah-v4). Through empirical evaluation, we analyze trade-offs in convergence speed, learning stability, and final performance, supported by a user-friendly GUI and mobile application for interactive training and visualization.","title":"Actor-Critic vs. Value-Based: Empirical Trade-offs"},{"content":"Introduction This paper critically examines the assumptions commonly accepted in modeling reinforcement learning problems, suggesting that these assumptions may impede progress in the field. The authors refer to these assumptions as \u0026ldquo;dogmas.\u0026rdquo;\nDogma: A fixed, especially religious, belief or set of beliefs that people are expected to accept without any doubts. ‚ÄîFrom the Cambridge Advanced Learner\u0026rsquo;s Dictionary \u0026amp; Thesaurus\nThe paper introduces three central dogmas:\nThe Environment Spotlight Learning as Finding a Solution The Reward Hypothesis (although not exactly a dogma) The author argues the true reinforcement learning landscape is actualy like this,\nIn the words of Rich Sutton:\nRL can be viewed as a microcosm of the whole AI problem.\nHowever, today\u0026rsquo;s RL landscape is overly simplified,\nThese three dogmas are responsible for narrowing the potential of RL,\nThe authors propose that we consider moving beyond these dogmas,\nTo reclaim the true landscape of RL,\nBackground The authors reference Thomas Kuhn\u0026rsquo;s book, \u0026ldquo;The Structure of Scientific Revolutions\u0026rdquo;,\nKuhn distinguishes between two phases of scientific activity,\nNormal Science: Resembling puzzle-solving. Revolutionary Phase: Involving a fundamental rethinking of the values, methods, and commitments of science, which Kuhn calls a \u0026ldquo;paradigm.\u0026rdquo; Here\u0026rsquo;s an example of a previous paradigm shift in science:\nThe authors explore the paradigm shift needed in RL:\nDogma One: The Environment Spotlight The first dogma we call the environment spotlight, which refers to our collective focus on modeling environments and environment-centric concepts rather than agents.\nWhat do we mean when we say that we focus on environments? We suggest that it is easy to answer only one of the following two questions:\nWhat is at least one canonical mathematical model of an environment in RL?\nMDP and its variants! And we define everything in terms of it. By embracing the MDP, we are allowed to import a variety of fundamental results and algorithms that define much of our primary research objectives and pathways. For example, we know every MDP has at least one deterministic, optimal, stationary policy, and that dynamic programming can be used to identify this policy. What is at least one canonical mathematical model of an agent in RL?\nIn contrast, this question has no clear answer! The author suggests it is important to define, model, and analyse agents in addition to environments. We should build toward a canonical mathematical model of an agent that can open us to the possibility of discovering general laws governing agents (if they exist).\nDogma Two: Learning as Finding a Solution The second dogma is embedded in the way we treat the concept of learning. We tend to view learning as a finite process involving the search for‚Äîand eventual discovery of‚Äîa solution to a given task.\nWe tend to implicitly assume that the learning agents we design will eventually find a solution to the task at hand, at which point learning can cease. Such agents can be understood as searching through a space of representable functions that captures the possible action-selection strategies available to an agent, similar to the Problem Space Hypothesis, and, critically, this space contains at least one function‚Äîsuch as the optimal policy of an MDP‚Äîthat is of sufficient quality to consider the task of interested solved. Often, we are then interested in designing learning agents that are guaranteed to converge to such an endpoint, at which point the agent can stop its search (and thus, stop its learning).\nThe author suggests to embrace the view that learning can also be treated as adaptation. As a consequence, our focus will drift away from optimality and toward a version of the RL problem in which agents continually improve, rather than focus on agents that are trying to solve a specific problem.\nWhen we move away from optimality,\nHow do we think about evaluation? How, precisely, can we define this form of learning, and differentiate it from others? What are the basic algorithmic building blocks that carry out this form of learning, and how are they different from the algorithms we use today? Do our standard analysis tools such as regret and sample complexity still apply? These questions are important, and require reorienting around this alternate view of learning.\nThe authors introduce the book \u0026ldquo;Finite and Infinite Games\u0026rdquo;,\nAnd the concept of Finite and Infinite Games is summarized in the following quote,\nThere are at least two kinds of games, One could be called finite; the other infinite. A finite game is played for the purpose of winning, an infinite game for the purpose of continuing the play.\nAnd argues alignment is an infinite game.\nDogma Three: The Reward Hypothesis The third dogma is the reward hypothesis, which states \u0026ldquo;All of what we mean by goals and purposes can be well thought of as maximization of the expected value of the cumulative sum of a received scalar signal (reward).\u0026rdquo;\nThe authors argue that the reward hypothesis is not truly a dogma. Nevertheless, it is crucial to understand its nuances as we continue to design intelligent agents.\nThe reward hypothesis basically says,\nIn recent analysis by [2] fully characterizes the implicit conditions required for the hypothesis to be true. These conditions come in two forms. First, [2] provide a pair of interpretative assumptions that clarify what it would mean for the reward hypothesis to be true or false‚Äîroughly, these amount to saying two things (brwon doors).\nFirst, that \u0026ldquo;goals and purposes\u0026rdquo; can be understood in terms of a preference relation on possible outcomes. Second, that a reward function captures these preferences if the ordering over agents induced by value functions matches that of the ordering induced by preference on agent outcomes. This leads to the following conjecture,\nThen, under this interpretation, a Markov reward function exists to capture a preference relation if and only if the preference relation satisfies the four von Neumann-Morgenstern axioms, and a fifth Bowling et al. call $\\gamma$-Temporal Indifference.\nAxiom 1: Completeness \u0026gt; You have a preference between every outcome pair.\nYou can always compare any two choices. Axiom 2: Transitivity \u0026gt; No preference cycles.\nIf you like chocolate more than vanilla, and vanilla more than strawberry, you must like chocolate more than strawberry. Axiom 3: Independence \u0026gt; Independent alternatives can\u0026rsquo;t change your preference.\nIf you like pizza more than salad, and you have to choose between a lottery of pizza or ice cream and a lottery of salad or ice cream, you should still prefer the pizza lottery over the salad lottery. Axiom 4: Continuity \u0026gt; There is always a break even chance.\nImagine you like a 100 dollar bill more than a 50 dollar bill, and a 50 dollar bill more than a 1 dollar bill. There should be a scenario where getting a chance at 100 dollar and 1 dollar, with certain probabilities, is equally good as getting the 50 dollar for sure. These 4 axioms are called the von Neumann-Morgenstern axioms.\nAxiom 5: Temporal $\\boldsymbol{\\gamma}$-Indifference \u0026gt; Discounting is consistent throughout time. Temporal $\\gamma$-indifference says that if you are indifferent between receiving a reward at time $t$ and receiving the same reward at time $t+1$, then your preference should not change if we move both time points by the same amount. For instance, if you don\u0026rsquo;t care whether you get a candy today or tomorrow, then you should also not care whether you get the candy next week or the week after. Taking these axioms into account, the reward conjecture becomes the reward theorem,\nIt is essential to consider that people do not always conform to these axioms, and human preferences can vary.\nIt is important that we are aware of the implicit restrictions we are placing on the viable goals and purposes under consideration when we represent a goal or purpose through a reward signal. We should become familiar with the requirements imposed by the five axioms, and be aware of what specifically we might be giving up when we choose to write down a reward function.\nSee Also David Abel Presentation @ ICML 2023 David Abel Personal Website Mark Ho Personal Website Anna Harutyunyan Personal Website References [1] Abel, David, Mark K. Ho, and Anna Harutyunyan. \u0026ldquo;Three Dogmas of Reinforcement Learning.\u0026rdquo; arXiv preprint arXiv:2407.10583 (2024).\n[2] Bowling, Michael, et al. \u0026ldquo;Settling the reward hypothesis.\u0026rdquo; International Conference on Machine Learning. PMLR, 2023.\n","permalink":"http://localhost:1313/posts/three-dogmas-of-reinforcement-learning/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eThis paper critically examines the assumptions commonly accepted in modeling reinforcement learning problems, suggesting that these assumptions may impede progress in the field. The authors refer to these assumptions as \u0026ldquo;dogmas.\u0026rdquo;\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003eDogma: A fixed, especially religious, belief or set of beliefs that people are expected to accept without any doubts.\u003c/strong\u003e \u003cem\u003e‚ÄîFrom the \u003ca href=\"https://dictionary.cambridge.org/dictionary/english\"\u003eCambridge Advanced Learner\u0026rsquo;s Dictionary \u0026amp; Thesaurus\u003c/a\u003e\u003c/em\u003e\u003c/p\u003e\u003c/blockquote\u003e\n\u003cp\u003eThe paper introduces three central dogmas:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eThe Environment Spotlight\u003c/li\u003e\n\u003cli\u003eLearning as Finding a Solution\u003c/li\u003e\n\u003cli\u003eThe Reward Hypothesis (although not exactly a dogma)\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eThe author argues the true reinforcement learning landscape is actualy like this,\u003c/p\u003e","title":"Three Dogmas of Reinforcement Learning"},{"content":"Introduction This paper studies the problem of zero-shot generalization in reinforcement learning and introduces an algorithm that trains an ensamble of maximum reward seeking agents and an maximum entropy agent. At test time, either the ensemble agrees on an action, and we generalize well, or we take exploratory actions with the help of maximum entropy agent and drive us to a novel part of the state space, where the ensemble may potentially agree again. This method combined with an invariance based approach achieves new state-of-the-art results on ProcGen.\nBackground I know it\u0026rsquo;s a lot to take in! You may be wondering:\nWhat is reinforcement learning? ü•∫ What generalization means for RL? ü•≤ What is zero-shot generalization? ü•π What are max reward and max entropy agents?! ‚òπÔ∏è What is an ensamble of them?!! üòü What is an invariance based approach? üòì And what the heck is ProcGen?!!! üò† Don\u0026rsquo;t worry! We are going to cover all of that and more! And you are going to fully understand this paper and finish reading this article with an smile üôÇ!\nWhat is reinforcement learning? Reinforcement learning is learning what to do‚Äîhow to map situations to actions‚Äîso as to maximize a numerical reward signal. Imagine yourself right now, in reinforcement learning terms, you are an agent and everything that is not you, is your environment. You perceive the world through your senses (e.g., eyes, ears, etc.) and what you perceive turns into electrical signals that your brain processes to form an understanding of your surroundings (state). Based on this understanding, you make decisions (actions) with the goal of achieving the best possible outcome for yourself (reward).\nIn a more formal sense, reinforcement learning involves the following components:\nAgent: The learner or decision maker (e.g., you). Environment: Everything the agent interacts with (e.g., the world around you). State ($s$): A representation of the current situation of the agent within the environment (e.g., what you see, hear, and feel at any given moment). Actions ($a$): The set of all possible moves the agent can make (e.g., moving your hand, walking, speaking). Reward ($r$): The feedback received from the environment in response to the agent‚Äôs action (e.g., pleasure from eating food, pain from touching a hot surface). Policy ($\\pi$): A strategy used by the agent to decide which actions to take based on the current state (e.g., your habits and decision-making processes). Value Function ($V$): A function that estimates the expected cumulative reward of being in a certain state and following a particular policy (e.g., your prediction of future happiness based on current actions). The objective of the agent is to develop a policy that maximizes the total cumulative reward over time. This is typically achieved through a process of exploration (trying out new actions to discover their effects) and exploitation (using known actions that yield high rewards).\nIn mathematical terms, the goal is to find a policy $\\pi$ that maximizes the expected return $G_t$, which is the cumulative sum of discounted rewards:\n$$ G_t = r_{t+1} + \\gamma r_{t+2} + \\gamma^2 r_{t+3} + \\cdots $$\nwhere $\\gamma$ (0 ‚â§ $\\gamma$ \u0026lt; 1) is the discount factor that determines the importance of future rewards.\nFor a more thorough and in depth explanation of reinforcement learning please refer to Lilian Weng excelent blog post A (Long) Peek into Reinforcement Learning.\nWhat generalization means for RL? In reinforcement learning, generalization involves an agent\u0026rsquo;s ability to apply learned policies or value functions to new states or environments that it has not encountered during training. This is essential because it is often impractical or impossible to train an agent on every possible state it might encounter.\nThere are bunch of methods that researchers have used to tackle the problem of generalization in reinforcement learning that are summarized in the following diagram.\nThis paper focuses on RL-Specific solutions and specifically exploration technique to tackle the problem of generalization. If you\u0026rsquo;re interested to learn more about the generalization problem in reinforcement learning please refer to reference [3].\nWhat is zero-shot generalization? Zero-shot generalization in RL refers to the ability of an agent to perform well in entirely new environments or tasks without any prior specific training or fine-tuning on those environments or tasks. This is a significant challenge because it requires the agent to leverage its learned representations and policies in a highly flexible and adaptive manner.\nIn order to define the objective of zero-shot generalization we first have to define what MDPs and POMDPs are.\nMarkov Decision Process (MDP):\nMDP is a mathematical framework used to describe an environment in reinforcement learning where the outcome is partly random and partly under the control of a decision-maker (agent). A MDP is a tuple $M = (S, A, P_{init}, P, r, \\gamma)$ where,\nStates ($S \\in \\mathbb{R}^{|S|}$): A finite set of states that describe all possible situations in which the agent can be. Actions ($A \\in \\mathbb{R}^{|A|}$): A finite set of actions available to the agent. Initial State Distribution ($P_{init}$): A distribution of starting state $(s_0 \\sim P_{init})$. Transition Probability ($P$): A function $P(s_{t+1}, s_t, a_t)$ representing the probability of transitioning from state $s_t$ to state $s_{t+1}$ after taking action $a_t$ $(s_{t+1} \\sim P(.|s_t,a_t))$. Reward ($r: S \\times A \\rightarrow \\mathbb{R}$): A function $r(s_t, a_t)$ representing the immediate reward $r_t$ received after transitioning from state $s_t$ due to action $a_t$ $(r_t = r(s_t, a_t))$. Discount Factor ($\\gamma$): $(0 \\leq \\gamma \u0026lt; 1)$ is a constant that determines the importance of future rewards. Partially Observable Markov Decision Process (POMDP):\nPOMDP extends MDPs to situations where the agent does not have complete information about the current state. Instead, the agent must make decisions based on partial observations. A POMDP is a tuple $M = (S, A, O, P_{init}, P, \\Sigma, r, \\gamma)$ where other that above definitions for MDP,\nObservation Space ($O$): A finite set of observations the agent can receive about the state. Observation Function ($\\Sigma$): A function that given current state $s_t$ and current action $a_t$ gives us the current observation $o_t$ $(o_t = \\Sigma(s_t, a_t) \\in O)$. If we set $O = S$ and $\\Sigma(s,a) = s$, the POMDP turns into regular MDP.\nLet the history at time $t$ be,\n$$ h_t = \\{ o_0, a_0, r_0, o_1, a_1, r_1, \\dots, o_t \\} $$\nThe agent‚Äôs next action is outlined by a policy $\\pi$, which is a stochastic mapping from the history to an action probability,\n$$ \\pi(a|h_t) = P(a_t=a|h_t) $$\nIn this formulation, a history-dependent policy (and not a Markov policy) is required both due to partially observed states, epistemic uncertainty, and also for optimal maxEnt exploration.\nWe assume a prior distribution over POMDPs $P(M)$, defined over some space of POMDPs. For a given POMDP, an optimal policy maximizes the expected discounted return,\n$$ \\mathbb{E}_{\\pi,M} \\left[ \\sum _{t=0}^{\\infty} \\gamma^t r(s_t,a_t) \\right] $$\nWhere the expectation is taken over the policy $\\pi(h_t)$, and the state transition probability $s_t \\sim P$ of POMDP $M$.\nOur generalization objective is to maximize the discounted cumulative reward taken in expectation over the POMDP prior,\n$$ \\mathcal{R}_ {pop}(\\pi) = \\mathbb{E}_ {M \\sim P(M)} {\\left[ \\mathbb{E}_{\\pi,M} {\\left[ \\sum _{t=0}^{\\infty} \\gamma^t r(s_t,a_t) \\right]} \\right]} $$\nSeeking a policy that performs well in expectation over any POMDP from the prior corresponds to zero-shot generalization.\nAnd as you may have guessed we don\u0026rsquo;t have access to true prior distribution of POMDPs so we have to estimate it with $N$ training POMDPs $M_1, M_2, \\dots, M_N$ sampled from the true prior distribution $P(M)$. So we are going to maximize empirical discounted cumulative reward,\n$$ \\begin{align*} \\mathcal{R}_ {emp}(\\pi) \u0026amp;= \\frac{1}{N} \\sum _ {i=1}^{N} \\mathbb{E} _ {\\pi,M_i} \\left[ \\sum _ {t=0}^{\\infty} \\gamma^t r(s_t,a_t) \\right] \\\\ \u0026amp;= \\mathbb{E}_ {M \\sim \\hat{P}(M)} {\\left[ \\mathbb{E}_{\\pi,M} {\\left[ \\sum _{t=0}^{\\infty} \\gamma^t r(s_t,a_t) \\right]} \\right]} \\end{align*} $$\nWhere the empirical POMDP distribution $\\hat{P}(M)$ can be different from the true distribution, i.e. $\\hat{P}(M) \\neq P(M)$. In general, a policy that optimizes the empirical reward may perform poorly on the population reward and this is known as overfitting in statistical learning theory.\nWhat are max reward and max entropy agents?! As we have seen, the goal of agents in reinforcement learning is to find a policy $\\pi$ that maximizes the expected discounted return by focusing on actions that lead to the greatest immediate or future rewards. We call these common RL agents \u0026ldquo;max reward agents\u0026rdquo; in this paper. On the other hand, \u0026ldquo;max entropy agents\u0026rdquo; aim to maximize the entropy of the policy for visiting different states. Maximizing entropy encourages the agent to explore a wider range of actions that lead the agent to visit new states even when they don\u0026rsquo;t contribute any reward.\nThis type of agent will help us to make decisions when we have epistemic uncertainty about what to do at test time. Epistemic uncertainty basically means the uncertainty that we have because of our lack of knowledge and can be improved by gathering more information about the situation.\nThe insight of the authors of this paper is that learning a policy that effectively explores the domain is harder to memorize than a policy that maximizes reward for a specific task, and therefore they expect such learned behavior to generalize well.\nWhat is an ensamble of them?!! Ensembling in reinforcement learning involves combining the policies of multiple individual agents to make more accurate and robust decisions. The idea is that by aggregating the outputs of different agents, we can leverage their diverse perspectives and expertise to improve overall performance.\nThis paper uses an ensamble of max reward agents to help the agent decide on the overal epistemic uncetainty that we have at test time.\nWhat is an invariance based approach? Invariance based algorithms in reinforcement learning focus on developing policies that are robust to changes and variations in the environment. These algorithms aim to identify and leverage invariant features or patterns that remain consistent across different environments or tasks. The goal is to ensure that the learned policy performs well not just in the training environment but also in new, unseen environments.\nThis paper uses IDAAC algorithm which is a special kind of DAAC algorithm as its invariance based algorithm.\nDAAC (Decoupled Advantage Actor-Critic) uses two separate networks, one for learning the policy and advantage, and one for learning the value. The value estimates are used to compute the advantage targets.\nIDAAC (Invariant Decoupled Advantage Actor-Critic) adds an additional regularizer to the DAAC policy encoder to ensure that it does not contain episode-specific information. The encoder is trained adversarially with a discriminator so that it cannot classify which observation from a given pair $(s_i, s_j)$ was first in a trajectory.\nFor more information about IDAAC algorithm please refer to reference [4].\nWhat the heck is ProcGen?!!! Procgen is a benchmark suite for evaluating the generalization capabilities of reinforcement learning agents. It was developed by OpenAI and consists of a collection of procedurally generated environments that vary in terms of visual appearance, dynamics, and difficulty. The goal of Procgen is to provide a standardized and challenging set of environments that can be used to assess the ability of RL algorithms to generalize to unseen scenarios.\nIf you are new to reinforcement learning, I know these explanations are a lot to take in! If you find yourself lost, I recommend to check out the following courses at your leisure:\nDeep Reinforcement Learning (by Hugging Face ü§ó) Reinforcement Learning (by Mutual Information) Introduction to Reinforcement Learning (by David Silver) Reinforcement Learning (by Michael Littman \u0026amp; Charles Isbell) Reinforcement Learning (by Emma Brunskill) Deep Reinforcement Learning Bootcamp 2017 Foundations of Deep RL (by Pieter Abbeel) Deep Reinforcement Learning \u0026amp; Control (by Katerina Fragkiadaki) Deep Reinforcement Learning (by Sergey Levine) After reviewing all this we can focus on the rest of the paper!\nHidden Maze Experiment One of the key observation of the authors of this paper is that invariance is not enough for zero-shot generalization of reinforcemen learning algorithm. They designed the hidden maze experiment too demonstrate that. Imagine Maze, but with the walls and goal hidden in the observation. Arguably, this is the most task-invariant observation possible, such that a solution can still be obtained in a reasonable time.\nAn agent with memory can be trained to optimally solve all training tasks: figuring out wall positions by trying to move ahead and observing the resulting motion, and identifying based on its movement history in which training maze it is currently in. Obviously, such a strategy will not generalize to test mazes. Performance in Maze, where the strategy for solving any particular training task must be indicative of that task, has largely not improved by methods based on invariance\nThe following figure shows PPO performance on the hidden maze task, indicating severe overfitting.\nAs described by [5], an agent can overcome test-time errors in its policy by treating the perfect policy as an unobserved variable. The resulting decision making problem, termed the epistemic POMDP, may require some exploration at test time to resolve uncertainty. The article further proposed the LEEP algorithm based on this principle, which trains an ensemble of agents and essentially chooses randomly between the members when the ensemble does not agree, and was the first method to present substantial generalization improvement on Maze.\nIn this paper authors extend this idea and asked, How to improve exploration at test time?, and their approach is based on a novel discovery, when they train an agent to explore the training domains using a maximum entropy objective, they observe that the learned exploration behavior generalizes surprisingly well (much better than the generalization attained when training the agent to maximize reward).\nIn the following section we gonna dig deep into internals of maximum entropy policy.\nMaxEnt Policy For simplicity the authors discuss this part for the MDP case. A policy $\\pi$, through its interaction with an MDP, induces a t-step state distribution over the state space $S$,\n$$ d _ {t,\\pi} (s) = p(s_t=s | \\pi) $$\nThe objective of maximum entropy exploration is given by:\n$$ \\mathcal{H}(d(.)) = -\\mathbb{E} _ {s \\sim d} \\left[ \\log{d(s)} \\right] $$\nWhere $d$ can be regarded as either,\nStationary state distribution (infinite horizon): $d _ {\\pi} = \\lim _ {t \\rightarrow \\infty} d _ {t,\\pi} (s)$ Discounted state distribution (infinite horizon): $d _ {\\gamma, \\pi} = (1-\\gamma) \\sum _ {t=0} ^ {\\infty} \\gamma^t d _ {t,\\pi} (s)$ Marginal state distribution (finite horizon): $d _ {T, \\pi} = \\frac{1}{T} \\sum _ {t=0} ^ {T} d _ {t,\\pi} (s)$ In this work they focus on the finite horizon setting and adapt the marginal state distribution $d _ {T, \\pi}$ in which $T$ equals the episode horizon $H$, so we seek to maximize the objective:\n$$ \\begin{align*} \\mathcal{R} _ {\\mathcal{H}} (\\pi) \u0026amp;= \\mathbb{E} _ {M \\sim \\hat{P}(M)} \\left[ \\mathcal{H}(d _ {H,\\pi}) \\right] \\\\ \u0026amp;=\\mathbb{E} _ {M \\sim \\hat{P}(M)} \\left[ \\mathcal{H}(\\frac{1}{H} \\sum _ {t=0} ^ {H} d _ {t,\\pi} (s)) \\right] \\end{align*} $$\nwhich yields a policy that \u0026ldquo;equally\u0026rdquo; visits all states during the episode.\nTo maximize this objective we can estimating the density of the agent\u0026rsquo;s state visitation distribution, but in this paper the authors adapt the non-parametric entropy estimation approach; we estimate the entropy using the particle based k-nearest neighbor (k-NN estimator).\nTo estimate the distribution $d _ {H,\\pi}$ over the states $S$, we consider each trajectory as $H$ samples of states $\\{ s_t \\} _ {t=1} ^ {H}$ and take $s _ t ^ {\\text{k-NN}}$ to be the k-NN of the state $s_t$ within the trajectory,\n$$ \\hat{ \\mathcal{H} } ^ {k,H} (d _ {H,\\pi}) \\approx \\frac{1}{H} \\sum _ {t=1} ^ {H} \\log ( \\parallel s_t - s_t^{\\text{k-NN}} \\parallel _ 2) $$\nIn which we define intrinsic reward function as,\n$$ r_I (s_t) \\coloneqq \\log ( \\parallel s_t - s_t^{\\text{k-NN}} \\parallel _ 2) $$\nThis formulation enables us to deploy any RL algorithm to approximately optimize objective.\nSpecifically, in this work we use the policy gradient algorithm PPOŸà where at every time step $t$, the state $s_t^{\\text{k-NN}}$ is chosen from previous states $\\{ s_t \\} _ {t=1} ^ {t-1}$ of the same episode. To improve computational efficiency, instead of taking the full observation as the state (64 x 64 RGB image), we sub-sample the observation by applying average pooling of 3 x 3 to produce an image of size 21 x 21.\nWe found that agents trained for maximum entropy exploration exhibit a smaller generalization gap compared with the standard approach of training solely with extrinsic reward. The policies are equipped with a memory unit (GRU) to allow learning of deterministic policies that maximize the entropy.\nIn all three environments, we demonstrate a small generalization gap, as test performance on unseen levels closely follows the performance achieved during training.\nIn addition, we verify that the train results are near optimal by comparing with a hand designed approximately optimal exploration policy. For example, on Maze we use the well known maze exploring strategy wall follower, also known as the left/right-hand rule.\nExpGen Algorithm Our main insight is that, given the generalization property of the entropy maximization policy established above, an agent can apply this behavior in a test MDP and expect effective exploration at test time. We pair this insight with the epistemic POMDP idea, and propose to play the exploration policy when the agent faces epistemic uncertainty, hopefully driving the agent to a different state where the reward-seeking policy is more certain.\nOur framework comprises two parts: an entropy maximizing network and an ensemble of networks that maximize an extrinsic reward to evaluate epistemic uncertainty. The first step entails training a network equipped with a memory unit to obtain a maxEnt policy $\\pi_H$ that maximizes entropy. Next, we train an ensemble of memory-less policy networks $\\{ \\pi _ r ^ j \\} _ {j=1} ^ {m} $ to maximize extrinsic reward.\nHere is the ExpGen algorithm,\nWe consider domains with a finite action space, and say that the policy $\\pi _ r ^ i$ is certain at state $s$ if its action $a_i \\sim \\pi _ r ^ i (a|s)$ is in consensus with the ensemble: $a_i = a_j$ for the majority of $k$ out of $m$, where $k$ is a hyperparameter of our algorithm.\nSwitching between two policies may result in a case where the agent repeatedly toggles between two states (if, say, the maxEnt policy takes the agent from state $s_1$ to a state $s_2$, where the ensemble agrees on an action that again moves to state $s_1$.). To avoid such ‚Äúmeta-stable‚Äù behavior, we randomly choose the number of maxEnt steps $n_{\\pi_{\\mathcal{H}}}$ from a Geometric distribution, $n_{\\pi_{\\mathcal{H}}} \\sim Geom(\\alpha)$.\nExperiments Our experimental setup follows ProcGen\u0026rsquo;s easy configuration, wherein agents are trained on 200 levels for 25M steps and subsequently tested on random levels. All agents are implemented using the IMPALA (Importance Weighted Actor-Learner Architectures) convolutional architecture, and trained using PPO or IDAAC. For the maximum entropy agent $\\pi_H$ we incorporate a single GRU at the final embedding of the IMPALA convolutional architecture. For all games, we use the same parameter $\\alpha=0.5$ of the Geometric distribution and form an ensemble of 10 networks.\nFollowing figure is comparison across all ProcGen games, with 95% bootstrap CIs highlighted in color. Score distributions of ExpGen, PPO, PLR, UCB-DrAC, PPG and IDAAC.\nFollowing figure shows in each row, the probability of algorithm X outperforming algorithm Y. The comparison illustrates the superiority of ExpGen over the leading contender IDAAC with probability 0.6, as well as over other methods with even higher probability.\nSee Also PyTorch implementation of ExpGen @ GitHub Ev Zisselman Presentation @ NeurIPS 2023 ExpGen Rebuttal Process @ OpenReview ExpGen Poster for NeurIPS 2023 References [1] Zisselman, Ev, et al. \u0026ldquo;Explore to generalize in zero-shot rl.\u0026rdquo; Advances in Neural Information Processing Systems 36 (2024).\n[2] Sutton, Richard S., and Andrew G. Barto. \u0026ldquo;Reinforcement learning: An introduction.\u0026rdquo; MIT press, (2020).\n[3] Kirk, Robert, et al. \u0026ldquo;A survey of zero-shot generalisation in deep reinforcement learning.\u0026rdquo; Journal of Artificial Intelligence Research 76 (2023): 201-264.\n[4] Raileanu, Roberta, and Rob Fergus. \u0026ldquo;Decoupling value and policy for generalization in reinforcement learning.\u0026rdquo; International Conference on Machine Learning. PMLR, 2021.\n[5] Ghosh, Dibya, et al. \u0026ldquo;Why generalization in rl is difficult: Epistemic pomdps and implicit partial observability.\u0026rdquo; Advances in neural information processing systems 34 (2021): 25502-25515.\n[6] Espeholt, Lasse, et al. \u0026ldquo;Impala: Scalable distributed deep-rl with importance weighted actor-learner architectures.\u0026rdquo; International conference on machine learning. PMLR, 2018.\n","permalink":"http://localhost:1313/posts/explore-to-generalize-in-zero-shot-rl/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eThis paper studies the problem of zero-shot generalization in reinforcement learning and introduces an algorithm that trains an ensamble of maximum reward seeking agents and an maximum entropy agent. At test time, either the ensemble agrees on an action, and we generalize well, or we take exploratory actions with the help of maximum entropy agent and drive us to a novel part of the state space, where the ensemble may potentially agree again. This method combined with an invariance based approach achieves new state-of-the-art results on ProcGen.\u003c/p\u003e","title":"ExpGen: Explore to Generalize in Zero-Shot RL"},{"content":"Welcome to our blog! We are a team of enthusiastic professors and students from Sharif University of Technology who are eager to share our insights and passion for this fascinating field. Our aim is to share our knowledge and excitement about this cutting-edge field with you ‚ù§Ô∏è.\nProfessors Professor Mohammad Hossein Rohban Professor Ehsaneddin Asgari Students Arash Alikhani Alireza Nobakht Labs RIML Lab NLP \u0026amp; DH Lab We hope you enjoy our blog and find our content both informative and inspiring üöÄ!\n","permalink":"http://localhost:1313/us/","summary":"\u003cp\u003eWelcome to our blog! We are a team of enthusiastic professors and students from Sharif University of Technology who are eager to share our insights and passion for this fascinating field. Our aim is to share our knowledge and excitement about this cutting-edge field with you ‚ù§Ô∏è.\u003c/p\u003e\n\u003ch2 id=\"professors\"\u003eProfessors\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eProfessor \u003ca href=\"https://www.linkedin.com/in/mohammad-hossein-rohban-75567677\"\u003eMohammad Hossein Rohban\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003eProfessor \u003ca href=\"https://www.linkedin.com/in/ehsaneddinasgari\"\u003eEhsaneddin Asgari\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"students\"\u003eStudents\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://www.linkedin.com/in/infinity2357\"\u003eArash Alikhani\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://www.linkedin.com/in/alireza-nobakht\"\u003eAlireza Nobakht\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"labs\"\u003eLabs\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/rohban-lab\"\u003eRIML Lab\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/language-ml\"\u003eNLP \u0026amp; DH Lab\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eWe hope you enjoy our blog and find our content both informative and inspiring üöÄ!\u003c/p\u003e","title":""},{"content":"Adversarial Attacks on Reinforcement Learning Policies Reinforcement Learning (RL) has powered breakthroughs in games, robotics, and autonomous systems. But just like image classifiers can be fooled by carefully crafted adversarial examples, RL agents are also vulnerable to adversarial attacks. These attacks can manipulate the agent‚Äôs perception of the environment or its decision process, leading to unsafe or suboptimal behavior.\nWhat Do Adversarial Attacks Mean in RL? In RL, an agent interacts with an environment, observes a state, takes an action, and receives a reward. An adversarial attack interferes with this process‚Äîoften by perturbing the input observations‚Äîso the agent chooses actions that look reasonable to it but are actually harmful.\nFor example:\nA self-driving car policy might mistake a stop sign for a speed-limit sign if subtle noise is added to the camera input. A robot trained to walk could be tripped by slight modifications to its sensor readings. Types of Attacks in Reinforcement Learning Researchers have identified several ways adversarial attacks can target RL policies. These attack vectors include:\nObservation Perturbation\nThe most widely studied form of attack: small, carefully crafted changes are made to the agent‚Äôs observations so it perceives the environment incorrectly.\nExample: Adding imperceptible noise to frames in an Atari game so the agent misjudges its next move.\nCommunication Perturbation\nIn multi-agent systems, communication messages can be perturbed, leading to miscoordination.\nExample: Two drones sharing location data receive slightly altered coordinates, causing a collision.\nMalicious Communications\nBeyond perturbations, adversaries may inject entirely fake or deceptive messages.\nExample: An attacker sends a false signal about enemy positions in a cooperative strategy game.\nWhy It Matters Adversarial attacks highlight the gap between high-performing RL policies in controlled benchmarks and their reliability in the real world. Understanding and mitigating these vulnerabilities is essential if we want RL to be trusted in safety-critical domains like autonomous driving, robotics, and healthcare.\nHow Vulnerable Are RL Policies to Adversarial Attacks? Reinforcement Learning (RL) has made huge strides in recent years, powering systems that can beat human champions in games and control robots with precision. But just like image classifiers can be fooled by imperceptible changes to inputs, RL policies are also highly vulnerable to adversarial attacks\nThe researchers show that even tiny perturbations‚Äîso small they are invisible to humans‚Äîcan cause RL agents to fail dramatically at test time. Using Atari games as a testbed, they evaluate three common deep RL algorithms: DQN, TRPO, and A3C. The results are clear: all three are susceptible, but DQN policies are especially vulnerable.\nWhite-Box Attacks with FGSM In white-box scenarios, where the adversary has full access to the policy and gradients, attacks are devastating. The team applies the Fast Gradient Sign Method (FGSM) to craft adversarial examples and finds:\nAn ‚Ñì‚àû-norm perturbation with Œµ = 0.001 can slash performance by over 50%. ‚Ñì1-norm attacks are even more powerful‚Äîby changing just a handful of pixels significantly, they can cripple the agent‚Äôs performance. Even policies trained with robust algorithms like TRPO and A3C experience sharp drops when faced with these attacks.\nBlack-Box Attacks and Transferability What if the adversary doesn‚Äôt have full access to the policy network? Surprisingly, attacks are still effective. In black-box settings, adversaries exploit the property of transferability:\nAdversarial examples created for one policy often transfer to another policy trained on the same task. Transferability also extends across algorithms‚Äîfor example, attacks generated against a DQN policy can still reduce the performance of an A3C policy. Effectiveness decreases with less knowledge, but performance still degrades significantly, especially with ‚Ñì1 attacks. Why This Matters The key takeaway is that adversarial examples are not just a computer vision problem. RL policies‚Äîdespite achieving high scores in training‚Äîcan be undermined by imperceptible perturbations. This fragility is dangerous for real-world applications like autonomous driving and robotics, where safety and reliability are non-negotiable.\nUniversal Adversarial Preservation (UAP) To ground these ideas, I re-implemented the 2017 Adversarial Attacks on Neural Network Policies setup (since there was no offical impelmentation) and trained a DQN agent on Atari Pong. After validating the baseline attacks, I implemented Universal Adversarial Perturbations (UAPs) and found that a single, fixed perturbation‚Äîcomputed once and then applied it to all observations, which was enough to consistently derail the policy across episodes and random seeds, without recomputing noise at every timestep. In other words, the attack generalized over time and trajectories, confirming that UAPs exploit stable perceptual quirks of the learned policy rather than moment-by-moment gradients. Practically, this feels much closer to a real-world threat model: an attacker only needs to tamper with the sensor once (think a sticker/overlay on the lens) instead of having high-bandwidth, per-step access to the system. Below you can see the plot of rewards vs Œµ bugdet and videos of different setups.\nbaselines\u0026rsquo; reward over different values of Œµ budget.\nClean ‚Äî original episode (no perturbation). Random uniform noise (Œµ = 2.0). FGSM (Œµ = 2.0) ‚Äî white-box attack. PGD (Œµ = 2.0) ‚Äî iterative, white-box attack. UAP (Œµ = 2.0) ‚Äî image-agnostic. Adversarial Policies: when weird opponents break strong RL TL;DR. Instead of adding pixel noise to an RL agent‚Äôs input, this paper shows you can train a policy that acts in the shared world to induce natural but adversarial observations for the victim‚Äîcausing robust, self-play‚Äìtrained agents to fail in zero-sum MuJoCo games. Fine-tuning helps‚Ä¶until a new adversary is learned.\nThe threat model: natural observations as the ‚Äúperturbation‚Äù In multi-agent settings, an attacker typically can‚Äôt flip pixels or edit state vectors. But it can choose actions that make the victim see carefully crafted, physically plausible observations. Hold the victim fixed and the two-player game becomes a single-agent MDP for the attacker, who learns a policy that elicits bad actions from the victim.\nQuick look:\nYou Shall Not Pass Kick \u0026amp; Defend Sumo (Human) Masked victim vs adversary Setup in a nutshell Victims: strong self-play policies (‚Äúagent zoo‚Äù) across four MuJoCo tasks: Kick \u0026amp; Defend, You Shall Not Pass, Sumo Humans, Sumo Ants. Attacker: trained with PPO for ~20M timesteps‚Äî\u0026lt; 3% of the 680‚Äì1360M timesteps used for the victims‚Äîyet reliably wins. Key idea: adversaries don‚Äôt become great players; they learn poses/motions that generate adversarial observations for the victim. Figures\nTasks used for evaluation.\nAdversary win rate rises quickly despite far fewer timesteps.\nWhat the learned adversary looks like (and why that matters) In Kick \u0026amp; Defend and YSNP, the adversary may never stand up‚Äîit finds contorted, stable poses that make the victim mis-act. In Sumo Humans, where falling loses immediately, it adopts a kneeling/stable stance that still provokes the victim to fall.\nQualitative behaviors: the ‚Äúpoint‚Äù is to confuse, not to excel at the nominal task.\nMasking test: evidence the attack is observational If wins come from manipulating what the victim sees, then hiding the adversary‚Äôs pose from the victim should help. That‚Äôs exactly what happens:\nAgainst normal opponents, the masked victim is (unsurprisingly) worse. Against the adversary, the masked victim becomes nearly immune (e.g., in YSNP: normal victim loses often; masked victim flips the outcome and wins almost always). Masking the adversary‚Äôs position removes the observation channel the attack exploits.\nDimensionality matters Victims are more vulnerable when more opponent DOFs are observed. The attack is stronger in Humanoid (higher-dimensional observed joints) than Ant (lower-dimensional). More controllable joints ‚Üí more ways to steer the victim off-distribution.\nHigher observed dimensionality correlates with higher adversary win rates.\nWhy it works: off-distribution activations Analyses of the victim‚Äôs network show adversarial opponents push internal activations farther from the training manifold than random or lifeless baselines.\nAdversarial policies drive ‚Äúweird‚Äù activations‚Äîmore off-distribution than simple OOD baselines.\nDefenses (and their limits) Fine-tuning the victim on the discovered adversary reduces that adversary‚Äôs success (often down to ~10% in YSNP), but:\nCatastrophic forgetting: performance vs normal opponents degrades (single-adversary fine-tune is worst; dual fine-tune helps but still regresses). Arms race: re-running the attack against the fine-tuned victim yields a new adversary that succeeds again‚Äîoften via a different failure mode (e.g., tripping rather than pure confusion). Before fine-tune After fine-tune (this adversary) Win-rate grid before/after fine-tuning against normal opponents and adversaries.\nTakeaways for practitioners Threat model upgrade: in multi-agent worlds, your attack surface includes other policies that craft natural observations‚Äîno pixel hacks needed. Exploitability check: training a targeted adversary lower-bounds your policy‚Äôs worst-case performance and reveals failure modes missed by self-play. Defense needs diversity: fine-tuning on a single adversary overfits. Prefer population-based or curriculum defenses that rotate diverse opponents and maintain competence vs normals. Robust Communicative Multi-Agent Reinforcement Learning with Active Defense By Yu et al., AAAI 2024\nüåê Why Communication Matters in Multi-Agent RL In multi-agent reinforcement learning (MARL), agents often face partial observability ‚Äî no single agent sees the full environment. To cooperate effectively, agents need to communicate, sharing information about what they see and what actions to take.\nThis communication has powered applications such as robot navigation and traffic light control.\nBut there‚Äôs a catch: in the real world, communication channels are noisy and vulnerable to adversarial attacks. If attackers tamper with even a few messages, the performance of MARL systems can collapse.\nüõ°Ô∏è Enter Active Defense: The Core Idea Yu et al. propose a new active defense strategy. Instead of blindly trusting all messages, agents:\nJudge the reliability of each incoming message using their own observations and history (hidden states). Adjust the influence of unreliable messages by reducing their weight in the decision process. üëâ Example: If one agent already searched location (1,1) and found nothing, but receives a message saying ‚ÄúTarget at (1,1)‚Äù, it can spot the inconsistency and downweight that message.\nüß© The ADMAC Framework The authors introduce Active Defense Multi-Agent Communication (ADMAC), which has two key components:\nReliability Estimator: A classifier that predicts whether a message is reliable (weight close to 1) or unreliable (weight close to 0). Decomposable Message Aggregation Policy Net: A structure that breaks down the influence of each message into an action preference vector, making it possible to scale its impact up or down. This allows agents to combine their own knowledge with weighted messages to make more robust decisions.\nThe figure above shows how an agent in the ADMAC framework generates its action distribution by combining its own observations with incoming messages from other agents:\nHidden state update: The agent maintains a hidden state (h·µ¢·µó‚Åª¬π), which is updated using the observation (o·µ¢·µó) through the GRU module f_HP. This captures past and current information. Base action preference: From the updated hidden state, the agent generates a base preference vector via f_BP, representing what it would do independently. Message influence: Each received message (m‚ÇÅ·µó, ‚Ä¶, m_N·µó) is processed with the observation through f_MP, producing a message-based action preference vector. Reliability estimation: A reliability estimator f_R evaluates each message, assigning it a weight w·µ¢(m‚±º·µó) that reflects how trustworthy it seems. Aggregation: The agent sums its base vector with all weighted message vectors to form a total action preference vector (v·µ¢·µó). Final decision: Applying a Softmax function converts this vector into a probability distribution over actions, from which the agent selects its next move. By downweighting unreliable messages, ADMAC enables agents to remain robust against malicious communication while still leveraging useful information from peers.\nReferences:\nHuang, S. H., Papernot, N., Goodfellow, I. J., Duan, Y., \u0026amp; Abbeel, P. (2017). Adversarial Attacks on Neural Network Policies. Gleave, A., Dennis, M., Kant, N., Wild, C., Levine, S., \u0026amp; Russell, S. (2019). Adversarial Policies: Attacking Deep Reinforcement Learning. Yu, L., Qiu, Y., Yao, Q., Shen, Y., Zhang, X., \u0026amp; Wang, J. (2023). Robust Communicative Multi-Agent Reinforcement Learning with Active Defense. Guo, W., Wu, X., Huang, S., \u0026amp; Xing, X. (2021). Adversarial Policy Learning in Two-player Competitive Games. https://www.youtube.com/watch?v=-_j-fmVpn_s https://rll.berkeley.edu/adversarial/ ","permalink":"http://localhost:1313/posts/adversarial-rl/","summary":"\u003ch1 id=\"adversarial-attacks-on-reinforcement-learning-policies\"\u003eAdversarial Attacks on Reinforcement Learning Policies\u003c/h1\u003e\n\u003cp\u003eReinforcement Learning (RL) has powered breakthroughs in games, robotics, and autonomous systems. But just like image classifiers can be fooled by carefully crafted adversarial examples, RL agents are also vulnerable to \u003cstrong\u003eadversarial attacks\u003c/strong\u003e. These attacks can manipulate the agent‚Äôs perception of the environment or its decision process, leading to unsafe or suboptimal behavior.\u003c/p\u003e\n\u003ch2 id=\"what-do-adversarial-attacks-mean-in-rl\"\u003eWhat Do Adversarial Attacks Mean in RL?\u003c/h2\u003e\n\u003cp\u003eIn RL, an agent interacts with an environment, observes a state, takes an action, and receives a reward. An adversarial attack interferes with this process‚Äîoften by perturbing the input observations‚Äîso the agent chooses actions that look reasonable to it but are actually harmful.\u003c/p\u003e","title":"Adversarial RL"},{"content":"Actor-Critic vs. Value-Based: Empirical Trade-offs Introduction In Reinforcement Learning (RL), one of the fundamental questions is:\nShould we focus on learning the value of states, or should we directly learn the optimal policy?\nBefore diving into this comparison, let‚Äôs briefly recall what RL is: a branch of machine learning in which an agent interacts with an environment to learn how to make decisions that maximize cumulative reward.\nTraditionally, RL algorithms fall into two main families:\nValue-Based methods : which aim to estimate the value of states or state‚Äìaction pairs. Policy-Based methods : which directly optimize a policy that maps states to actions. Actor-Critic algorithms combine the strengths of both worlds, simultaneously learning a value function and an explicit policy. This hybrid structure can often lead to more stable and efficient learning.\nValue-Based and Actor-Critic approaches represent fundamentally different perspectives: one focuses solely on learning state values, while the other integrates both value and policy learning. Comparing these two perspectives helps us better understand the impact of incorporating value or policy components in different environments.\nIn this project, we empirically evaluate these two families in both discrete and continuous action spaces. Four representative algorithms ( DQN, A2C, NAF, and SAC ) were implemented, along with a user-friendly graphical user interface (GUI) for training and evaluation.\nOur ultimate goal is to analyze trade-offs such as convergence speed, learning stability, and final performance across diverse scenarios.\nBackground The reinforcement learning algorithms used in this project fall into two main families:\nValue-Based\nIn this approach, the agent learns only a value function, such as $Q(s, a)$.\nThe policy is derived implicitly by selecting the action with the highest value:\n$\\pi(s) = \\arg\\max_a Q(s,a)$\nThis method is typically simpler, more stable, and computationally efficient.\nHowever, it faces limitations when dealing with continuous action spaces.\nExample algorithms: DQN, NAF.\nValue-Based methods are often well-suited for discrete action spaces with relatively small state‚Äìaction domains, where enumerating or approximating the value for each action is feasible.\nActor-Critic\nIn this framework, the agent consists of two components:\nActor : a parameterized policy that directly produces actions. Critic : a value function that evaluates the Actor‚Äôs performance and guides its updates. This combination can provide greater learning stability, improved performance in complex environments, and high flexibility in continuous action spaces.\nExample algorithms: A2C, SAC.\nActor-Critic methods are generally more suitable for continuous or high-dimensional action spaces, as the Actor can output actions directly without exhaustive value estimation.\nMethodology Project Design This project was designed to perform an empirical comparison between two major families of reinforcement learning algorithms: Value-Based and Actor-Critic.\nFour representative algorithms were selected and implemented in diverse discrete and continuous environments.\nTraining, evaluation, and comparison were carried out through a fully interactive, user-friendly graphical interface.\nImplemented Algorithms Representative algorithms from the two families were selected based on their reported performance in different environments according to the literature.\nFor each algorithm, the training procedure was reproduced in accordance with its original paper.\nThe overall structure of each algorithm is summarized below:\nAlgorithm Family Action Space Description Reference Deep Q-Network (DQN) Value-Based Discrete Uses experience replay and a fixed target network to stabilize learning. 1 Normalized Advantage Function (NAF) Value-Based Continuous Value-based method for continuous spaces using a specific Q-structure to simplify action selection. 2 Advantage Actor-Critic (A2C) Actor-Critic Discrete/Continuous Direct policy optimization guided by an advantage function. 3 Soft Actor-Critic (SAC) Actor-Critic Continuous Off-policy actor-critic method maximizing entropy for stability in complex environments. 4 Deep Q-Network (DQN) The pseudocode of DQN highlights the use of experience replay and a target network, which together reduce correlations between samples and stabilize training.\nNormalized Advantage Function (NAF) NAF handles continuous action spaces by constraining the Q-function into a quadratic form, which makes action selection computationally efficient.\nAdvantage Actor-Critic (A2C) A2C directly optimizes a parameterized policy (Actor) with guidance from the Critic, using advantage estimation to reduce gradient variance and improve learning stability.\nSoft Actor-Critic (SAC) SAC introduces entropy maximization in the objective, encouraging exploration and robustness in complex continuous environments.\nEnvironments There were many 2D and 3D environments available so that we could compare them.\nThe 12 famous environments are listed below:\nFour environments from the Gym library were selected to provide a diverse set of challenges that cover both discrete and continuous action spaces, as well as varying levels of complexity and dynamics:\nMountainCar-v0 (Discrete): A classic control problem where the agent must drive a car up a hill using discrete acceleration commands. This environment tests basic exploration and planning in a low-dimensional, discrete action space. Pendulum-v1 (Continuous): Requires applying continuous torque to keep a pendulum upright. This environment is ideal for evaluating continuous control algorithms and stabilizing dynamics. FrozenLake-v1 (Discrete): A gridworld task where the agent navigates an icy lake to reach a goal while avoiding holes. This environment emphasizes decision-making under uncertainty in a discrete setting. HalfCheetah-v4 (Continuous): A high-dimensional continuous control environment where the agent controls a bipedal cheetah to run efficiently. It challenges advanced continuous control and balance strategies. These environments were chosen to allow a comprehensive comparison of algorithms across different action types, state complexities, and control challenges.\nEnvironment Type Description MountainCar-v0 Discrete Drive a car up a hill by controlling acceleration in a discrete space. Pendulum-v1 Continuous Apply torque to keep a pendulum upright and stable. FrozenLake-v1 Discrete Navigate an icy grid to reach the goal without falling into holes. HalfCheetah-v4 Continuous Control the speed and balance of a simulated bipedal cheetah for fast running. Environment snapshots were recorded during training and appear as GIFs in the Results section.\nConfiguration and Evaluation Algorithms were run with optimized settings for each environment. Key hyperparameters (learning rate, Œ≥, batch size, buffer size) were tuned through trial-and-error, leveraging existing GitHub implementations for optimal performance. Comparisons were based on convergence speed, training stability, and final performance. Example configurations:\nAlgorithm Environment Œ≥ Learning Rate Batch Size Buffer Size DQN FrozenLake 0.93 6e-4 32 4,000 A2C MountainCar 0.96 1e-3 ‚Äì ‚Äì NAF Pendulum 0.99 3e-4 64 400,000 SAC HalfCheetah 0.99 3e-4 256 1,000,000 Graphical User Interface (GUI) A user-friendly GUI was developed to simplify training and comparing algorithms, enabling full project execution without direct coding.The code is available at: This Github Link The project structure is as follows:\nProject_Code/\r‚îú‚îÄ‚îÄ plots/\r‚îÇ ‚îî‚îÄ‚îÄ learning_curves/\r‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ Continuous/\r‚îÇ ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ Pendulum-v1/\r‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ SAC/\r‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ NAF/\r‚îÇ ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ HalfCheetah-v4/\r‚îÇ ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ SAC/\r‚îÇ ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ NAF/\r‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ Discrete/\r‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ FrozenLake-v1/\r‚îÇ ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ DQN/\r‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ 4x4\r‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ 8x8\r‚îÇ ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ A2C/\r‚îÇ ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ 4x4\r‚îÇ ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ 8x8\r‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ MountainCar-v0/\r‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ DQN/\r‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ A2C/\r‚îÇ ‚îú‚îÄ‚îÄ comparison_table.png\r‚îÇ ‚îî‚îÄ‚îÄ directory_structure.png\r‚îú‚îÄ‚îÄ requirements/\r‚îÇ ‚îú‚îÄ‚îÄ base.txt\r‚îÇ ‚îú‚îÄ‚îÄ dev.txt\r‚îÇ ‚îú‚îÄ‚îÄ env.txt\r‚îÇ ‚îî‚îÄ‚îÄ config.py\r‚îú‚îÄ‚îÄ src/\r‚îÇ ‚îú‚îÄ‚îÄ agents/\r‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ a2c.py\r‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ dqn.py\r‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ naf.py\r‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ sac.py\r‚îÇ ‚îú‚îÄ‚îÄ envs/\r‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ continuous_envs.py\r‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ discrete_envs.py\r‚îÇ ‚îú‚îÄ‚îÄ main.py\r‚îÇ ‚îú‚îÄ‚îÄ train.py\r‚îÇ ‚îî‚îÄ‚îÄ utils.py\r‚îú‚îÄ‚îÄ videos/\r‚îÇ ‚îú‚îÄ‚îÄ Continuous/\r‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ Pendulum-v1/\r‚îÇ ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ SAC/\r‚îÇ ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ NAF/\r‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ HalfCheetah-v4/\r‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ SAC/\r‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ NAF/\r‚îÇ ‚îî‚îÄ‚îÄ Discrete/\r‚îÇ ‚îú‚îÄ‚îÄ FrozenLake-v1/\r‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ DQN/\r‚îÇ ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ 4x4\r‚îÇ ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ 8x8\r‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ A2C/\r‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ 4x4\r‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ 8x8\r‚îÇ ‚îî‚îÄ‚îÄ MountainCar-v0/\r‚îÇ ‚îú‚îÄ‚îÄ DQN/\r‚îÇ ‚îî‚îÄ‚îÄ A2C/\r‚îú‚îÄ‚îÄ models/\r‚îÇ ‚îú‚îÄ‚îÄ Continuous/\r‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ Pendulum-v1/\r‚îÇ ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ SAC/\r‚îÇ ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ NAF/\r‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ HalfCheetah-v4/\r‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ SAC/\r‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ NAF/\r‚îÇ ‚îî‚îÄ‚îÄ Discrete/\r‚îÇ ‚îú‚îÄ‚îÄ FrozenLake-v1/\r‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ DQN/\r‚îÇ ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ 4x4\r‚îÇ ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ 8x8\r‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ A2C/\r‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ 4x4\r‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ 8x8\r‚îÇ ‚îî‚îÄ‚îÄ MountainCar-v0/\r‚îÇ ‚îú‚îÄ‚îÄ DQN/\r‚îÇ ‚îî‚îÄ‚îÄ A2C/\r‚îî‚îÄ‚îÄ README.md Main features:\nSelect algorithm, environment, action space type (discrete/continuous), and execution mode (train/test) with just a few clicks. Launch training with a Run button, and view results via Show Plots and Show Videos. Compare algorithms interactively using a dedicated Compare Algorithms window. Display key settings such as hyperparameters and project structure. Real-time console output for monitoring execution status and system messages. Interactive Mobile Application In addition to the desktop GUI, a mobile application for Android and iOS has been developed to provide interactive access to the project. By simply scanning the poster, users can explore various features, including:\nViewing videos of agent executions in different environments. Opening the project‚Äôs website for additional resources and documentation. Displaying the poster digitally for interactive exploration. Comparing learning curves and results across different algorithms. The images below showcase some sections of the app interface:\nResults This section presents the empirical evaluation of four reinforcement learning algorithms (DQN, NAF, A2C, and SAC) from the Value-Based and Actor-Critic families, across both discrete and continuous action space environments. The performance is analyzed based on final reward, convergence speed, and training stability, supported by quantitative metrics, qualitative visualizations (GIFs), and learning curves with a moving average (MA) applied to reduce noise. The experiments were conducted using the Gymnasium library, with optimized hyperparameters (see Methodology for details) and a fixed random seed for reproducibility. Training was performed on a single NVIDIA RTX 4050 GPU, with average runtimes of 1‚Äì4 hours per algorithm-environment pair.\nDiscrete Environments The discrete action space environments tested were FrozenLake-v1 (8x8 grid) and MountainCar-v0, which challenge the algorithms with stochastic transitions and sparse rewards, respectively. The table below summarizes the performance, followed by detailed analyses and visualizations.\nEnvironment Algorithm Final Reward (Avg ¬± Std) Convergence Speed (Episodes to 90% of Max) Stability (Std of Reward) FrozenLake-v1 DQN 0.98 ¬± 0.14 ~1,475 0.50 A2C 1.00 ¬± 0.00 ~1,209 0.48 MountainCar-v0 DQN -22.21 ¬± 79.32 ~2,000 81.50 A2C -27.87 ¬± 62.35 ~2,000 40.83 FrozenLake-v1 In FrozenLake-v1, a stochastic gridworld, A2C outperformed DQN in final reward (1.00 vs. 0.98 success rate) and converged faster (~1209 vs. ~1475 episodes to reach 90% of max reward). A2C‚Äôs advantage estimation provided greater stability, as evidenced by its lower standard deviation (0.48 vs. 0.50). The GIF below illustrates agent behaviors, showing A2C‚Äôs smoother navigation to the goal compared to DQN‚Äôs occasional missteps.\nReward comparison in FrozenLake-v1: A2C converges quickly and maintains stable performance compared to DQN.\nMountainCar-v0 In MountainCar-v0, a deterministic environment with sparse rewards, DQN achieved a better final reward (-22.21 vs. -27.87 timesteps to goal) but both algorithms converged at similar speeds (~2000 episodes). However, A2C exhibited greater stability (std of 40.83 vs. 81.50), avoiding large oscillations in learning. The GIF below shows DQN‚Äôs quicker ascent to the hilltop, while A2C maintains more consistent swings.\nReward comparison in MountainCar-v0: DQN converges faster and reaches higher rewards, while A2C shows greater stability.\nContinuous Environments The continuous action space environments tested were Pendulum-v1 and HalfCheetah-v4, which require precise control and balance in low- and high-dimensional settings, respectively. The table below summarizes the performance.\nEnvironment Algorithm Final Reward (Avg ¬± Std) Convergence Speed (Episodes to 90% of Max) Stability (Std of Reward) Pendulum-v1 NAF -141.17 ¬± 85.58 ~246 199.46 SAC 287.66 ¬± 62.38 ~152 113.23 HalfCheetah-v4 NAF 3,693.35 ¬± 575.60 ~862 1,077.01 SAC 10,247.42 ¬± 584.31 ~1,127 2,493.55 Pendulum-v1 In Pendulum-v1, SAC significantly outperformed NAF in final reward (287.66 vs. -141.17, higher is better) and converged faster (~152 vs. ~246 episodes). SAC‚Äôs entropy maximization ensured smoother learning, with a standard deviation of 113.23 compared to NAF‚Äôs 199.46. The GIF below highlights SAC‚Äôs ability to stabilize the pendulum upright, while NAF struggles with inconsistent torque.\nReward comparison in Pendulum-v1: SAC achieves higher rewards with smoother convergence compared to NAF.\nHalfCheetah-v4 In HalfCheetah-v4, a high-dimensional control task, SAC achieved a much higher final reward (10247.42 vs. 3693.35) but converged slightly slower (~1127 vs. ~862 episodes). SAC‚Äôs stability (std of 2493.55 vs. 1077.01) reflects its robustness in complex dynamics, though NAF shows lower variance. The GIF below shows SAC‚Äôs fluid running motion compared to NAF‚Äôs less coordinated movements.\nReward comparison in HalfCheetah-v4: SAC consistently outperforms NAF in both speed and stability.\nDiscussion \u0026amp; Conclusion This project examined the performance differences between Value-Based and Actor-Critic algorithms in both discrete and continuous environments.\nThe experimental results indicate that no single algorithm is universally superior; rather, the environment characteristics and action space type play a decisive role in determining performance.\nAnalysis \u0026amp; Interpretation Continuous Action Spaces SAC consistently outperformed NAF in both Pendulum-v1 and HalfCheetah-v4, thanks to its entropy maximization strategy, which promotes exploration and robustness. NAF‚Äôs fixed quadratic Q-function structure limited its flexibility in high-dimensional or complex tasks, leading to slower convergence and higher variance in rewards. SAC‚Äôs ability to directly optimize a stochastic policy made it particularly effective in continuous control scenarios.\nDiscrete Action Spaces In FrozenLake-v1, a stochastic environment, A2C‚Äôs stability (due to advantage estimation) gave it an edge over DQN, achieving higher success rates and faster convergence. In MountainCar-v0, a deterministic environment with a small action space, DQN‚Äôs value-based approach excelled in final reward and convergence speed, though A2C remained more stable. This highlights the suitability of Value-Based methods for simpler, deterministic settings and Actor-Critic methods for stochastic or complex environments.\nKey Findings: In simple discrete environments, such as FrozenLake, Value-Based algorithms (e.g., DQN) achieved competitive performance, but Actor-Critic algorithms (e.g., A2C) showed faster convergence and more stable learning. In continuous and more complex environments, Actor-Critic algorithms ‚Äî particularly SAC ‚Äî outperformed their Value-Based counterparts in terms of final reward and convergence speed. Observed Trade-offs: Aspect Value-Based Actor-Critic Simplicity of implementation Yes More complex Initial learning speed High in simple environments Depends on tuning Training stability More oscillations More stable Suitability for continuous spaces Not always Yes Overall, the choice between Value-Based and Actor-Critic methods should be guided by the nature of the task, the complexity of the environment, and the available computational budget.\nObservations Based on Environment Characteristics Our experimental results further reveal that the nature of the environment‚Äîin terms of action space, state space, and reward structure‚Äîsignificantly impacts algorithm performance:\nAction Space (Discrete vs. Continuous) Discrete Action Spaces: Value-Based algorithms like DQN tend to perform competitively, especially in small and low-dimensional discrete action spaces. They converge quickly and reliably, but may struggle when the action space grows larger or stochasticity increases. Actor-Critic methods such as A2C can still provide improved stability in these scenarios, especially under stochastic transitions. Continuous Action Spaces: Actor-Critic methods (e.g. SAC) dominate due to their ability to output continuous actions directly. Value-Based methods require specialized approximations (like NAF), which often limit flexibility and performance in high-dimensional or continuous control tasks. State Space (Low-dimensional vs. High-dimensional) Low-dimensional states (e.g., MountainCar, FrozenLake) generally favor Value-Based methods, which can efficiently enumerate or approximate Q-values. High-dimensional states (e.g., HalfCheetah) require the policy network of Actor-Critic methods to generalize across large state spaces. These methods better handle complex dynamics and correlations among state variables. Reward Structure (Sparse vs. Dense) Sparse Reward Environments (e.g., FrozenLake) challenge Value-Based methods to propagate value signals efficiently, potentially slowing convergence. Actor-Critic algorithms can leverage advantage estimation and policy gradients to maintain learning stability even with sparse rewards. Dense Reward Environments (e.g., HalfCheetah, Pendulum) allow both families to learn effectively, but Actor-Critic methods often achieve smoother and faster convergence due to direct policy optimization combined with value guidance. The interplay between action space type, state space complexity, and reward sparsity fundamentally shapes the suitability of each algorithm. In general:\nDiscrete + Low-dimensional + Dense reward ‚Üí Value-Based methods are competitive. Continuous + High-dimensional + Sparse or Dense reward ‚Üí Actor-Critic methods provide superior learning stability and higher final performance. These insights complement the empirical trade-offs already observed in our study, providing a more nuanced understanding of when and why certain RL algorithms excel under different environment characteristics.\nLimitations and Future Work Limitations The present project, which evaluates the performance of reinforcement learning algorithms (DQN, A2C, NAF, and SAC) in discrete (FrozenLake-v1 and MountainCar-v0) and continuous (Pendulum-v1 and HalfCheetah-v4) environments, provides valuable insights but is subject to several limitations, outlined below:\nLimited Number of Environments:\nThe study only examines four specific environments, which may not provide sufficient diversity to generalize results across all types of reinforcement learning environments. More complex environments with larger state or action spaces or different dynamics could yield different outcomes. Lack of Random Seed Variation:\nThe reported results are based on a single run or an average of a limited number of runs. Conducting multiple experiments with different random seeds could better demonstrate the robustness and reliability of the results. Focus on Specific Metrics:\nThe evaluation metrics (final reward average, convergence speed, and stability) cover only certain aspects of algorithm performance. Other metrics, such as computational efficiency, training time, or robustness to environmental noise, were not assessed. Future Work To build upon the findings of this project, which evaluated the performance of reinforcement learning algorithms (DQN, A2C, NAF, and SAC) in discrete (FrozenLake-v1, MountainCar-v0) and continuous (Pendulum-v1, HalfCheetah-v4) environments, several directions for future research and development can be pursued to address the limitations and extend the scope of the study:\nBroader Range of Environments:\nFuture work could include testing the algorithms on a wider variety of environments, such as those with larger state and action spaces, partially observable states (e.g., POMDPs), or real-world-inspired tasks. This would help validate the generalizability of the observed performance trends. Incorporating Random Seed Variations:\nConducting multiple runs with different random seeds would improve the robustness of results and allow for statistical analysis of performance variability, ensuring that conclusions are not biased by specific initial conditions. Evaluation of Additional Metrics:\nFuture studies could incorporate metrics such as computational efficiency, memory usage, training time, and robustness to environmental perturbations (e.g., noise or dynamic changes). This would provide a more holistic view of algorithm suitability for practical applications. Real-World Application: Robotics The insights gained from this project have significant potential for real-world applications, particularly in robotics, where reinforcement learning can enable autonomous systems to perform complex tasks. The following outlines how the evaluated algorithms could be applied and extended in robotics contexts:\nRobotic Manipulation:\nAlgorithms like SAC, which performed well in continuous control tasks (e.g., Pendulum-v1, HalfCheetah-v4), could be applied to robotic arms for tasks such as grasping, object manipulation, or assembly. SAC‚Äôs ability to handle continuous action spaces makes it suitable for precise control in high-dimensional settings. Autonomous Navigation:\nDiscrete action space algorithms like DQN and A2C, tested in environments like FrozenLake-v1, could be adapted for robot navigation in grid-like or structured environments (e.g., warehouse robots). A2C‚Äôs stability in stochastic settings could be particularly useful for navigating dynamic or uncertain environments. Locomotion and Mobility:\nThe success of SAC in HalfCheetah-v4 suggests its potential for controlling legged robots or humanoid robots for locomotion tasks. Future work could involve applying SAC to real-world robotic platforms to achieve robust and efficient walking or running behaviors. These future research directions and real-world applications highlight the potential to extend the current study‚Äôs findings to more diverse and practical scenarios. By addressing the identified limitations and applying the algorithms to robotics, this work can contribute to the development of more robust, efficient, and adaptable autonomous systems.\nReferences A2C (Advantage Actor-Critic):\nMnih, V., et al. (2016). \u0026ldquo;Asynchronous Methods for Deep Reinforcement Learning.\u0026rdquo; ICML 2016. Paper Stable-Baselines3 A2C Implementation: GitHub SAC (Soft Actor-Critic):\nHaarnoja, T., et al. (2018). \u0026ldquo;Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor.\u0026rdquo; ICML 2018. Paper Stable-Baselines3 SAC Implementation: GitHub DQN (Deep Q-Network):\nMnih, V., et al. (2015). \u0026ldquo;Human-level control through deep reinforcement learning.\u0026rdquo; Nature, 518(7540), 529-533. Paper Stable-Baselines3 DQN Implementation: GitHub NAF (Normalized Advantage Function):\nGu, S., et al. (2016). \u0026ldquo;Continuous Deep Q-Learning with Model-based Acceleration.\u0026rdquo; ICML 2016. Paper Gymnasium:\nOfficial Gymnasium Documentation: Gymnasium ","permalink":"http://localhost:1313/posts/actor-critic-vs-value-based-empirical-trade-offs/","summary":"This study compares Value-Based (DQN, NAF) and Actor-Critic (A2C, SAC) reinforcement learning algorithms in diverse environments (MountainCar-v0, Pendulum-v1, FrozenLake-v1, HalfCheetah-v4). Through empirical evaluation, we analyze trade-offs in convergence speed, learning stability, and final performance, supported by a user-friendly GUI and mobile application for interactive training and visualization.","title":"Actor-Critic vs. Value-Based: Empirical Trade-offs"},{"content":"Introduction This paper critically examines the assumptions commonly accepted in modeling reinforcement learning problems, suggesting that these assumptions may impede progress in the field. The authors refer to these assumptions as \u0026ldquo;dogmas.\u0026rdquo;\nDogma: A fixed, especially religious, belief or set of beliefs that people are expected to accept without any doubts. ‚ÄîFrom the Cambridge Advanced Learner\u0026rsquo;s Dictionary \u0026amp; Thesaurus\nThe paper introduces three central dogmas:\nThe Environment Spotlight Learning as Finding a Solution The Reward Hypothesis (although not exactly a dogma) The author argues the true reinforcement learning landscape is actualy like this,\nIn the words of Rich Sutton:\nRL can be viewed as a microcosm of the whole AI problem.\nHowever, today\u0026rsquo;s RL landscape is overly simplified,\nThese three dogmas are responsible for narrowing the potential of RL,\nThe authors propose that we consider moving beyond these dogmas,\nTo reclaim the true landscape of RL,\nBackground The authors reference Thomas Kuhn\u0026rsquo;s book, \u0026ldquo;The Structure of Scientific Revolutions\u0026rdquo;,\nKuhn distinguishes between two phases of scientific activity,\nNormal Science: Resembling puzzle-solving. Revolutionary Phase: Involving a fundamental rethinking of the values, methods, and commitments of science, which Kuhn calls a \u0026ldquo;paradigm.\u0026rdquo; Here\u0026rsquo;s an example of a previous paradigm shift in science:\nThe authors explore the paradigm shift needed in RL:\nDogma One: The Environment Spotlight The first dogma we call the environment spotlight, which refers to our collective focus on modeling environments and environment-centric concepts rather than agents.\nWhat do we mean when we say that we focus on environments? We suggest that it is easy to answer only one of the following two questions:\nWhat is at least one canonical mathematical model of an environment in RL?\nMDP and its variants! And we define everything in terms of it. By embracing the MDP, we are allowed to import a variety of fundamental results and algorithms that define much of our primary research objectives and pathways. For example, we know every MDP has at least one deterministic, optimal, stationary policy, and that dynamic programming can be used to identify this policy. What is at least one canonical mathematical model of an agent in RL?\nIn contrast, this question has no clear answer! The author suggests it is important to define, model, and analyse agents in addition to environments. We should build toward a canonical mathematical model of an agent that can open us to the possibility of discovering general laws governing agents (if they exist).\nDogma Two: Learning as Finding a Solution The second dogma is embedded in the way we treat the concept of learning. We tend to view learning as a finite process involving the search for‚Äîand eventual discovery of‚Äîa solution to a given task.\nWe tend to implicitly assume that the learning agents we design will eventually find a solution to the task at hand, at which point learning can cease. Such agents can be understood as searching through a space of representable functions that captures the possible action-selection strategies available to an agent, similar to the Problem Space Hypothesis, and, critically, this space contains at least one function‚Äîsuch as the optimal policy of an MDP‚Äîthat is of sufficient quality to consider the task of interested solved. Often, we are then interested in designing learning agents that are guaranteed to converge to such an endpoint, at which point the agent can stop its search (and thus, stop its learning).\nThe author suggests to embrace the view that learning can also be treated as adaptation. As a consequence, our focus will drift away from optimality and toward a version of the RL problem in which agents continually improve, rather than focus on agents that are trying to solve a specific problem.\nWhen we move away from optimality,\nHow do we think about evaluation? How, precisely, can we define this form of learning, and differentiate it from others? What are the basic algorithmic building blocks that carry out this form of learning, and how are they different from the algorithms we use today? Do our standard analysis tools such as regret and sample complexity still apply? These questions are important, and require reorienting around this alternate view of learning.\nThe authors introduce the book \u0026ldquo;Finite and Infinite Games\u0026rdquo;,\nAnd the concept of Finite and Infinite Games is summarized in the following quote,\nThere are at least two kinds of games, One could be called finite; the other infinite. A finite game is played for the purpose of winning, an infinite game for the purpose of continuing the play.\nAnd argues alignment is an infinite game.\nDogma Three: The Reward Hypothesis The third dogma is the reward hypothesis, which states \u0026ldquo;All of what we mean by goals and purposes can be well thought of as maximization of the expected value of the cumulative sum of a received scalar signal (reward).\u0026rdquo;\nThe authors argue that the reward hypothesis is not truly a dogma. Nevertheless, it is crucial to understand its nuances as we continue to design intelligent agents.\nThe reward hypothesis basically says,\nIn recent analysis by [2] fully characterizes the implicit conditions required for the hypothesis to be true. These conditions come in two forms. First, [2] provide a pair of interpretative assumptions that clarify what it would mean for the reward hypothesis to be true or false‚Äîroughly, these amount to saying two things (brwon doors).\nFirst, that \u0026ldquo;goals and purposes\u0026rdquo; can be understood in terms of a preference relation on possible outcomes. Second, that a reward function captures these preferences if the ordering over agents induced by value functions matches that of the ordering induced by preference on agent outcomes. This leads to the following conjecture,\nThen, under this interpretation, a Markov reward function exists to capture a preference relation if and only if the preference relation satisfies the four von Neumann-Morgenstern axioms, and a fifth Bowling et al. call $\\gamma$-Temporal Indifference.\nAxiom 1: Completeness \u0026gt; You have a preference between every outcome pair.\nYou can always compare any two choices. Axiom 2: Transitivity \u0026gt; No preference cycles.\nIf you like chocolate more than vanilla, and vanilla more than strawberry, you must like chocolate more than strawberry. Axiom 3: Independence \u0026gt; Independent alternatives can\u0026rsquo;t change your preference.\nIf you like pizza more than salad, and you have to choose between a lottery of pizza or ice cream and a lottery of salad or ice cream, you should still prefer the pizza lottery over the salad lottery. Axiom 4: Continuity \u0026gt; There is always a break even chance.\nImagine you like a 100 dollar bill more than a 50 dollar bill, and a 50 dollar bill more than a 1 dollar bill. There should be a scenario where getting a chance at 100 dollar and 1 dollar, with certain probabilities, is equally good as getting the 50 dollar for sure. These 4 axioms are called the von Neumann-Morgenstern axioms.\nAxiom 5: Temporal $\\boldsymbol{\\gamma}$-Indifference \u0026gt; Discounting is consistent throughout time. Temporal $\\gamma$-indifference says that if you are indifferent between receiving a reward at time $t$ and receiving the same reward at time $t+1$, then your preference should not change if we move both time points by the same amount. For instance, if you don\u0026rsquo;t care whether you get a candy today or tomorrow, then you should also not care whether you get the candy next week or the week after. Taking these axioms into account, the reward conjecture becomes the reward theorem,\nIt is essential to consider that people do not always conform to these axioms, and human preferences can vary.\nIt is important that we are aware of the implicit restrictions we are placing on the viable goals and purposes under consideration when we represent a goal or purpose through a reward signal. We should become familiar with the requirements imposed by the five axioms, and be aware of what specifically we might be giving up when we choose to write down a reward function.\nSee Also David Abel Presentation @ ICML 2023 David Abel Personal Website Mark Ho Personal Website Anna Harutyunyan Personal Website References [1] Abel, David, Mark K. Ho, and Anna Harutyunyan. \u0026ldquo;Three Dogmas of Reinforcement Learning.\u0026rdquo; arXiv preprint arXiv:2407.10583 (2024).\n[2] Bowling, Michael, et al. \u0026ldquo;Settling the reward hypothesis.\u0026rdquo; International Conference on Machine Learning. PMLR, 2023.\n","permalink":"http://localhost:1313/posts/three-dogmas-of-reinforcement-learning/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eThis paper critically examines the assumptions commonly accepted in modeling reinforcement learning problems, suggesting that these assumptions may impede progress in the field. The authors refer to these assumptions as \u0026ldquo;dogmas.\u0026rdquo;\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003eDogma: A fixed, especially religious, belief or set of beliefs that people are expected to accept without any doubts.\u003c/strong\u003e \u003cem\u003e‚ÄîFrom the \u003ca href=\"https://dictionary.cambridge.org/dictionary/english\"\u003eCambridge Advanced Learner\u0026rsquo;s Dictionary \u0026amp; Thesaurus\u003c/a\u003e\u003c/em\u003e\u003c/p\u003e\u003c/blockquote\u003e\n\u003cp\u003eThe paper introduces three central dogmas:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eThe Environment Spotlight\u003c/li\u003e\n\u003cli\u003eLearning as Finding a Solution\u003c/li\u003e\n\u003cli\u003eThe Reward Hypothesis (although not exactly a dogma)\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eThe author argues the true reinforcement learning landscape is actualy like this,\u003c/p\u003e","title":"Three Dogmas of Reinforcement Learning"},{"content":"Introduction This paper studies the problem of zero-shot generalization in reinforcement learning and introduces an algorithm that trains an ensamble of maximum reward seeking agents and an maximum entropy agent. At test time, either the ensemble agrees on an action, and we generalize well, or we take exploratory actions with the help of maximum entropy agent and drive us to a novel part of the state space, where the ensemble may potentially agree again. This method combined with an invariance based approach achieves new state-of-the-art results on ProcGen.\nBackground I know it\u0026rsquo;s a lot to take in! You may be wondering:\nWhat is reinforcement learning? ü•∫ What generalization means for RL? ü•≤ What is zero-shot generalization? ü•π What are max reward and max entropy agents?! ‚òπÔ∏è What is an ensamble of them?!! üòü What is an invariance based approach? üòì And what the heck is ProcGen?!!! üò† Don\u0026rsquo;t worry! We are going to cover all of that and more! And you are going to fully understand this paper and finish reading this article with an smile üôÇ!\nWhat is reinforcement learning? Reinforcement learning is learning what to do‚Äîhow to map situations to actions‚Äîso as to maximize a numerical reward signal. Imagine yourself right now, in reinforcement learning terms, you are an agent and everything that is not you, is your environment. You perceive the world through your senses (e.g., eyes, ears, etc.) and what you perceive turns into electrical signals that your brain processes to form an understanding of your surroundings (state). Based on this understanding, you make decisions (actions) with the goal of achieving the best possible outcome for yourself (reward).\nIn a more formal sense, reinforcement learning involves the following components:\nAgent: The learner or decision maker (e.g., you). Environment: Everything the agent interacts with (e.g., the world around you). State ($s$): A representation of the current situation of the agent within the environment (e.g., what you see, hear, and feel at any given moment). Actions ($a$): The set of all possible moves the agent can make (e.g., moving your hand, walking, speaking). Reward ($r$): The feedback received from the environment in response to the agent‚Äôs action (e.g., pleasure from eating food, pain from touching a hot surface). Policy ($\\pi$): A strategy used by the agent to decide which actions to take based on the current state (e.g., your habits and decision-making processes). Value Function ($V$): A function that estimates the expected cumulative reward of being in a certain state and following a particular policy (e.g., your prediction of future happiness based on current actions). The objective of the agent is to develop a policy that maximizes the total cumulative reward over time. This is typically achieved through a process of exploration (trying out new actions to discover their effects) and exploitation (using known actions that yield high rewards).\nIn mathematical terms, the goal is to find a policy $\\pi$ that maximizes the expected return $G_t$, which is the cumulative sum of discounted rewards:\n$$ G_t = r_{t+1} + \\gamma r_{t+2} + \\gamma^2 r_{t+3} + \\cdots $$\nwhere $\\gamma$ (0 ‚â§ $\\gamma$ \u0026lt; 1) is the discount factor that determines the importance of future rewards.\nFor a more thorough and in depth explanation of reinforcement learning please refer to Lilian Weng excelent blog post A (Long) Peek into Reinforcement Learning.\nWhat generalization means for RL? In reinforcement learning, generalization involves an agent\u0026rsquo;s ability to apply learned policies or value functions to new states or environments that it has not encountered during training. This is essential because it is often impractical or impossible to train an agent on every possible state it might encounter.\nThere are bunch of methods that researchers have used to tackle the problem of generalization in reinforcement learning that are summarized in the following diagram.\nThis paper focuses on RL-Specific solutions and specifically exploration technique to tackle the problem of generalization. If you\u0026rsquo;re interested to learn more about the generalization problem in reinforcement learning please refer to reference [3].\nWhat is zero-shot generalization? Zero-shot generalization in RL refers to the ability of an agent to perform well in entirely new environments or tasks without any prior specific training or fine-tuning on those environments or tasks. This is a significant challenge because it requires the agent to leverage its learned representations and policies in a highly flexible and adaptive manner.\nIn order to define the objective of zero-shot generalization we first have to define what MDPs and POMDPs are.\nMarkov Decision Process (MDP):\nMDP is a mathematical framework used to describe an environment in reinforcement learning where the outcome is partly random and partly under the control of a decision-maker (agent). A MDP is a tuple $M = (S, A, P_{init}, P, r, \\gamma)$ where,\nStates ($S \\in \\mathbb{R}^{|S|}$): A finite set of states that describe all possible situations in which the agent can be. Actions ($A \\in \\mathbb{R}^{|A|}$): A finite set of actions available to the agent. Initial State Distribution ($P_{init}$): A distribution of starting state $(s_0 \\sim P_{init})$. Transition Probability ($P$): A function $P(s_{t+1}, s_t, a_t)$ representing the probability of transitioning from state $s_t$ to state $s_{t+1}$ after taking action $a_t$ $(s_{t+1} \\sim P(.|s_t,a_t))$. Reward ($r: S \\times A \\rightarrow \\mathbb{R}$): A function $r(s_t, a_t)$ representing the immediate reward $r_t$ received after transitioning from state $s_t$ due to action $a_t$ $(r_t = r(s_t, a_t))$. Discount Factor ($\\gamma$): $(0 \\leq \\gamma \u0026lt; 1)$ is a constant that determines the importance of future rewards. Partially Observable Markov Decision Process (POMDP):\nPOMDP extends MDPs to situations where the agent does not have complete information about the current state. Instead, the agent must make decisions based on partial observations. A POMDP is a tuple $M = (S, A, O, P_{init}, P, \\Sigma, r, \\gamma)$ where other that above definitions for MDP,\nObservation Space ($O$): A finite set of observations the agent can receive about the state. Observation Function ($\\Sigma$): A function that given current state $s_t$ and current action $a_t$ gives us the current observation $o_t$ $(o_t = \\Sigma(s_t, a_t) \\in O)$. If we set $O = S$ and $\\Sigma(s,a) = s$, the POMDP turns into regular MDP.\nLet the history at time $t$ be,\n$$ h_t = \\{ o_0, a_0, r_0, o_1, a_1, r_1, \\dots, o_t \\} $$\nThe agent‚Äôs next action is outlined by a policy $\\pi$, which is a stochastic mapping from the history to an action probability,\n$$ \\pi(a|h_t) = P(a_t=a|h_t) $$\nIn this formulation, a history-dependent policy (and not a Markov policy) is required both due to partially observed states, epistemic uncertainty, and also for optimal maxEnt exploration.\nWe assume a prior distribution over POMDPs $P(M)$, defined over some space of POMDPs. For a given POMDP, an optimal policy maximizes the expected discounted return,\n$$ \\mathbb{E}_{\\pi,M} \\left[ \\sum _{t=0}^{\\infty} \\gamma^t r(s_t,a_t) \\right] $$\nWhere the expectation is taken over the policy $\\pi(h_t)$, and the state transition probability $s_t \\sim P$ of POMDP $M$.\nOur generalization objective is to maximize the discounted cumulative reward taken in expectation over the POMDP prior,\n$$ \\mathcal{R}_ {pop}(\\pi) = \\mathbb{E}_ {M \\sim P(M)} {\\left[ \\mathbb{E}_{\\pi,M} {\\left[ \\sum _{t=0}^{\\infty} \\gamma^t r(s_t,a_t) \\right]} \\right]} $$\nSeeking a policy that performs well in expectation over any POMDP from the prior corresponds to zero-shot generalization.\nAnd as you may have guessed we don\u0026rsquo;t have access to true prior distribution of POMDPs so we have to estimate it with $N$ training POMDPs $M_1, M_2, \\dots, M_N$ sampled from the true prior distribution $P(M)$. So we are going to maximize empirical discounted cumulative reward,\n$$ \\begin{align*} \\mathcal{R}_ {emp}(\\pi) \u0026amp;= \\frac{1}{N} \\sum _ {i=1}^{N} \\mathbb{E} _ {\\pi,M_i} \\left[ \\sum _ {t=0}^{\\infty} \\gamma^t r(s_t,a_t) \\right] \\\\ \u0026amp;= \\mathbb{E}_ {M \\sim \\hat{P}(M)} {\\left[ \\mathbb{E}_{\\pi,M} {\\left[ \\sum _{t=0}^{\\infty} \\gamma^t r(s_t,a_t) \\right]} \\right]} \\end{align*} $$\nWhere the empirical POMDP distribution $\\hat{P}(M)$ can be different from the true distribution, i.e. $\\hat{P}(M) \\neq P(M)$. In general, a policy that optimizes the empirical reward may perform poorly on the population reward and this is known as overfitting in statistical learning theory.\nWhat are max reward and max entropy agents?! As we have seen, the goal of agents in reinforcement learning is to find a policy $\\pi$ that maximizes the expected discounted return by focusing on actions that lead to the greatest immediate or future rewards. We call these common RL agents \u0026ldquo;max reward agents\u0026rdquo; in this paper. On the other hand, \u0026ldquo;max entropy agents\u0026rdquo; aim to maximize the entropy of the policy for visiting different states. Maximizing entropy encourages the agent to explore a wider range of actions that lead the agent to visit new states even when they don\u0026rsquo;t contribute any reward.\nThis type of agent will help us to make decisions when we have epistemic uncertainty about what to do at test time. Epistemic uncertainty basically means the uncertainty that we have because of our lack of knowledge and can be improved by gathering more information about the situation.\nThe insight of the authors of this paper is that learning a policy that effectively explores the domain is harder to memorize than a policy that maximizes reward for a specific task, and therefore they expect such learned behavior to generalize well.\nWhat is an ensamble of them?!! Ensembling in reinforcement learning involves combining the policies of multiple individual agents to make more accurate and robust decisions. The idea is that by aggregating the outputs of different agents, we can leverage their diverse perspectives and expertise to improve overall performance.\nThis paper uses an ensamble of max reward agents to help the agent decide on the overal epistemic uncetainty that we have at test time.\nWhat is an invariance based approach? Invariance based algorithms in reinforcement learning focus on developing policies that are robust to changes and variations in the environment. These algorithms aim to identify and leverage invariant features or patterns that remain consistent across different environments or tasks. The goal is to ensure that the learned policy performs well not just in the training environment but also in new, unseen environments.\nThis paper uses IDAAC algorithm which is a special kind of DAAC algorithm as its invariance based algorithm.\nDAAC (Decoupled Advantage Actor-Critic) uses two separate networks, one for learning the policy and advantage, and one for learning the value. The value estimates are used to compute the advantage targets.\nIDAAC (Invariant Decoupled Advantage Actor-Critic) adds an additional regularizer to the DAAC policy encoder to ensure that it does not contain episode-specific information. The encoder is trained adversarially with a discriminator so that it cannot classify which observation from a given pair $(s_i, s_j)$ was first in a trajectory.\nFor more information about IDAAC algorithm please refer to reference [4].\nWhat the heck is ProcGen?!!! Procgen is a benchmark suite for evaluating the generalization capabilities of reinforcement learning agents. It was developed by OpenAI and consists of a collection of procedurally generated environments that vary in terms of visual appearance, dynamics, and difficulty. The goal of Procgen is to provide a standardized and challenging set of environments that can be used to assess the ability of RL algorithms to generalize to unseen scenarios.\nIf you are new to reinforcement learning, I know these explanations are a lot to take in! If you find yourself lost, I recommend to check out the following courses at your leisure:\nDeep Reinforcement Learning (by Hugging Face ü§ó) Reinforcement Learning (by Mutual Information) Introduction to Reinforcement Learning (by David Silver) Reinforcement Learning (by Michael Littman \u0026amp; Charles Isbell) Reinforcement Learning (by Emma Brunskill) Deep Reinforcement Learning Bootcamp 2017 Foundations of Deep RL (by Pieter Abbeel) Deep Reinforcement Learning \u0026amp; Control (by Katerina Fragkiadaki) Deep Reinforcement Learning (by Sergey Levine) After reviewing all this we can focus on the rest of the paper!\nHidden Maze Experiment One of the key observation of the authors of this paper is that invariance is not enough for zero-shot generalization of reinforcemen learning algorithm. They designed the hidden maze experiment too demonstrate that. Imagine Maze, but with the walls and goal hidden in the observation. Arguably, this is the most task-invariant observation possible, such that a solution can still be obtained in a reasonable time.\nAn agent with memory can be trained to optimally solve all training tasks: figuring out wall positions by trying to move ahead and observing the resulting motion, and identifying based on its movement history in which training maze it is currently in. Obviously, such a strategy will not generalize to test mazes. Performance in Maze, where the strategy for solving any particular training task must be indicative of that task, has largely not improved by methods based on invariance\nThe following figure shows PPO performance on the hidden maze task, indicating severe overfitting.\nAs described by [5], an agent can overcome test-time errors in its policy by treating the perfect policy as an unobserved variable. The resulting decision making problem, termed the epistemic POMDP, may require some exploration at test time to resolve uncertainty. The article further proposed the LEEP algorithm based on this principle, which trains an ensemble of agents and essentially chooses randomly between the members when the ensemble does not agree, and was the first method to present substantial generalization improvement on Maze.\nIn this paper authors extend this idea and asked, How to improve exploration at test time?, and their approach is based on a novel discovery, when they train an agent to explore the training domains using a maximum entropy objective, they observe that the learned exploration behavior generalizes surprisingly well (much better than the generalization attained when training the agent to maximize reward).\nIn the following section we gonna dig deep into internals of maximum entropy policy.\nMaxEnt Policy For simplicity the authors discuss this part for the MDP case. A policy $\\pi$, through its interaction with an MDP, induces a t-step state distribution over the state space $S$,\n$$ d _ {t,\\pi} (s) = p(s_t=s | \\pi) $$\nThe objective of maximum entropy exploration is given by:\n$$ \\mathcal{H}(d(.)) = -\\mathbb{E} _ {s \\sim d} \\left[ \\log{d(s)} \\right] $$\nWhere $d$ can be regarded as either,\nStationary state distribution (infinite horizon): $d _ {\\pi} = \\lim _ {t \\rightarrow \\infty} d _ {t,\\pi} (s)$ Discounted state distribution (infinite horizon): $d _ {\\gamma, \\pi} = (1-\\gamma) \\sum _ {t=0} ^ {\\infty} \\gamma^t d _ {t,\\pi} (s)$ Marginal state distribution (finite horizon): $d _ {T, \\pi} = \\frac{1}{T} \\sum _ {t=0} ^ {T} d _ {t,\\pi} (s)$ In this work they focus on the finite horizon setting and adapt the marginal state distribution $d _ {T, \\pi}$ in which $T$ equals the episode horizon $H$, so we seek to maximize the objective:\n$$ \\begin{align*} \\mathcal{R} _ {\\mathcal{H}} (\\pi) \u0026amp;= \\mathbb{E} _ {M \\sim \\hat{P}(M)} \\left[ \\mathcal{H}(d _ {H,\\pi}) \\right] \\\\ \u0026amp;=\\mathbb{E} _ {M \\sim \\hat{P}(M)} \\left[ \\mathcal{H}(\\frac{1}{H} \\sum _ {t=0} ^ {H} d _ {t,\\pi} (s)) \\right] \\end{align*} $$\nwhich yields a policy that \u0026ldquo;equally\u0026rdquo; visits all states during the episode.\nTo maximize this objective we can estimating the density of the agent\u0026rsquo;s state visitation distribution, but in this paper the authors adapt the non-parametric entropy estimation approach; we estimate the entropy using the particle based k-nearest neighbor (k-NN estimator).\nTo estimate the distribution $d _ {H,\\pi}$ over the states $S$, we consider each trajectory as $H$ samples of states $\\{ s_t \\} _ {t=1} ^ {H}$ and take $s _ t ^ {\\text{k-NN}}$ to be the k-NN of the state $s_t$ within the trajectory,\n$$ \\hat{ \\mathcal{H} } ^ {k,H} (d _ {H,\\pi}) \\approx \\frac{1}{H} \\sum _ {t=1} ^ {H} \\log ( \\parallel s_t - s_t^{\\text{k-NN}} \\parallel _ 2) $$\nIn which we define intrinsic reward function as,\n$$ r_I (s_t) \\coloneqq \\log ( \\parallel s_t - s_t^{\\text{k-NN}} \\parallel _ 2) $$\nThis formulation enables us to deploy any RL algorithm to approximately optimize objective.\nSpecifically, in this work we use the policy gradient algorithm PPOŸà where at every time step $t$, the state $s_t^{\\text{k-NN}}$ is chosen from previous states $\\{ s_t \\} _ {t=1} ^ {t-1}$ of the same episode. To improve computational efficiency, instead of taking the full observation as the state (64 x 64 RGB image), we sub-sample the observation by applying average pooling of 3 x 3 to produce an image of size 21 x 21.\nWe found that agents trained for maximum entropy exploration exhibit a smaller generalization gap compared with the standard approach of training solely with extrinsic reward. The policies are equipped with a memory unit (GRU) to allow learning of deterministic policies that maximize the entropy.\nIn all three environments, we demonstrate a small generalization gap, as test performance on unseen levels closely follows the performance achieved during training.\nIn addition, we verify that the train results are near optimal by comparing with a hand designed approximately optimal exploration policy. For example, on Maze we use the well known maze exploring strategy wall follower, also known as the left/right-hand rule.\nExpGen Algorithm Our main insight is that, given the generalization property of the entropy maximization policy established above, an agent can apply this behavior in a test MDP and expect effective exploration at test time. We pair this insight with the epistemic POMDP idea, and propose to play the exploration policy when the agent faces epistemic uncertainty, hopefully driving the agent to a different state where the reward-seeking policy is more certain.\nOur framework comprises two parts: an entropy maximizing network and an ensemble of networks that maximize an extrinsic reward to evaluate epistemic uncertainty. The first step entails training a network equipped with a memory unit to obtain a maxEnt policy $\\pi_H$ that maximizes entropy. Next, we train an ensemble of memory-less policy networks $\\{ \\pi _ r ^ j \\} _ {j=1} ^ {m} $ to maximize extrinsic reward.\nHere is the ExpGen algorithm,\nWe consider domains with a finite action space, and say that the policy $\\pi _ r ^ i$ is certain at state $s$ if its action $a_i \\sim \\pi _ r ^ i (a|s)$ is in consensus with the ensemble: $a_i = a_j$ for the majority of $k$ out of $m$, where $k$ is a hyperparameter of our algorithm.\nSwitching between two policies may result in a case where the agent repeatedly toggles between two states (if, say, the maxEnt policy takes the agent from state $s_1$ to a state $s_2$, where the ensemble agrees on an action that again moves to state $s_1$.). To avoid such ‚Äúmeta-stable‚Äù behavior, we randomly choose the number of maxEnt steps $n_{\\pi_{\\mathcal{H}}}$ from a Geometric distribution, $n_{\\pi_{\\mathcal{H}}} \\sim Geom(\\alpha)$.\nExperiments Our experimental setup follows ProcGen\u0026rsquo;s easy configuration, wherein agents are trained on 200 levels for 25M steps and subsequently tested on random levels. All agents are implemented using the IMPALA (Importance Weighted Actor-Learner Architectures) convolutional architecture, and trained using PPO or IDAAC. For the maximum entropy agent $\\pi_H$ we incorporate a single GRU at the final embedding of the IMPALA convolutional architecture. For all games, we use the same parameter $\\alpha=0.5$ of the Geometric distribution and form an ensemble of 10 networks.\nFollowing figure is comparison across all ProcGen games, with 95% bootstrap CIs highlighted in color. Score distributions of ExpGen, PPO, PLR, UCB-DrAC, PPG and IDAAC.\nFollowing figure shows in each row, the probability of algorithm X outperforming algorithm Y. The comparison illustrates the superiority of ExpGen over the leading contender IDAAC with probability 0.6, as well as over other methods with even higher probability.\nSee Also PyTorch implementation of ExpGen @ GitHub Ev Zisselman Presentation @ NeurIPS 2023 ExpGen Rebuttal Process @ OpenReview ExpGen Poster for NeurIPS 2023 References [1] Zisselman, Ev, et al. \u0026ldquo;Explore to generalize in zero-shot rl.\u0026rdquo; Advances in Neural Information Processing Systems 36 (2024).\n[2] Sutton, Richard S., and Andrew G. Barto. \u0026ldquo;Reinforcement learning: An introduction.\u0026rdquo; MIT press, (2020).\n[3] Kirk, Robert, et al. \u0026ldquo;A survey of zero-shot generalisation in deep reinforcement learning.\u0026rdquo; Journal of Artificial Intelligence Research 76 (2023): 201-264.\n[4] Raileanu, Roberta, and Rob Fergus. \u0026ldquo;Decoupling value and policy for generalization in reinforcement learning.\u0026rdquo; International Conference on Machine Learning. PMLR, 2021.\n[5] Ghosh, Dibya, et al. \u0026ldquo;Why generalization in rl is difficult: Epistemic pomdps and implicit partial observability.\u0026rdquo; Advances in neural information processing systems 34 (2021): 25502-25515.\n[6] Espeholt, Lasse, et al. \u0026ldquo;Impala: Scalable distributed deep-rl with importance weighted actor-learner architectures.\u0026rdquo; International conference on machine learning. PMLR, 2018.\n","permalink":"http://localhost:1313/posts/explore-to-generalize-in-zero-shot-rl/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eThis paper studies the problem of zero-shot generalization in reinforcement learning and introduces an algorithm that trains an ensamble of maximum reward seeking agents and an maximum entropy agent. At test time, either the ensemble agrees on an action, and we generalize well, or we take exploratory actions with the help of maximum entropy agent and drive us to a novel part of the state space, where the ensemble may potentially agree again. This method combined with an invariance based approach achieves new state-of-the-art results on ProcGen.\u003c/p\u003e","title":"ExpGen: Explore to Generalize in Zero-Shot RL"},{"content":"Welcome to our blog! We are a team of enthusiastic professors and students from Sharif University of Technology who are eager to share our insights and passion for this fascinating field. Our aim is to share our knowledge and excitement about this cutting-edge field with you ‚ù§Ô∏è.\nProfessors Professor Mohammad Hossein Rohban Professor Ehsaneddin Asgari Students Arash Alikhani Alireza Nobakht Labs RIML Lab NLP \u0026amp; DH Lab We hope you enjoy our blog and find our content both informative and inspiring üöÄ!\n","permalink":"http://localhost:1313/us/","summary":"\u003cp\u003eWelcome to our blog! We are a team of enthusiastic professors and students from Sharif University of Technology who are eager to share our insights and passion for this fascinating field. Our aim is to share our knowledge and excitement about this cutting-edge field with you ‚ù§Ô∏è.\u003c/p\u003e\n\u003ch2 id=\"professors\"\u003eProfessors\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eProfessor \u003ca href=\"https://www.linkedin.com/in/mohammad-hossein-rohban-75567677\"\u003eMohammad Hossein Rohban\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003eProfessor \u003ca href=\"https://www.linkedin.com/in/ehsaneddinasgari\"\u003eEhsaneddin Asgari\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"students\"\u003eStudents\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://www.linkedin.com/in/infinity2357\"\u003eArash Alikhani\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://www.linkedin.com/in/alireza-nobakht\"\u003eAlireza Nobakht\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"labs\"\u003eLabs\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/rohban-lab\"\u003eRIML Lab\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/language-ml\"\u003eNLP \u0026amp; DH Lab\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eWe hope you enjoy our blog and find our content both informative and inspiring üöÄ!\u003c/p\u003e","title":""},{"content":"Adversarial Attacks on Reinforcement Learning Policies Reinforcement Learning (RL) has powered breakthroughs in games, robotics, and autonomous systems. But just like image classifiers can be fooled by carefully crafted adversarial examples, RL agents are also vulnerable to adversarial attacks. These attacks can manipulate the agent‚Äôs perception of the environment or its decision process, leading to unsafe or suboptimal behavior.\nWhat Do Adversarial Attacks Mean in RL? In RL, an agent interacts with an environment, observes a state, takes an action, and receives a reward. An adversarial attack interferes with this process‚Äîoften by perturbing the input observations‚Äîso the agent chooses actions that look reasonable to it but are actually harmful.\nFor example:\nA self-driving car policy might mistake a stop sign for a speed-limit sign if subtle noise is added to the camera input. A robot trained to walk could be tripped by slight modifications to its sensor readings. Types of Attacks in Reinforcement Learning Researchers have identified several ways adversarial attacks can target RL policies. These attack vectors include:\nObservation Perturbation\nThe most widely studied form of attack: small, carefully crafted changes are made to the agent‚Äôs observations so it perceives the environment incorrectly.\nExample: Adding imperceptible noise to frames in an Atari game so the agent misjudges its next move.\nCommunication Perturbation\nIn multi-agent systems, communication messages can be perturbed, leading to miscoordination.\nExample: Two drones sharing location data receive slightly altered coordinates, causing a collision.\nMalicious Communications\nBeyond perturbations, adversaries may inject entirely fake or deceptive messages.\nExample: An attacker sends a false signal about enemy positions in a cooperative strategy game.\nWhy It Matters Adversarial attacks highlight the gap between high-performing RL policies in controlled benchmarks and their reliability in the real world. Understanding and mitigating these vulnerabilities is essential if we want RL to be trusted in safety-critical domains like autonomous driving, robotics, and healthcare.\nHow Vulnerable Are RL Policies to Adversarial Attacks? Reinforcement Learning (RL) has made huge strides in recent years, powering systems that can beat human champions in games and control robots with precision. But just like image classifiers can be fooled by imperceptible changes to inputs, RL policies are also highly vulnerable to adversarial attacks\nThe researchers show that even tiny perturbations‚Äîso small they are invisible to humans‚Äîcan cause RL agents to fail dramatically at test time. Using Atari games as a testbed, they evaluate three common deep RL algorithms: DQN, TRPO, and A3C. The results are clear: all three are susceptible, but DQN policies are especially vulnerable.\nWhite-Box Attacks with FGSM In white-box scenarios, where the adversary has full access to the policy and gradients, attacks are devastating. The team applies the Fast Gradient Sign Method (FGSM) to craft adversarial examples and finds:\nAn ‚Ñì‚àû-norm perturbation with Œµ = 0.001 can slash performance by over 50%. ‚Ñì1-norm attacks are even more powerful‚Äîby changing just a handful of pixels significantly, they can cripple the agent‚Äôs performance. Even policies trained with robust algorithms like TRPO and A3C experience sharp drops when faced with these attacks.\nBlack-Box Attacks and Transferability What if the adversary doesn‚Äôt have full access to the policy network? Surprisingly, attacks are still effective. In black-box settings, adversaries exploit the property of transferability:\nAdversarial examples created for one policy often transfer to another policy trained on the same task. Transferability also extends across algorithms‚Äîfor example, attacks generated against a DQN policy can still reduce the performance of an A3C policy. Effectiveness decreases with less knowledge, but performance still degrades significantly, especially with ‚Ñì1 attacks. Why This Matters The key takeaway is that adversarial examples are not just a computer vision problem. RL policies‚Äîdespite achieving high scores in training‚Äîcan be undermined by imperceptible perturbations. This fragility is dangerous for real-world applications like autonomous driving and robotics, where safety and reliability are non-negotiable.\nUniversal Adversarial Preservation (UAP) To ground these ideas, I re-implemented the 2017 Adversarial Attacks on Neural Network Policies setup (since there was no offical impelmentation) and trained a DQN agent on Atari Pong. After validating the baseline attacks, I implemented Universal Adversarial Perturbations (UAPs) and found that a single, fixed perturbation‚Äîcomputed once and then applied it to all observations, which was enough to consistently derail the policy across episodes and random seeds, without recomputing noise at every timestep. In other words, the attack generalized over time and trajectories, confirming that UAPs exploit stable perceptual quirks of the learned policy rather than moment-by-moment gradients. Practically, this feels much closer to a real-world threat model: an attacker only needs to tamper with the sensor once (think a sticker/overlay on the lens) instead of having high-bandwidth, per-step access to the system. Below you can see the plot of rewards vs Œµ bugdet and videos of different setups.\nbaselines\u0026rsquo; reward over different values of Œµ budget.\nClean ‚Äî original episode (no perturbation). Random uniform noise (Œµ = 2.0). FGSM (Œµ = 2.0) ‚Äî white-box attack. PGD (Œµ = 2.0) ‚Äî iterative, white-box attack. UAP (Œµ = 2.0) ‚Äî image-agnostic. Adversarial Policies: when weird opponents break strong RL TL;DR. Instead of adding pixel noise to an RL agent‚Äôs input, this paper shows you can train a policy that acts in the shared world to induce natural but adversarial observations for the victim‚Äîcausing robust, self-play‚Äìtrained agents to fail in zero-sum MuJoCo games. Fine-tuning helps‚Ä¶until a new adversary is learned.\nThe threat model: natural observations as the ‚Äúperturbation‚Äù In multi-agent settings, an attacker typically can‚Äôt flip pixels or edit state vectors. But it can choose actions that make the victim see carefully crafted, physically plausible observations. Hold the victim fixed and the two-player game becomes a single-agent MDP for the attacker, who learns a policy that elicits bad actions from the victim.\nQuick look:\nYou Shall Not Pass Kick \u0026amp; Defend Sumo (Human) Masked victim vs adversary Setup in a nutshell Victims: strong self-play policies (‚Äúagent zoo‚Äù) across four MuJoCo tasks: Kick \u0026amp; Defend, You Shall Not Pass, Sumo Humans, Sumo Ants. Attacker: trained with PPO for ~20M timesteps‚Äî\u0026lt; 3% of the 680‚Äì1360M timesteps used for the victims‚Äîyet reliably wins. Key idea: adversaries don‚Äôt become great players; they learn poses/motions that generate adversarial observations for the victim. Figures\nTasks used for evaluation.\nAdversary win rate rises quickly despite far fewer timesteps.\nWhat the learned adversary looks like (and why that matters) In Kick \u0026amp; Defend and YSNP, the adversary may never stand up‚Äîit finds contorted, stable poses that make the victim mis-act. In Sumo Humans, where falling loses immediately, it adopts a kneeling/stable stance that still provokes the victim to fall.\nQualitative behaviors: the ‚Äúpoint‚Äù is to confuse, not to excel at the nominal task.\nMasking test: evidence the attack is observational If wins come from manipulating what the victim sees, then hiding the adversary‚Äôs pose from the victim should help. That‚Äôs exactly what happens:\nAgainst normal opponents, the masked victim is (unsurprisingly) worse. Against the adversary, the masked victim becomes nearly immune (e.g., in YSNP: normal victim loses often; masked victim flips the outcome and wins almost always). Masking the adversary‚Äôs position removes the observation channel the attack exploits.\nDimensionality matters Victims are more vulnerable when more opponent DOFs are observed. The attack is stronger in Humanoid (higher-dimensional observed joints) than Ant (lower-dimensional). More controllable joints ‚Üí more ways to steer the victim off-distribution.\nHigher observed dimensionality correlates with higher adversary win rates.\nWhy it works: off-distribution activations Analyses of the victim‚Äôs network show adversarial opponents push internal activations farther from the training manifold than random or lifeless baselines.\nAdversarial policies drive ‚Äúweird‚Äù activations‚Äîmore off-distribution than simple OOD baselines.\nDefenses (and their limits) Fine-tuning the victim on the discovered adversary reduces that adversary‚Äôs success (often down to ~10% in YSNP), but:\nCatastrophic forgetting: performance vs normal opponents degrades (single-adversary fine-tune is worst; dual fine-tune helps but still regresses). Arms race: re-running the attack against the fine-tuned victim yields a new adversary that succeeds again‚Äîoften via a different failure mode (e.g., tripping rather than pure confusion). Before fine-tune After fine-tune (this adversary) Win-rate grid before/after fine-tuning against normal opponents and adversaries.\nTakeaways for practitioners Threat model upgrade: in multi-agent worlds, your attack surface includes other policies that craft natural observations‚Äîno pixel hacks needed. Exploitability check: training a targeted adversary lower-bounds your policy‚Äôs worst-case performance and reveals failure modes missed by self-play. Defense needs diversity: fine-tuning on a single adversary overfits. Prefer population-based or curriculum defenses that rotate diverse opponents and maintain competence vs normals. Robust Communicative Multi-Agent Reinforcement Learning with Active Defense By Yu et al., AAAI 2024\nüåê Why Communication Matters in Multi-Agent RL In multi-agent reinforcement learning (MARL), agents often face partial observability ‚Äî no single agent sees the full environment. To cooperate effectively, agents need to communicate, sharing information about what they see and what actions to take.\nThis communication has powered applications such as robot navigation and traffic light control.\nBut there‚Äôs a catch: in the real world, communication channels are noisy and vulnerable to adversarial attacks. If attackers tamper with even a few messages, the performance of MARL systems can collapse.\nüõ°Ô∏è Enter Active Defense: The Core Idea Yu et al. propose a new active defense strategy. Instead of blindly trusting all messages, agents:\nJudge the reliability of each incoming message using their own observations and history (hidden states). Adjust the influence of unreliable messages by reducing their weight in the decision process. üëâ Example: If one agent already searched location (1,1) and found nothing, but receives a message saying ‚ÄúTarget at (1,1)‚Äù, it can spot the inconsistency and downweight that message.\nüß© The ADMAC Framework The authors introduce Active Defense Multi-Agent Communication (ADMAC), which has two key components:\nReliability Estimator: A classifier that predicts whether a message is reliable (weight close to 1) or unreliable (weight close to 0). Decomposable Message Aggregation Policy Net: A structure that breaks down the influence of each message into an action preference vector, making it possible to scale its impact up or down. This allows agents to combine their own knowledge with weighted messages to make more robust decisions.\nThe figure above shows how an agent in the ADMAC framework generates its action distribution by combining its own observations with incoming messages from other agents:\nHidden state update: The agent maintains a hidden state (h·µ¢·µó‚Åª¬π), which is updated using the observation (o·µ¢·µó) through the GRU module f_HP. This captures past and current information. Base action preference: From the updated hidden state, the agent generates a base preference vector via f_BP, representing what it would do independently. Message influence: Each received message (m‚ÇÅ·µó, ‚Ä¶, m_N·µó) is processed with the observation through f_MP, producing a message-based action preference vector. Reliability estimation: A reliability estimator f_R evaluates each message, assigning it a weight w·µ¢(m‚±º·µó) that reflects how trustworthy it seems. Aggregation: The agent sums its base vector with all weighted message vectors to form a total action preference vector (v·µ¢·µó). Final decision: Applying a Softmax function converts this vector into a probability distribution over actions, from which the agent selects its next move. By downweighting unreliable messages, ADMAC enables agents to remain robust against malicious communication while still leveraging useful information from peers.\nReferences:\nHuang, S. H., Papernot, N., Goodfellow, I. J., Duan, Y., \u0026amp; Abbeel, P. (2017). Adversarial Attacks on Neural Network Policies. Gleave, A., Dennis, M., Kant, N., Wild, C., Levine, S., \u0026amp; Russell, S. (2019). Adversarial Policies: Attacking Deep Reinforcement Learning. Yu, L., Qiu, Y., Yao, Q., Shen, Y., Zhang, X., \u0026amp; Wang, J. (2023). Robust Communicative Multi-Agent Reinforcement Learning with Active Defense. Guo, W., Wu, X., Huang, S., \u0026amp; Xing, X. (2021). Adversarial Policy Learning in Two-player Competitive Games. https://www.youtube.com/watch?v=-_j-fmVpn_s https://rll.berkeley.edu/adversarial/ ","permalink":"http://localhost:1313/posts/adversarial-rl/","summary":"\u003ch1 id=\"adversarial-attacks-on-reinforcement-learning-policies\"\u003eAdversarial Attacks on Reinforcement Learning Policies\u003c/h1\u003e\n\u003cp\u003eReinforcement Learning (RL) has powered breakthroughs in games, robotics, and autonomous systems. But just like image classifiers can be fooled by carefully crafted adversarial examples, RL agents are also vulnerable to \u003cstrong\u003eadversarial attacks\u003c/strong\u003e. These attacks can manipulate the agent‚Äôs perception of the environment or its decision process, leading to unsafe or suboptimal behavior.\u003c/p\u003e\n\u003ch2 id=\"what-do-adversarial-attacks-mean-in-rl\"\u003eWhat Do Adversarial Attacks Mean in RL?\u003c/h2\u003e\n\u003cp\u003eIn RL, an agent interacts with an environment, observes a state, takes an action, and receives a reward. An adversarial attack interferes with this process‚Äîoften by perturbing the input observations‚Äîso the agent chooses actions that look reasonable to it but are actually harmful.\u003c/p\u003e","title":"Adversarial RL"},{"content":"Actor-Critic vs. Value-Based: Empirical Trade-offs Introduction In Reinforcement Learning (RL), one of the fundamental questions is:\nShould we focus on learning the value of states, or should we directly learn the optimal policy?\nBefore diving into this comparison, let‚Äôs briefly recall what RL is: a branch of machine learning in which an agent interacts with an environment to learn how to make decisions that maximize cumulative reward.\nTraditionally, RL algorithms fall into two main families:\nValue-Based methods : which aim to estimate the value of states or state‚Äìaction pairs. Policy-Based methods : which directly optimize a policy that maps states to actions. Actor-Critic algorithms combine the strengths of both worlds, simultaneously learning a value function and an explicit policy. This hybrid structure can often lead to more stable and efficient learning.\nValue-Based and Actor-Critic approaches represent fundamentally different perspectives: one focuses solely on learning state values, while the other integrates both value and policy learning. Comparing these two perspectives helps us better understand the impact of incorporating value or policy components in different environments.\nIn this project, we empirically evaluate these two families in both discrete and continuous action spaces. Four representative algorithms ( DQN, A2C, NAF, and SAC ) were implemented, along with a user-friendly graphical user interface (GUI) for training and evaluation.\nOur ultimate goal is to analyze trade-offs such as convergence speed, learning stability, and final performance across diverse scenarios.\nBackground The reinforcement learning algorithms used in this project fall into two main families:\nValue-Based\nIn this approach, the agent learns only a value function, such as $Q(s, a)$.\nThe policy is derived implicitly by selecting the action with the highest value:\n$\\pi(s) = \\arg\\max_a Q(s,a)$\nThis method is typically simpler, more stable, and computationally efficient.\nHowever, it faces limitations when dealing with continuous action spaces.\nExample algorithms: DQN, NAF.\nValue-Based methods are often well-suited for discrete action spaces with relatively small state‚Äìaction domains, where enumerating or approximating the value for each action is feasible.\nActor-Critic\nIn this framework, the agent consists of two components:\nActor : a parameterized policy that directly produces actions. Critic : a value function that evaluates the Actor‚Äôs performance and guides its updates. This combination can provide greater learning stability, improved performance in complex environments, and high flexibility in continuous action spaces.\nExample algorithms: A2C, SAC.\nActor-Critic methods are generally more suitable for continuous or high-dimensional action spaces, as the Actor can output actions directly without exhaustive value estimation.\nMethodology Project Design This project was designed to perform an empirical comparison between two major families of reinforcement learning algorithms: Value-Based and Actor-Critic.\nFour representative algorithms were selected and implemented in diverse discrete and continuous environments.\nTraining, evaluation, and comparison were carried out through a fully interactive, user-friendly graphical interface.\nImplemented Algorithms Representative algorithms from the two families were selected based on their reported performance in different environments according to the literature.\nFor each algorithm, the training procedure was reproduced in accordance with its original paper.\nThe overall structure of each algorithm is summarized below:\nAlgorithm Family Action Space Description Reference Deep Q-Network (DQN) Value-Based Discrete Uses experience replay and a fixed target network to stabilize learning. 1 Normalized Advantage Function (NAF) Value-Based Continuous Value-based method for continuous spaces using a specific Q-structure to simplify action selection. 2 Advantage Actor-Critic (A2C) Actor-Critic Discrete/Continuous Direct policy optimization guided by an advantage function. 3 Soft Actor-Critic (SAC) Actor-Critic Continuous Off-policy actor-critic method maximizing entropy for stability in complex environments. 4 Deep Q-Network (DQN) The pseudocode of DQN highlights the use of experience replay and a target network, which together reduce correlations between samples and stabilize training.\nNormalized Advantage Function (NAF) NAF handles continuous action spaces by constraining the Q-function into a quadratic form, which makes action selection computationally efficient.\nAdvantage Actor-Critic (A2C) A2C directly optimizes a parameterized policy (Actor) with guidance from the Critic, using advantage estimation to reduce gradient variance and improve learning stability.\nSoft Actor-Critic (SAC) SAC introduces entropy maximization in the objective, encouraging exploration and robustness in complex continuous environments.\nEnvironments There were many 2D and 3D environments available so that we could compare them.\nThe 12 famous environments are listed below:\nFour environments from the Gym library were selected to provide a diverse set of challenges that cover both discrete and continuous action spaces, as well as varying levels of complexity and dynamics:\nMountainCar-v0 (Discrete): A classic control problem where the agent must drive a car up a hill using discrete acceleration commands. This environment tests basic exploration and planning in a low-dimensional, discrete action space. Pendulum-v1 (Continuous): Requires applying continuous torque to keep a pendulum upright. This environment is ideal for evaluating continuous control algorithms and stabilizing dynamics. FrozenLake-v1 (Discrete): A gridworld task where the agent navigates an icy lake to reach a goal while avoiding holes. This environment emphasizes decision-making under uncertainty in a discrete setting. HalfCheetah-v4 (Continuous): A high-dimensional continuous control environment where the agent controls a bipedal cheetah to run efficiently. It challenges advanced continuous control and balance strategies. These environments were chosen to allow a comprehensive comparison of algorithms across different action types, state complexities, and control challenges.\nEnvironment Type Description MountainCar-v0 Discrete Drive a car up a hill by controlling acceleration in a discrete space. Pendulum-v1 Continuous Apply torque to keep a pendulum upright and stable. FrozenLake-v1 Discrete Navigate an icy grid to reach the goal without falling into holes. HalfCheetah-v4 Continuous Control the speed and balance of a simulated bipedal cheetah for fast running. Environment snapshots were recorded during training and appear as GIFs in the Results section.\nConfiguration and Evaluation Algorithms were run with optimized settings for each environment. Key hyperparameters (learning rate, Œ≥, batch size, buffer size) were tuned through trial-and-error, leveraging existing GitHub implementations for optimal performance. Comparisons were based on convergence speed, training stability, and final performance. Example configurations:\nAlgorithm Environment Œ≥ Learning Rate Batch Size Buffer Size DQN FrozenLake 0.93 6e-4 32 4,000 A2C MountainCar 0.96 1e-3 ‚Äì ‚Äì NAF Pendulum 0.99 3e-4 64 400,000 SAC HalfCheetah 0.99 3e-4 256 1,000,000 Graphical User Interface (GUI) A user-friendly GUI was developed to simplify training and comparing algorithms, enabling full project execution without direct coding.The code is available at: This Github Link The project structure is as follows:\nProject_Code/\r‚îú‚îÄ‚îÄ plots/\r‚îÇ ‚îî‚îÄ‚îÄ learning_curves/\r‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ Continuous/\r‚îÇ ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ Pendulum-v1/\r‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ SAC/\r‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ NAF/\r‚îÇ ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ HalfCheetah-v4/\r‚îÇ ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ SAC/\r‚îÇ ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ NAF/\r‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ Discrete/\r‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ FrozenLake-v1/\r‚îÇ ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ DQN/\r‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ 4x4\r‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ 8x8\r‚îÇ ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ A2C/\r‚îÇ ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ 4x4\r‚îÇ ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ 8x8\r‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ MountainCar-v0/\r‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ DQN/\r‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ A2C/\r‚îÇ ‚îú‚îÄ‚îÄ comparison_table.png\r‚îÇ ‚îî‚îÄ‚îÄ directory_structure.png\r‚îú‚îÄ‚îÄ requirements/\r‚îÇ ‚îú‚îÄ‚îÄ base.txt\r‚îÇ ‚îú‚îÄ‚îÄ dev.txt\r‚îÇ ‚îú‚îÄ‚îÄ env.txt\r‚îÇ ‚îî‚îÄ‚îÄ config.py\r‚îú‚îÄ‚îÄ src/\r‚îÇ ‚îú‚îÄ‚îÄ agents/\r‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ a2c.py\r‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ dqn.py\r‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ naf.py\r‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ sac.py\r‚îÇ ‚îú‚îÄ‚îÄ envs/\r‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ continuous_envs.py\r‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ discrete_envs.py\r‚îÇ ‚îú‚îÄ‚îÄ main.py\r‚îÇ ‚îú‚îÄ‚îÄ train.py\r‚îÇ ‚îî‚îÄ‚îÄ utils.py\r‚îú‚îÄ‚îÄ videos/\r‚îÇ ‚îú‚îÄ‚îÄ Continuous/\r‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ Pendulum-v1/\r‚îÇ ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ SAC/\r‚îÇ ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ NAF/\r‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ HalfCheetah-v4/\r‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ SAC/\r‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ NAF/\r‚îÇ ‚îî‚îÄ‚îÄ Discrete/\r‚îÇ ‚îú‚îÄ‚îÄ FrozenLake-v1/\r‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ DQN/\r‚îÇ ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ 4x4\r‚îÇ ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ 8x8\r‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ A2C/\r‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ 4x4\r‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ 8x8\r‚îÇ ‚îî‚îÄ‚îÄ MountainCar-v0/\r‚îÇ ‚îú‚îÄ‚îÄ DQN/\r‚îÇ ‚îî‚îÄ‚îÄ A2C/\r‚îú‚îÄ‚îÄ models/\r‚îÇ ‚îú‚îÄ‚îÄ Continuous/\r‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ Pendulum-v1/\r‚îÇ ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ SAC/\r‚îÇ ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ NAF/\r‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ HalfCheetah-v4/\r‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ SAC/\r‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ NAF/\r‚îÇ ‚îî‚îÄ‚îÄ Discrete/\r‚îÇ ‚îú‚îÄ‚îÄ FrozenLake-v1/\r‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ DQN/\r‚îÇ ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ 4x4\r‚îÇ ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ 8x8\r‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ A2C/\r‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ 4x4\r‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ 8x8\r‚îÇ ‚îî‚îÄ‚îÄ MountainCar-v0/\r‚îÇ ‚îú‚îÄ‚îÄ DQN/\r‚îÇ ‚îî‚îÄ‚îÄ A2C/\r‚îî‚îÄ‚îÄ README.md Main features:\nSelect algorithm, environment, action space type (discrete/continuous), and execution mode (train/test) with just a few clicks. Launch training with a Run button, and view results via Show Plots and Show Videos. Compare algorithms interactively using a dedicated Compare Algorithms window. Display key settings such as hyperparameters and project structure. Real-time console output for monitoring execution status and system messages. Interactive Mobile Application In addition to the desktop GUI, a mobile application for Android and iOS has been developed to provide interactive access to the project. By simply scanning the poster, users can explore various features, including:\nViewing videos of agent executions in different environments. Opening the project‚Äôs website for additional resources and documentation. Displaying the poster digitally for interactive exploration. Comparing learning curves and results across different algorithms. The images below showcase some sections of the app interface:\nResults This section presents the empirical evaluation of four reinforcement learning algorithms (DQN, NAF, A2C, and SAC) from the Value-Based and Actor-Critic families, across both discrete and continuous action space environments. The performance is analyzed based on final reward, convergence speed, and training stability, supported by quantitative metrics, qualitative visualizations (GIFs), and learning curves with a moving average (MA) applied to reduce noise. The experiments were conducted using the Gymnasium library, with optimized hyperparameters (see Methodology for details) and a fixed random seed for reproducibility. Training was performed on a single NVIDIA RTX 4050 GPU, with average runtimes of 1‚Äì4 hours per algorithm-environment pair.\nDiscrete Environments The discrete action space environments tested were FrozenLake-v1 (8x8 grid) and MountainCar-v0, which challenge the algorithms with stochastic transitions and sparse rewards, respectively. The table below summarizes the performance, followed by detailed analyses and visualizations.\nEnvironment Algorithm Final Reward (Avg ¬± Std) Convergence Speed (Episodes to 90% of Max) Stability (Std of Reward) FrozenLake-v1 DQN 0.98 ¬± 0.14 ~1,475 0.50 A2C 1.00 ¬± 0.00 ~1,209 0.48 MountainCar-v0 DQN -22.21 ¬± 79.32 ~2,000 81.50 A2C -27.87 ¬± 62.35 ~2,000 40.83 FrozenLake-v1 In FrozenLake-v1, a stochastic gridworld, A2C outperformed DQN in final reward (1.00 vs. 0.98 success rate) and converged faster (~1209 vs. ~1475 episodes to reach 90% of max reward). A2C‚Äôs advantage estimation provided greater stability, as evidenced by its lower standard deviation (0.48 vs. 0.50). The GIF below illustrates agent behaviors, showing A2C‚Äôs smoother navigation to the goal compared to DQN‚Äôs occasional missteps.\nReward comparison in FrozenLake-v1: A2C converges quickly and maintains stable performance compared to DQN.\nMountainCar-v0 In MountainCar-v0, a deterministic environment with sparse rewards, DQN achieved a better final reward (-22.21 vs. -27.87 timesteps to goal) but both algorithms converged at similar speeds (~2000 episodes). However, A2C exhibited greater stability (std of 40.83 vs. 81.50), avoiding large oscillations in learning. The GIF below shows DQN‚Äôs quicker ascent to the hilltop, while A2C maintains more consistent swings.\nReward comparison in MountainCar-v0: DQN converges faster and reaches higher rewards, while A2C shows greater stability.\nContinuous Environments The continuous action space environments tested were Pendulum-v1 and HalfCheetah-v4, which require precise control and balance in low- and high-dimensional settings, respectively. The table below summarizes the performance.\nEnvironment Algorithm Final Reward (Avg ¬± Std) Convergence Speed (Episodes to 90% of Max) Stability (Std of Reward) Pendulum-v1 NAF -141.17 ¬± 85.58 ~246 199.46 SAC 287.66 ¬± 62.38 ~152 113.23 HalfCheetah-v4 NAF 3,693.35 ¬± 575.60 ~862 1,077.01 SAC 10,247.42 ¬± 584.31 ~1,127 2,493.55 Pendulum-v1 In Pendulum-v1, SAC significantly outperformed NAF in final reward (287.66 vs. -141.17, higher is better) and converged faster (~152 vs. ~246 episodes). SAC‚Äôs entropy maximization ensured smoother learning, with a standard deviation of 113.23 compared to NAF‚Äôs 199.46. The GIF below highlights SAC‚Äôs ability to stabilize the pendulum upright, while NAF struggles with inconsistent torque.\nReward comparison in Pendulum-v1: SAC achieves higher rewards with smoother convergence compared to NAF.\nHalfCheetah-v4 In HalfCheetah-v4, a high-dimensional control task, SAC achieved a much higher final reward (10247.42 vs. 3693.35) but converged slightly slower (~1127 vs. ~862 episodes). SAC‚Äôs stability (std of 2493.55 vs. 1077.01) reflects its robustness in complex dynamics, though NAF shows lower variance. The GIF below shows SAC‚Äôs fluid running motion compared to NAF‚Äôs less coordinated movements.\nReward comparison in HalfCheetah-v4: SAC consistently outperforms NAF in both speed and stability.\nDiscussion \u0026amp; Conclusion This project examined the performance differences between Value-Based and Actor-Critic algorithms in both discrete and continuous environments.\nThe experimental results indicate that no single algorithm is universally superior; rather, the environment characteristics and action space type play a decisive role in determining performance.\nAnalysis \u0026amp; Interpretation Continuous Action Spaces SAC consistently outperformed NAF in both Pendulum-v1 and HalfCheetah-v4, thanks to its entropy maximization strategy, which promotes exploration and robustness. NAF‚Äôs fixed quadratic Q-function structure limited its flexibility in high-dimensional or complex tasks, leading to slower convergence and higher variance in rewards. SAC‚Äôs ability to directly optimize a stochastic policy made it particularly effective in continuous control scenarios.\nDiscrete Action Spaces In FrozenLake-v1, a stochastic environment, A2C‚Äôs stability (due to advantage estimation) gave it an edge over DQN, achieving higher success rates and faster convergence. In MountainCar-v0, a deterministic environment with a small action space, DQN‚Äôs value-based approach excelled in final reward and convergence speed, though A2C remained more stable. This highlights the suitability of Value-Based methods for simpler, deterministic settings and Actor-Critic methods for stochastic or complex environments.\nKey Findings: In simple discrete environments, such as FrozenLake, Value-Based algorithms (e.g., DQN) achieved competitive performance, but Actor-Critic algorithms (e.g., A2C) showed faster convergence and more stable learning. In continuous and more complex environments, Actor-Critic algorithms ‚Äî particularly SAC ‚Äî outperformed their Value-Based counterparts in terms of final reward and convergence speed. Observed Trade-offs: Aspect Value-Based Actor-Critic Simplicity of implementation Yes More complex Initial learning speed High in simple environments Depends on tuning Training stability More oscillations More stable Suitability for continuous spaces Not always Yes Overall, the choice between Value-Based and Actor-Critic methods should be guided by the nature of the task, the complexity of the environment, and the available computational budget.\nObservations Based on Environment Characteristics Our experimental results further reveal that the nature of the environment‚Äîin terms of action space, state space, and reward structure‚Äîsignificantly impacts algorithm performance:\nAction Space (Discrete vs. Continuous) Discrete Action Spaces: Value-Based algorithms like DQN tend to perform competitively, especially in small and low-dimensional discrete action spaces. They converge quickly and reliably, but may struggle when the action space grows larger or stochasticity increases. Actor-Critic methods such as A2C can still provide improved stability in these scenarios, especially under stochastic transitions. Continuous Action Spaces: Actor-Critic methods (e.g. SAC) dominate due to their ability to output continuous actions directly. Value-Based methods require specialized approximations (like NAF), which often limit flexibility and performance in high-dimensional or continuous control tasks. State Space (Low-dimensional vs. High-dimensional) Low-dimensional states (e.g., MountainCar, FrozenLake) generally favor Value-Based methods, which can efficiently enumerate or approximate Q-values. High-dimensional states (e.g., HalfCheetah) require the policy network of Actor-Critic methods to generalize across large state spaces. These methods better handle complex dynamics and correlations among state variables. Reward Structure (Sparse vs. Dense) Sparse Reward Environments (e.g., FrozenLake) challenge Value-Based methods to propagate value signals efficiently, potentially slowing convergence. Actor-Critic algorithms can leverage advantage estimation and policy gradients to maintain learning stability even with sparse rewards. Dense Reward Environments (e.g., HalfCheetah, Pendulum) allow both families to learn effectively, but Actor-Critic methods often achieve smoother and faster convergence due to direct policy optimization combined with value guidance. The interplay between action space type, state space complexity, and reward sparsity fundamentally shapes the suitability of each algorithm. In general:\nDiscrete + Low-dimensional + Dense reward ‚Üí Value-Based methods are competitive. Continuous + High-dimensional + Sparse or Dense reward ‚Üí Actor-Critic methods provide superior learning stability and higher final performance. These insights complement the empirical trade-offs already observed in our study, providing a more nuanced understanding of when and why certain RL algorithms excel under different environment characteristics.\nLimitations and Future Work Limitations The present project, which evaluates the performance of reinforcement learning algorithms (DQN, A2C, NAF, and SAC) in discrete (FrozenLake-v1 and MountainCar-v0) and continuous (Pendulum-v1 and HalfCheetah-v4) environments, provides valuable insights but is subject to several limitations, outlined below:\nLimited Number of Environments:\nThe study only examines four specific environments, which may not provide sufficient diversity to generalize results across all types of reinforcement learning environments. More complex environments with larger state or action spaces or different dynamics could yield different outcomes. Lack of Random Seed Variation:\nThe reported results are based on a single run or an average of a limited number of runs. Conducting multiple experiments with different random seeds could better demonstrate the robustness and reliability of the results. Focus on Specific Metrics:\nThe evaluation metrics (final reward average, convergence speed, and stability) cover only certain aspects of algorithm performance. Other metrics, such as computational efficiency, training time, or robustness to environmental noise, were not assessed. Future Work To build upon the findings of this project, which evaluated the performance of reinforcement learning algorithms (DQN, A2C, NAF, and SAC) in discrete (FrozenLake-v1, MountainCar-v0) and continuous (Pendulum-v1, HalfCheetah-v4) environments, several directions for future research and development can be pursued to address the limitations and extend the scope of the study:\nBroader Range of Environments:\nFuture work could include testing the algorithms on a wider variety of environments, such as those with larger state and action spaces, partially observable states (e.g., POMDPs), or real-world-inspired tasks. This would help validate the generalizability of the observed performance trends. Incorporating Random Seed Variations:\nConducting multiple runs with different random seeds would improve the robustness of results and allow for statistical analysis of performance variability, ensuring that conclusions are not biased by specific initial conditions. Evaluation of Additional Metrics:\nFuture studies could incorporate metrics such as computational efficiency, memory usage, training time, and robustness to environmental perturbations (e.g., noise or dynamic changes). This would provide a more holistic view of algorithm suitability for practical applications. Real-World Application: Robotics The insights gained from this project have significant potential for real-world applications, particularly in robotics, where reinforcement learning can enable autonomous systems to perform complex tasks. The following outlines how the evaluated algorithms could be applied and extended in robotics contexts:\nRobotic Manipulation:\nAlgorithms like SAC, which performed well in continuous control tasks (e.g., Pendulum-v1, HalfCheetah-v4), could be applied to robotic arms for tasks such as grasping, object manipulation, or assembly. SAC‚Äôs ability to handle continuous action spaces makes it suitable for precise control in high-dimensional settings. Autonomous Navigation:\nDiscrete action space algorithms like DQN and A2C, tested in environments like FrozenLake-v1, could be adapted for robot navigation in grid-like or structured environments (e.g., warehouse robots). A2C‚Äôs stability in stochastic settings could be particularly useful for navigating dynamic or uncertain environments. Locomotion and Mobility:\nThe success of SAC in HalfCheetah-v4 suggests its potential for controlling legged robots or humanoid robots for locomotion tasks. Future work could involve applying SAC to real-world robotic platforms to achieve robust and efficient walking or running behaviors. These future research directions and real-world applications highlight the potential to extend the current study‚Äôs findings to more diverse and practical scenarios. By addressing the identified limitations and applying the algorithms to robotics, this work can contribute to the development of more robust, efficient, and adaptable autonomous systems.\nReferences A2C (Advantage Actor-Critic):\nMnih, V., et al. (2016). \u0026ldquo;Asynchronous Methods for Deep Reinforcement Learning.\u0026rdquo; ICML 2016. Paper Stable-Baselines3 A2C Implementation: GitHub SAC (Soft Actor-Critic):\nHaarnoja, T., et al. (2018). \u0026ldquo;Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor.\u0026rdquo; ICML 2018. Paper Stable-Baselines3 SAC Implementation: GitHub DQN (Deep Q-Network):\nMnih, V., et al. (2015). \u0026ldquo;Human-level control through deep reinforcement learning.\u0026rdquo; Nature, 518(7540), 529-533. Paper Stable-Baselines3 DQN Implementation: GitHub NAF (Normalized Advantage Function):\nGu, S., et al. (2016). \u0026ldquo;Continuous Deep Q-Learning with Model-based Acceleration.\u0026rdquo; ICML 2016. Paper Gymnasium:\nOfficial Gymnasium Documentation: Gymnasium ","permalink":"http://localhost:1313/posts/actor-critic-vs-value-based-empirical-trade-offs/","summary":"This study compares Value-Based (DQN, NAF) and Actor-Critic (A2C, SAC) reinforcement learning algorithms in diverse environments (MountainCar-v0, Pendulum-v1, FrozenLake-v1, HalfCheetah-v4). Through empirical evaluation, we analyze trade-offs in convergence speed, learning stability, and final performance, supported by a user-friendly GUI and mobile application for interactive training and visualization.","title":"Actor-Critic vs. Value-Based: Empirical Trade-offs"},{"content":"Introduction This paper critically examines the assumptions commonly accepted in modeling reinforcement learning problems, suggesting that these assumptions may impede progress in the field. The authors refer to these assumptions as \u0026ldquo;dogmas.\u0026rdquo;\nDogma: A fixed, especially religious, belief or set of beliefs that people are expected to accept without any doubts. ‚ÄîFrom the Cambridge Advanced Learner\u0026rsquo;s Dictionary \u0026amp; Thesaurus\nThe paper introduces three central dogmas:\nThe Environment Spotlight Learning as Finding a Solution The Reward Hypothesis (although not exactly a dogma) The author argues the true reinforcement learning landscape is actualy like this,\nIn the words of Rich Sutton:\nRL can be viewed as a microcosm of the whole AI problem.\nHowever, today\u0026rsquo;s RL landscape is overly simplified,\nThese three dogmas are responsible for narrowing the potential of RL,\nThe authors propose that we consider moving beyond these dogmas,\nTo reclaim the true landscape of RL,\nBackground The authors reference Thomas Kuhn\u0026rsquo;s book, \u0026ldquo;The Structure of Scientific Revolutions\u0026rdquo;,\nKuhn distinguishes between two phases of scientific activity,\nNormal Science: Resembling puzzle-solving. Revolutionary Phase: Involving a fundamental rethinking of the values, methods, and commitments of science, which Kuhn calls a \u0026ldquo;paradigm.\u0026rdquo; Here\u0026rsquo;s an example of a previous paradigm shift in science:\nThe authors explore the paradigm shift needed in RL:\nDogma One: The Environment Spotlight The first dogma we call the environment spotlight, which refers to our collective focus on modeling environments and environment-centric concepts rather than agents.\nWhat do we mean when we say that we focus on environments? We suggest that it is easy to answer only one of the following two questions:\nWhat is at least one canonical mathematical model of an environment in RL?\nMDP and its variants! And we define everything in terms of it. By embracing the MDP, we are allowed to import a variety of fundamental results and algorithms that define much of our primary research objectives and pathways. For example, we know every MDP has at least one deterministic, optimal, stationary policy, and that dynamic programming can be used to identify this policy. What is at least one canonical mathematical model of an agent in RL?\nIn contrast, this question has no clear answer! The author suggests it is important to define, model, and analyse agents in addition to environments. We should build toward a canonical mathematical model of an agent that can open us to the possibility of discovering general laws governing agents (if they exist).\nDogma Two: Learning as Finding a Solution The second dogma is embedded in the way we treat the concept of learning. We tend to view learning as a finite process involving the search for‚Äîand eventual discovery of‚Äîa solution to a given task.\nWe tend to implicitly assume that the learning agents we design will eventually find a solution to the task at hand, at which point learning can cease. Such agents can be understood as searching through a space of representable functions that captures the possible action-selection strategies available to an agent, similar to the Problem Space Hypothesis, and, critically, this space contains at least one function‚Äîsuch as the optimal policy of an MDP‚Äîthat is of sufficient quality to consider the task of interested solved. Often, we are then interested in designing learning agents that are guaranteed to converge to such an endpoint, at which point the agent can stop its search (and thus, stop its learning).\nThe author suggests to embrace the view that learning can also be treated as adaptation. As a consequence, our focus will drift away from optimality and toward a version of the RL problem in which agents continually improve, rather than focus on agents that are trying to solve a specific problem.\nWhen we move away from optimality,\nHow do we think about evaluation? How, precisely, can we define this form of learning, and differentiate it from others? What are the basic algorithmic building blocks that carry out this form of learning, and how are they different from the algorithms we use today? Do our standard analysis tools such as regret and sample complexity still apply? These questions are important, and require reorienting around this alternate view of learning.\nThe authors introduce the book \u0026ldquo;Finite and Infinite Games\u0026rdquo;,\nAnd the concept of Finite and Infinite Games is summarized in the following quote,\nThere are at least two kinds of games, One could be called finite; the other infinite. A finite game is played for the purpose of winning, an infinite game for the purpose of continuing the play.\nAnd argues alignment is an infinite game.\nDogma Three: The Reward Hypothesis The third dogma is the reward hypothesis, which states \u0026ldquo;All of what we mean by goals and purposes can be well thought of as maximization of the expected value of the cumulative sum of a received scalar signal (reward).\u0026rdquo;\nThe authors argue that the reward hypothesis is not truly a dogma. Nevertheless, it is crucial to understand its nuances as we continue to design intelligent agents.\nThe reward hypothesis basically says,\nIn recent analysis by [2] fully characterizes the implicit conditions required for the hypothesis to be true. These conditions come in two forms. First, [2] provide a pair of interpretative assumptions that clarify what it would mean for the reward hypothesis to be true or false‚Äîroughly, these amount to saying two things (brwon doors).\nFirst, that \u0026ldquo;goals and purposes\u0026rdquo; can be understood in terms of a preference relation on possible outcomes. Second, that a reward function captures these preferences if the ordering over agents induced by value functions matches that of the ordering induced by preference on agent outcomes. This leads to the following conjecture,\nThen, under this interpretation, a Markov reward function exists to capture a preference relation if and only if the preference relation satisfies the four von Neumann-Morgenstern axioms, and a fifth Bowling et al. call $\\gamma$-Temporal Indifference.\nAxiom 1: Completeness \u0026gt; You have a preference between every outcome pair.\nYou can always compare any two choices. Axiom 2: Transitivity \u0026gt; No preference cycles.\nIf you like chocolate more than vanilla, and vanilla more than strawberry, you must like chocolate more than strawberry. Axiom 3: Independence \u0026gt; Independent alternatives can\u0026rsquo;t change your preference.\nIf you like pizza more than salad, and you have to choose between a lottery of pizza or ice cream and a lottery of salad or ice cream, you should still prefer the pizza lottery over the salad lottery. Axiom 4: Continuity \u0026gt; There is always a break even chance.\nImagine you like a 100 dollar bill more than a 50 dollar bill, and a 50 dollar bill more than a 1 dollar bill. There should be a scenario where getting a chance at 100 dollar and 1 dollar, with certain probabilities, is equally good as getting the 50 dollar for sure. These 4 axioms are called the von Neumann-Morgenstern axioms.\nAxiom 5: Temporal $\\boldsymbol{\\gamma}$-Indifference \u0026gt; Discounting is consistent throughout time. Temporal $\\gamma$-indifference says that if you are indifferent between receiving a reward at time $t$ and receiving the same reward at time $t+1$, then your preference should not change if we move both time points by the same amount. For instance, if you don\u0026rsquo;t care whether you get a candy today or tomorrow, then you should also not care whether you get the candy next week or the week after. Taking these axioms into account, the reward conjecture becomes the reward theorem,\nIt is essential to consider that people do not always conform to these axioms, and human preferences can vary.\nIt is important that we are aware of the implicit restrictions we are placing on the viable goals and purposes under consideration when we represent a goal or purpose through a reward signal. We should become familiar with the requirements imposed by the five axioms, and be aware of what specifically we might be giving up when we choose to write down a reward function.\nSee Also David Abel Presentation @ ICML 2023 David Abel Personal Website Mark Ho Personal Website Anna Harutyunyan Personal Website References [1] Abel, David, Mark K. Ho, and Anna Harutyunyan. \u0026ldquo;Three Dogmas of Reinforcement Learning.\u0026rdquo; arXiv preprint arXiv:2407.10583 (2024).\n[2] Bowling, Michael, et al. \u0026ldquo;Settling the reward hypothesis.\u0026rdquo; International Conference on Machine Learning. PMLR, 2023.\n","permalink":"http://localhost:1313/posts/three-dogmas-of-reinforcement-learning/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eThis paper critically examines the assumptions commonly accepted in modeling reinforcement learning problems, suggesting that these assumptions may impede progress in the field. The authors refer to these assumptions as \u0026ldquo;dogmas.\u0026rdquo;\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003eDogma: A fixed, especially religious, belief or set of beliefs that people are expected to accept without any doubts.\u003c/strong\u003e \u003cem\u003e‚ÄîFrom the \u003ca href=\"https://dictionary.cambridge.org/dictionary/english\"\u003eCambridge Advanced Learner\u0026rsquo;s Dictionary \u0026amp; Thesaurus\u003c/a\u003e\u003c/em\u003e\u003c/p\u003e\u003c/blockquote\u003e\n\u003cp\u003eThe paper introduces three central dogmas:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eThe Environment Spotlight\u003c/li\u003e\n\u003cli\u003eLearning as Finding a Solution\u003c/li\u003e\n\u003cli\u003eThe Reward Hypothesis (although not exactly a dogma)\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eThe author argues the true reinforcement learning landscape is actualy like this,\u003c/p\u003e","title":"Three Dogmas of Reinforcement Learning"},{"content":"Introduction This paper studies the problem of zero-shot generalization in reinforcement learning and introduces an algorithm that trains an ensamble of maximum reward seeking agents and an maximum entropy agent. At test time, either the ensemble agrees on an action, and we generalize well, or we take exploratory actions with the help of maximum entropy agent and drive us to a novel part of the state space, where the ensemble may potentially agree again. This method combined with an invariance based approach achieves new state-of-the-art results on ProcGen.\nBackground I know it\u0026rsquo;s a lot to take in! You may be wondering:\nWhat is reinforcement learning? ü•∫ What generalization means for RL? ü•≤ What is zero-shot generalization? ü•π What are max reward and max entropy agents?! ‚òπÔ∏è What is an ensamble of them?!! üòü What is an invariance based approach? üòì And what the heck is ProcGen?!!! üò† Don\u0026rsquo;t worry! We are going to cover all of that and more! And you are going to fully understand this paper and finish reading this article with an smile üôÇ!\nWhat is reinforcement learning? Reinforcement learning is learning what to do‚Äîhow to map situations to actions‚Äîso as to maximize a numerical reward signal. Imagine yourself right now, in reinforcement learning terms, you are an agent and everything that is not you, is your environment. You perceive the world through your senses (e.g., eyes, ears, etc.) and what you perceive turns into electrical signals that your brain processes to form an understanding of your surroundings (state). Based on this understanding, you make decisions (actions) with the goal of achieving the best possible outcome for yourself (reward).\nIn a more formal sense, reinforcement learning involves the following components:\nAgent: The learner or decision maker (e.g., you). Environment: Everything the agent interacts with (e.g., the world around you). State ($s$): A representation of the current situation of the agent within the environment (e.g., what you see, hear, and feel at any given moment). Actions ($a$): The set of all possible moves the agent can make (e.g., moving your hand, walking, speaking). Reward ($r$): The feedback received from the environment in response to the agent‚Äôs action (e.g., pleasure from eating food, pain from touching a hot surface). Policy ($\\pi$): A strategy used by the agent to decide which actions to take based on the current state (e.g., your habits and decision-making processes). Value Function ($V$): A function that estimates the expected cumulative reward of being in a certain state and following a particular policy (e.g., your prediction of future happiness based on current actions). The objective of the agent is to develop a policy that maximizes the total cumulative reward over time. This is typically achieved through a process of exploration (trying out new actions to discover their effects) and exploitation (using known actions that yield high rewards).\nIn mathematical terms, the goal is to find a policy $\\pi$ that maximizes the expected return $G_t$, which is the cumulative sum of discounted rewards:\n$$ G_t = r_{t+1} + \\gamma r_{t+2} + \\gamma^2 r_{t+3} + \\cdots $$\nwhere $\\gamma$ (0 ‚â§ $\\gamma$ \u0026lt; 1) is the discount factor that determines the importance of future rewards.\nFor a more thorough and in depth explanation of reinforcement learning please refer to Lilian Weng excelent blog post A (Long) Peek into Reinforcement Learning.\nWhat generalization means for RL? In reinforcement learning, generalization involves an agent\u0026rsquo;s ability to apply learned policies or value functions to new states or environments that it has not encountered during training. This is essential because it is often impractical or impossible to train an agent on every possible state it might encounter.\nThere are bunch of methods that researchers have used to tackle the problem of generalization in reinforcement learning that are summarized in the following diagram.\nThis paper focuses on RL-Specific solutions and specifically exploration technique to tackle the problem of generalization. If you\u0026rsquo;re interested to learn more about the generalization problem in reinforcement learning please refer to reference [3].\nWhat is zero-shot generalization? Zero-shot generalization in RL refers to the ability of an agent to perform well in entirely new environments or tasks without any prior specific training or fine-tuning on those environments or tasks. This is a significant challenge because it requires the agent to leverage its learned representations and policies in a highly flexible and adaptive manner.\nIn order to define the objective of zero-shot generalization we first have to define what MDPs and POMDPs are.\nMarkov Decision Process (MDP):\nMDP is a mathematical framework used to describe an environment in reinforcement learning where the outcome is partly random and partly under the control of a decision-maker (agent). A MDP is a tuple $M = (S, A, P_{init}, P, r, \\gamma)$ where,\nStates ($S \\in \\mathbb{R}^{|S|}$): A finite set of states that describe all possible situations in which the agent can be. Actions ($A \\in \\mathbb{R}^{|A|}$): A finite set of actions available to the agent. Initial State Distribution ($P_{init}$): A distribution of starting state $(s_0 \\sim P_{init})$. Transition Probability ($P$): A function $P(s_{t+1}, s_t, a_t)$ representing the probability of transitioning from state $s_t$ to state $s_{t+1}$ after taking action $a_t$ $(s_{t+1} \\sim P(.|s_t,a_t))$. Reward ($r: S \\times A \\rightarrow \\mathbb{R}$): A function $r(s_t, a_t)$ representing the immediate reward $r_t$ received after transitioning from state $s_t$ due to action $a_t$ $(r_t = r(s_t, a_t))$. Discount Factor ($\\gamma$): $(0 \\leq \\gamma \u0026lt; 1)$ is a constant that determines the importance of future rewards. Partially Observable Markov Decision Process (POMDP):\nPOMDP extends MDPs to situations where the agent does not have complete information about the current state. Instead, the agent must make decisions based on partial observations. A POMDP is a tuple $M = (S, A, O, P_{init}, P, \\Sigma, r, \\gamma)$ where other that above definitions for MDP,\nObservation Space ($O$): A finite set of observations the agent can receive about the state. Observation Function ($\\Sigma$): A function that given current state $s_t$ and current action $a_t$ gives us the current observation $o_t$ $(o_t = \\Sigma(s_t, a_t) \\in O)$. If we set $O = S$ and $\\Sigma(s,a) = s$, the POMDP turns into regular MDP.\nLet the history at time $t$ be,\n$$ h_t = \\{ o_0, a_0, r_0, o_1, a_1, r_1, \\dots, o_t \\} $$\nThe agent‚Äôs next action is outlined by a policy $\\pi$, which is a stochastic mapping from the history to an action probability,\n$$ \\pi(a|h_t) = P(a_t=a|h_t) $$\nIn this formulation, a history-dependent policy (and not a Markov policy) is required both due to partially observed states, epistemic uncertainty, and also for optimal maxEnt exploration.\nWe assume a prior distribution over POMDPs $P(M)$, defined over some space of POMDPs. For a given POMDP, an optimal policy maximizes the expected discounted return,\n$$ \\mathbb{E}_{\\pi,M} \\left[ \\sum _{t=0}^{\\infty} \\gamma^t r(s_t,a_t) \\right] $$\nWhere the expectation is taken over the policy $\\pi(h_t)$, and the state transition probability $s_t \\sim P$ of POMDP $M$.\nOur generalization objective is to maximize the discounted cumulative reward taken in expectation over the POMDP prior,\n$$ \\mathcal{R}_ {pop}(\\pi) = \\mathbb{E}_ {M \\sim P(M)} {\\left[ \\mathbb{E}_{\\pi,M} {\\left[ \\sum _{t=0}^{\\infty} \\gamma^t r(s_t,a_t) \\right]} \\right]} $$\nSeeking a policy that performs well in expectation over any POMDP from the prior corresponds to zero-shot generalization.\nAnd as you may have guessed we don\u0026rsquo;t have access to true prior distribution of POMDPs so we have to estimate it with $N$ training POMDPs $M_1, M_2, \\dots, M_N$ sampled from the true prior distribution $P(M)$. So we are going to maximize empirical discounted cumulative reward,\n$$ \\begin{align*} \\mathcal{R}_ {emp}(\\pi) \u0026amp;= \\frac{1}{N} \\sum _ {i=1}^{N} \\mathbb{E} _ {\\pi,M_i} \\left[ \\sum _ {t=0}^{\\infty} \\gamma^t r(s_t,a_t) \\right] \\\\ \u0026amp;= \\mathbb{E}_ {M \\sim \\hat{P}(M)} {\\left[ \\mathbb{E}_{\\pi,M} {\\left[ \\sum _{t=0}^{\\infty} \\gamma^t r(s_t,a_t) \\right]} \\right]} \\end{align*} $$\nWhere the empirical POMDP distribution $\\hat{P}(M)$ can be different from the true distribution, i.e. $\\hat{P}(M) \\neq P(M)$. In general, a policy that optimizes the empirical reward may perform poorly on the population reward and this is known as overfitting in statistical learning theory.\nWhat are max reward and max entropy agents?! As we have seen, the goal of agents in reinforcement learning is to find a policy $\\pi$ that maximizes the expected discounted return by focusing on actions that lead to the greatest immediate or future rewards. We call these common RL agents \u0026ldquo;max reward agents\u0026rdquo; in this paper. On the other hand, \u0026ldquo;max entropy agents\u0026rdquo; aim to maximize the entropy of the policy for visiting different states. Maximizing entropy encourages the agent to explore a wider range of actions that lead the agent to visit new states even when they don\u0026rsquo;t contribute any reward.\nThis type of agent will help us to make decisions when we have epistemic uncertainty about what to do at test time. Epistemic uncertainty basically means the uncertainty that we have because of our lack of knowledge and can be improved by gathering more information about the situation.\nThe insight of the authors of this paper is that learning a policy that effectively explores the domain is harder to memorize than a policy that maximizes reward for a specific task, and therefore they expect such learned behavior to generalize well.\nWhat is an ensamble of them?!! Ensembling in reinforcement learning involves combining the policies of multiple individual agents to make more accurate and robust decisions. The idea is that by aggregating the outputs of different agents, we can leverage their diverse perspectives and expertise to improve overall performance.\nThis paper uses an ensamble of max reward agents to help the agent decide on the overal epistemic uncetainty that we have at test time.\nWhat is an invariance based approach? Invariance based algorithms in reinforcement learning focus on developing policies that are robust to changes and variations in the environment. These algorithms aim to identify and leverage invariant features or patterns that remain consistent across different environments or tasks. The goal is to ensure that the learned policy performs well not just in the training environment but also in new, unseen environments.\nThis paper uses IDAAC algorithm which is a special kind of DAAC algorithm as its invariance based algorithm.\nDAAC (Decoupled Advantage Actor-Critic) uses two separate networks, one for learning the policy and advantage, and one for learning the value. The value estimates are used to compute the advantage targets.\nIDAAC (Invariant Decoupled Advantage Actor-Critic) adds an additional regularizer to the DAAC policy encoder to ensure that it does not contain episode-specific information. The encoder is trained adversarially with a discriminator so that it cannot classify which observation from a given pair $(s_i, s_j)$ was first in a trajectory.\nFor more information about IDAAC algorithm please refer to reference [4].\nWhat the heck is ProcGen?!!! Procgen is a benchmark suite for evaluating the generalization capabilities of reinforcement learning agents. It was developed by OpenAI and consists of a collection of procedurally generated environments that vary in terms of visual appearance, dynamics, and difficulty. The goal of Procgen is to provide a standardized and challenging set of environments that can be used to assess the ability of RL algorithms to generalize to unseen scenarios.\nIf you are new to reinforcement learning, I know these explanations are a lot to take in! If you find yourself lost, I recommend to check out the following courses at your leisure:\nDeep Reinforcement Learning (by Hugging Face ü§ó) Reinforcement Learning (by Mutual Information) Introduction to Reinforcement Learning (by David Silver) Reinforcement Learning (by Michael Littman \u0026amp; Charles Isbell) Reinforcement Learning (by Emma Brunskill) Deep Reinforcement Learning Bootcamp 2017 Foundations of Deep RL (by Pieter Abbeel) Deep Reinforcement Learning \u0026amp; Control (by Katerina Fragkiadaki) Deep Reinforcement Learning (by Sergey Levine) After reviewing all this we can focus on the rest of the paper!\nHidden Maze Experiment One of the key observation of the authors of this paper is that invariance is not enough for zero-shot generalization of reinforcemen learning algorithm. They designed the hidden maze experiment too demonstrate that. Imagine Maze, but with the walls and goal hidden in the observation. Arguably, this is the most task-invariant observation possible, such that a solution can still be obtained in a reasonable time.\nAn agent with memory can be trained to optimally solve all training tasks: figuring out wall positions by trying to move ahead and observing the resulting motion, and identifying based on its movement history in which training maze it is currently in. Obviously, such a strategy will not generalize to test mazes. Performance in Maze, where the strategy for solving any particular training task must be indicative of that task, has largely not improved by methods based on invariance\nThe following figure shows PPO performance on the hidden maze task, indicating severe overfitting.\nAs described by [5], an agent can overcome test-time errors in its policy by treating the perfect policy as an unobserved variable. The resulting decision making problem, termed the epistemic POMDP, may require some exploration at test time to resolve uncertainty. The article further proposed the LEEP algorithm based on this principle, which trains an ensemble of agents and essentially chooses randomly between the members when the ensemble does not agree, and was the first method to present substantial generalization improvement on Maze.\nIn this paper authors extend this idea and asked, How to improve exploration at test time?, and their approach is based on a novel discovery, when they train an agent to explore the training domains using a maximum entropy objective, they observe that the learned exploration behavior generalizes surprisingly well (much better than the generalization attained when training the agent to maximize reward).\nIn the following section we gonna dig deep into internals of maximum entropy policy.\nMaxEnt Policy For simplicity the authors discuss this part for the MDP case. A policy $\\pi$, through its interaction with an MDP, induces a t-step state distribution over the state space $S$,\n$$ d _ {t,\\pi} (s) = p(s_t=s | \\pi) $$\nThe objective of maximum entropy exploration is given by:\n$$ \\mathcal{H}(d(.)) = -\\mathbb{E} _ {s \\sim d} \\left[ \\log{d(s)} \\right] $$\nWhere $d$ can be regarded as either,\nStationary state distribution (infinite horizon): $d _ {\\pi} = \\lim _ {t \\rightarrow \\infty} d _ {t,\\pi} (s)$ Discounted state distribution (infinite horizon): $d _ {\\gamma, \\pi} = (1-\\gamma) \\sum _ {t=0} ^ {\\infty} \\gamma^t d _ {t,\\pi} (s)$ Marginal state distribution (finite horizon): $d _ {T, \\pi} = \\frac{1}{T} \\sum _ {t=0} ^ {T} d _ {t,\\pi} (s)$ In this work they focus on the finite horizon setting and adapt the marginal state distribution $d _ {T, \\pi}$ in which $T$ equals the episode horizon $H$, so we seek to maximize the objective:\n$$ \\begin{align*} \\mathcal{R} _ {\\mathcal{H}} (\\pi) \u0026amp;= \\mathbb{E} _ {M \\sim \\hat{P}(M)} \\left[ \\mathcal{H}(d _ {H,\\pi}) \\right] \\\\ \u0026amp;=\\mathbb{E} _ {M \\sim \\hat{P}(M)} \\left[ \\mathcal{H}(\\frac{1}{H} \\sum _ {t=0} ^ {H} d _ {t,\\pi} (s)) \\right] \\end{align*} $$\nwhich yields a policy that \u0026ldquo;equally\u0026rdquo; visits all states during the episode.\nTo maximize this objective we can estimating the density of the agent\u0026rsquo;s state visitation distribution, but in this paper the authors adapt the non-parametric entropy estimation approach; we estimate the entropy using the particle based k-nearest neighbor (k-NN estimator).\nTo estimate the distribution $d _ {H,\\pi}$ over the states $S$, we consider each trajectory as $H$ samples of states $\\{ s_t \\} _ {t=1} ^ {H}$ and take $s _ t ^ {\\text{k-NN}}$ to be the k-NN of the state $s_t$ within the trajectory,\n$$ \\hat{ \\mathcal{H} } ^ {k,H} (d _ {H,\\pi}) \\approx \\frac{1}{H} \\sum _ {t=1} ^ {H} \\log ( \\parallel s_t - s_t^{\\text{k-NN}} \\parallel _ 2) $$\nIn which we define intrinsic reward function as,\n$$ r_I (s_t) \\coloneqq \\log ( \\parallel s_t - s_t^{\\text{k-NN}} \\parallel _ 2) $$\nThis formulation enables us to deploy any RL algorithm to approximately optimize objective.\nSpecifically, in this work we use the policy gradient algorithm PPOŸà where at every time step $t$, the state $s_t^{\\text{k-NN}}$ is chosen from previous states $\\{ s_t \\} _ {t=1} ^ {t-1}$ of the same episode. To improve computational efficiency, instead of taking the full observation as the state (64 x 64 RGB image), we sub-sample the observation by applying average pooling of 3 x 3 to produce an image of size 21 x 21.\nWe found that agents trained for maximum entropy exploration exhibit a smaller generalization gap compared with the standard approach of training solely with extrinsic reward. The policies are equipped with a memory unit (GRU) to allow learning of deterministic policies that maximize the entropy.\nIn all three environments, we demonstrate a small generalization gap, as test performance on unseen levels closely follows the performance achieved during training.\nIn addition, we verify that the train results are near optimal by comparing with a hand designed approximately optimal exploration policy. For example, on Maze we use the well known maze exploring strategy wall follower, also known as the left/right-hand rule.\nExpGen Algorithm Our main insight is that, given the generalization property of the entropy maximization policy established above, an agent can apply this behavior in a test MDP and expect effective exploration at test time. We pair this insight with the epistemic POMDP idea, and propose to play the exploration policy when the agent faces epistemic uncertainty, hopefully driving the agent to a different state where the reward-seeking policy is more certain.\nOur framework comprises two parts: an entropy maximizing network and an ensemble of networks that maximize an extrinsic reward to evaluate epistemic uncertainty. The first step entails training a network equipped with a memory unit to obtain a maxEnt policy $\\pi_H$ that maximizes entropy. Next, we train an ensemble of memory-less policy networks $\\{ \\pi _ r ^ j \\} _ {j=1} ^ {m} $ to maximize extrinsic reward.\nHere is the ExpGen algorithm,\nWe consider domains with a finite action space, and say that the policy $\\pi _ r ^ i$ is certain at state $s$ if its action $a_i \\sim \\pi _ r ^ i (a|s)$ is in consensus with the ensemble: $a_i = a_j$ for the majority of $k$ out of $m$, where $k$ is a hyperparameter of our algorithm.\nSwitching between two policies may result in a case where the agent repeatedly toggles between two states (if, say, the maxEnt policy takes the agent from state $s_1$ to a state $s_2$, where the ensemble agrees on an action that again moves to state $s_1$.). To avoid such ‚Äúmeta-stable‚Äù behavior, we randomly choose the number of maxEnt steps $n_{\\pi_{\\mathcal{H}}}$ from a Geometric distribution, $n_{\\pi_{\\mathcal{H}}} \\sim Geom(\\alpha)$.\nExperiments Our experimental setup follows ProcGen\u0026rsquo;s easy configuration, wherein agents are trained on 200 levels for 25M steps and subsequently tested on random levels. All agents are implemented using the IMPALA (Importance Weighted Actor-Learner Architectures) convolutional architecture, and trained using PPO or IDAAC. For the maximum entropy agent $\\pi_H$ we incorporate a single GRU at the final embedding of the IMPALA convolutional architecture. For all games, we use the same parameter $\\alpha=0.5$ of the Geometric distribution and form an ensemble of 10 networks.\nFollowing figure is comparison across all ProcGen games, with 95% bootstrap CIs highlighted in color. Score distributions of ExpGen, PPO, PLR, UCB-DrAC, PPG and IDAAC.\nFollowing figure shows in each row, the probability of algorithm X outperforming algorithm Y. The comparison illustrates the superiority of ExpGen over the leading contender IDAAC with probability 0.6, as well as over other methods with even higher probability.\nSee Also PyTorch implementation of ExpGen @ GitHub Ev Zisselman Presentation @ NeurIPS 2023 ExpGen Rebuttal Process @ OpenReview ExpGen Poster for NeurIPS 2023 References [1] Zisselman, Ev, et al. \u0026ldquo;Explore to generalize in zero-shot rl.\u0026rdquo; Advances in Neural Information Processing Systems 36 (2024).\n[2] Sutton, Richard S., and Andrew G. Barto. \u0026ldquo;Reinforcement learning: An introduction.\u0026rdquo; MIT press, (2020).\n[3] Kirk, Robert, et al. \u0026ldquo;A survey of zero-shot generalisation in deep reinforcement learning.\u0026rdquo; Journal of Artificial Intelligence Research 76 (2023): 201-264.\n[4] Raileanu, Roberta, and Rob Fergus. \u0026ldquo;Decoupling value and policy for generalization in reinforcement learning.\u0026rdquo; International Conference on Machine Learning. PMLR, 2021.\n[5] Ghosh, Dibya, et al. \u0026ldquo;Why generalization in rl is difficult: Epistemic pomdps and implicit partial observability.\u0026rdquo; Advances in neural information processing systems 34 (2021): 25502-25515.\n[6] Espeholt, Lasse, et al. \u0026ldquo;Impala: Scalable distributed deep-rl with importance weighted actor-learner architectures.\u0026rdquo; International conference on machine learning. PMLR, 2018.\n","permalink":"http://localhost:1313/posts/explore-to-generalize-in-zero-shot-rl/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eThis paper studies the problem of zero-shot generalization in reinforcement learning and introduces an algorithm that trains an ensamble of maximum reward seeking agents and an maximum entropy agent. At test time, either the ensemble agrees on an action, and we generalize well, or we take exploratory actions with the help of maximum entropy agent and drive us to a novel part of the state space, where the ensemble may potentially agree again. This method combined with an invariance based approach achieves new state-of-the-art results on ProcGen.\u003c/p\u003e","title":"ExpGen: Explore to Generalize in Zero-Shot RL"},{"content":"Welcome to our blog! We are a team of enthusiastic professors and students from Sharif University of Technology who are eager to share our insights and passion for this fascinating field. Our aim is to share our knowledge and excitement about this cutting-edge field with you ‚ù§Ô∏è.\nProfessors Professor Mohammad Hossein Rohban Professor Ehsaneddin Asgari Students Arash Alikhani Alireza Nobakht Labs RIML Lab NLP \u0026amp; DH Lab We hope you enjoy our blog and find our content both informative and inspiring üöÄ!\n","permalink":"http://localhost:1313/us/","summary":"\u003cp\u003eWelcome to our blog! We are a team of enthusiastic professors and students from Sharif University of Technology who are eager to share our insights and passion for this fascinating field. Our aim is to share our knowledge and excitement about this cutting-edge field with you ‚ù§Ô∏è.\u003c/p\u003e\n\u003ch2 id=\"professors\"\u003eProfessors\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eProfessor \u003ca href=\"https://www.linkedin.com/in/mohammad-hossein-rohban-75567677\"\u003eMohammad Hossein Rohban\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003eProfessor \u003ca href=\"https://www.linkedin.com/in/ehsaneddinasgari\"\u003eEhsaneddin Asgari\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"students\"\u003eStudents\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://www.linkedin.com/in/infinity2357\"\u003eArash Alikhani\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://www.linkedin.com/in/alireza-nobakht\"\u003eAlireza Nobakht\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"labs\"\u003eLabs\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/rohban-lab\"\u003eRIML Lab\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/language-ml\"\u003eNLP \u0026amp; DH Lab\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eWe hope you enjoy our blog and find our content both informative and inspiring üöÄ!\u003c/p\u003e","title":""}]