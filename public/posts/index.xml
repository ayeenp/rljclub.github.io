<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Posts on ðŸ§  RL Journal Club</title>
    <link>http://localhost:1313/posts/</link>
    <description>Recent content in Posts on ðŸ§  RL Journal Club</description>
    <generator>Hugo -- 0.149.1</generator>
    <language>en-us</language>
    <lastBuildDate>Sat, 06 Sep 2025 00:00:00 +0000</lastBuildDate>
    <atom:link href="http://localhost:1313/posts/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Emergent Communication in Multi-Agent RL</title>
      <link>http://localhost:1313/posts/emergent-communication-in-multi-agent-reinforcement-learning/</link>
      <pubDate>Sat, 06 Sep 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/emergent-communication-in-multi-agent-reinforcement-learning/</guid>
      <description>&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Beyond Silence: The Rise of Communication in Multi-Agent Reinforcement Learning
Imagine a team of robots on a mission: some are searching for an object, others are navigating a complex maze. They can&amp;rsquo;t see what their teammates see, and they can&amp;rsquo;t shout instructions across the field. This is the challenge of multi-agent reinforcement learning (MARL), where coordination is key but communication is often an afterthought.&lt;/p&gt;
&lt;p&gt;Traditional MARL often faces a major hurdle: partial observability. Each agent only sees a small part of the environment, making it difficult to make globally optimal decisions. Another problem is non-stationarity, where the environment is constantly changing due to the actions of other agents. But what if we could teach these agents to talk to each other? Thatâ€™s where the fascinating field of multi-agent deep reinforcement learning with communication (Comm-MADRL) comes in.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Actor-Critic vs. Value-Based: Empirical Trade-offs</title>
      <link>http://localhost:1313/posts/actor-critic-vs-value-based-empirical-trade-offs/</link>
      <pubDate>Sun, 17 Aug 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/actor-critic-vs-value-based-empirical-trade-offs/</guid>
      <description>An empirical comparison of Value-Based and Actor-Critic reinforcement learning algorithms across discrete and continuous action spaces, analyzing trade-offs in convergence speed, stability, and performance.</description>
    </item>
    <item>
      <title>Three Dogmas of Reinforcement Learning</title>
      <link>http://localhost:1313/posts/three-dogmas-of-reinforcement-learning/</link>
      <pubDate>Tue, 06 Aug 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/three-dogmas-of-reinforcement-learning/</guid>
      <description>by David Abel, Mark K. Ho, and Anna Harutyunyan</description>
    </item>
    <item>
      <title>ExpGen: Explore to Generalize in Zero-Shot RL</title>
      <link>http://localhost:1313/posts/explore-to-generalize-in-zero-shot-rl/</link>
      <pubDate>Mon, 22 Jul 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/explore-to-generalize-in-zero-shot-rl/</guid>
      <description>by Ev Zisselman, Itai Lavie, Daniel Soudry, and Aviv Tamar</description>
    </item>
  </channel>
</rss>
