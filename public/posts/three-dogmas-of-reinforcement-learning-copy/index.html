<!DOCTYPE html>
<html lang="en" dir="auto">

<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="noindex, nofollow">
<title>Three Dogmas of Reinforcement Learning | üß† RL Journal Club</title>
<meta name="keywords" content="dogmas, reward-hypothesis, philosophy, paradigm, ICML, RLC, 2024, RLC24">
<meta name="description" content="by David Abel, Mark K. Ho, and Anna Harutyunyan">
<meta name="author" content="Arash Alikhani">
<link rel="canonical" href="http://localhost:1313/posts/three-dogmas-of-reinforcement-learning-copy/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.fc220c15db4aef0318bbf30adc45d33d4d7c88deff3238b23eb255afdc472ca6.css" integrity="sha256-/CIMFdtK7wMYu/MK3EXTPU18iN7/MjiyPrJVr9xHLKY=" rel="preload stylesheet" as="style">
<link rel="icon" href="http://localhost:1313/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="http://localhost:1313/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="http://localhost:1313/favicon-32x32.png">
<link rel="apple-touch-icon" href="http://localhost:1313/apple-touch-icon.png">
<link rel="mask-icon" href="http://localhost:1313/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="http://localhost:1313/posts/three-dogmas-of-reinforcement-learning-copy/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>

    
        <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/katex.min.css" integrity="sha384-nB0miv6/jRmo5UMMR1wu3Gz6NLsoTkbqJghGIsx//Rlm+ZU03BU6SQNC66uf4l5+" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/katex.min.js" integrity="sha384-7zkQWkzuo3B5mTepMUcHkMB5jZaolc2xDwL6VFqjFALcbeS9Ggm/Yr2r3Dy4lfFg" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/contrib/auto-render.min.js" integrity="sha384-43gviWU0YVjaDtb/GhzOouOXtZMP/7XUzwPTstBeZFe/+rCMvRwr4yROQP43s0Xk" crossorigin="anonymous"></script>
<script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement(document.body, {
          
          
          delimiters: [
              {left: '$$', right: '$$', display: true},
              {left: '$', right: '$', display: false},
              {left: '\\(', right: '\\)', display: false},
              {left: '\\[', right: '\\]', display: true}
          ],
          
          throwOnError : false
        });
    });
</script>
    
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="http://localhost:1313/" accesskey="h" title="üß† RL Journal Club (Alt + H)">üß† RL Journal Club</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="http://localhost:1313/archives" title="üóÉÔ∏è Archive">
                    <span>üóÉÔ∏è Archive</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/search" title="üîç Search">
                    <span>üîç Search</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/tags/" title="üè∑Ô∏è Tags">
                    <span>üè∑Ô∏è Tags</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/us/" title="üë§ Us">
                    <span>üë§ Us</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title entry-hint-parent">
      Three Dogmas of Reinforcement Learning
    </h1>
    <div class="post-description">
      by David Abel, Mark K. Ho, and Anna Harutyunyan
    </div>
    <div class="post-meta"><span title='2024-08-06 00:00:00 +0000 UTC'>August 6, 2024</span>&nbsp;¬∑&nbsp;Arash Alikhani

</div>
  </header> <div class="toc">
    <details  open>
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#introduction" aria-label="Introduction">Introduction</a></li>
                <li>
                    <a href="#background" aria-label="Background">Background</a></li>
                <li>
                    <a href="#dogma-one-the-environment-spotlight" aria-label="Dogma One: The Environment Spotlight">Dogma One: The Environment Spotlight</a></li>
                <li>
                    <a href="#dogma-two-learning-as-finding-a-solution" aria-label="Dogma Two: Learning as Finding a Solution">Dogma Two: Learning as Finding a Solution</a></li>
                <li>
                    <a href="#dogma-three-the-reward-hypothesis" aria-label="Dogma Three: The Reward Hypothesis">Dogma Three: The Reward Hypothesis</a></li>
                <li>
                    <a href="#see-also" aria-label="See Also">See Also</a></li>
                <li>
                    <a href="#references" aria-label="References">References</a>
                </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><h2 id="introduction">Introduction<a hidden class="anchor" aria-hidden="true" href="#introduction">#</a></h2>
<p>This paper critically examines the assumptions commonly accepted in modeling reinforcement learning problems, suggesting that these assumptions may impede progress in the field. The authors refer to these assumptions as &ldquo;dogmas.&rdquo;</p>
<blockquote>
<p><strong>Dogma: A fixed, especially religious, belief or set of beliefs that people are expected to accept without any doubts.</strong> <em>‚ÄîFrom the <a href="https://dictionary.cambridge.org/dictionary/english">Cambridge Advanced Learner&rsquo;s Dictionary &amp; Thesaurus</a></em></p></blockquote>
<p>The paper introduces three central dogmas:</p>
<ol>
<li>The Environment Spotlight</li>
<li>Learning as Finding a Solution</li>
<li>The Reward Hypothesis (although not exactly a dogma)</li>
</ol>
<p>The author argues the true reinforcement learning landscape is actualy like this,</p>
<p><img loading="lazy" src="rl1.png#center" alt="rl landscape"  />
</p>
<p>In the words of Rich Sutton:</p>
<blockquote>
<p><strong>RL can be viewed as a microcosm of the whole AI problem.</strong></p></blockquote>
<p>However, today&rsquo;s RL landscape is overly simplified,</p>
<p><img loading="lazy" src="rl2.png#center" alt="rl landscape"  />
</p>
<p>These three dogmas are responsible for narrowing the potential of RL,</p>
<p><img loading="lazy" src="rl3.png#center" alt="rl landscape"  />
</p>
<p>The authors propose that we consider moving beyond these dogmas,</p>
<p><img loading="lazy" src="rl4.png#center" alt="rl landscape"  />
</p>
<p>To reclaim the true landscape of RL,</p>
<p><img loading="lazy" src="rl5.png#center" alt="rl landscape"  />
</p>
<h2 id="background">Background<a hidden class="anchor" aria-hidden="true" href="#background">#</a></h2>
<p>The authors reference Thomas Kuhn&rsquo;s book, &ldquo;The Structure of Scientific Revolutions&rdquo;,</p>
<p><img loading="lazy" src="book1.jpg#center" alt="The Structure of Scientific Revolution"  />
</p>
<p>Kuhn distinguishes between two phases of scientific activity,</p>
<p><img loading="lazy" src="science.png#center" alt="scientific activity"  />
</p>
<ul>
<li><strong>Normal Science:</strong> Resembling puzzle-solving.</li>
<li><strong>Revolutionary Phase:</strong> Involving a fundamental rethinking of the values, methods, and commitments of science, which Kuhn calls a &ldquo;paradigm.&rdquo;</li>
</ul>
<p>Here&rsquo;s an example of a previous paradigm shift in science:</p>
<p><img loading="lazy" src="science-example.png#center" alt="paradigm shift"  />
</p>
<p>The authors explore the paradigm shift needed in RL:</p>
<p><img loading="lazy" src="rl-paradigm-shift.png#center" alt="rl paradigm shift"  />
</p>
<h2 id="dogma-one-the-environment-spotlight">Dogma One: The Environment Spotlight<a hidden class="anchor" aria-hidden="true" href="#dogma-one-the-environment-spotlight">#</a></h2>
<p>The first dogma we call <em>the environment spotlight</em>, which refers to our collective focus on modeling environments and environment-centric concepts rather than agents.</p>
<p><img loading="lazy" src="dogma1pic.png#center" alt="Dogma One: The Environment Spotlight"  />
</p>
<p>What do we mean when we say that we focus on environments? We suggest that it is easy to answer only one of the following two questions:</p>
<ol>
<li>
<p>What is at least one canonical mathematical model of an <strong>environment</strong> in RL?</p>
<ul>
<li>MDP and its variants! And we define everything in terms of it. By embracing the MDP, we are allowed to import a variety of fundamental results and algorithms that define much of our primary research objectives and pathways. For example, we know every MDP has at least one deterministic, optimal, stationary policy, and that dynamic programming can be used to identify this policy.</li>
</ul>
<p><img loading="lazy" src="dogma1-env.png#center" alt="Dogma One: The Environment Spotlight"  />
</p>
</li>
<li>
<p>What is at least one canonical mathematical model of an <strong>agent</strong> in RL?</p>
<ul>
<li>In contrast, this question has no clear answer!</li>
</ul>
</li>
</ol>
<p>The author suggests it is important to define, model, and analyse agents in addition to environments. We should build toward a canonical mathematical model of an agent that can open us to the possibility of discovering general laws governing agents (if they exist).</p>
<p><img loading="lazy" src="dogma1-agent.png#center" alt="Dogma One: The Environment Spotlight"  />
</p>
<p><img loading="lazy" src="dogma1.png#center" alt="Dogma One: The Environment Spotlight"  />
</p>
<h2 id="dogma-two-learning-as-finding-a-solution">Dogma Two: Learning as Finding a Solution<a hidden class="anchor" aria-hidden="true" href="#dogma-two-learning-as-finding-a-solution">#</a></h2>
<p>The second dogma is embedded in the way we treat the concept of learning. We tend to view learning as a finite process involving the search for‚Äîand eventual discovery of‚Äîa solution to a given task.</p>
<p><img loading="lazy" src="dogma2pic.png#center" alt="Dogma Two: Learning as Finding a Solution"  />
</p>
<p>We tend to implicitly assume that the learning agents we design will eventually find a solution to the task at hand, at which point learning can cease. Such agents can be understood as searching through a space of representable functions that captures the possible action-selection strategies available to an agent, similar to the Problem Space Hypothesis, and, critically, this space contains at least one function‚Äîsuch as the optimal policy of an MDP‚Äîthat is of sufficient quality to consider the task of interested solved. Often, we are then interested in designing learning agents that are guaranteed to converge to such an endpoint, at which point the agent can stop its search (and thus, stop its learning).</p>
<p><img loading="lazy" src="standard_rl.png#center" alt="standard rl"  />
</p>
<p>The author suggests to embrace the view that learning can also be treated as adaptation. As a consequence, our focus will drift away from optimality and toward a version of the RL problem in which agents continually improve, rather than focus on agents that are trying to solve a specific problem.</p>
<p><img loading="lazy" src="continual_rl.png#center" alt="continual rl"  />
</p>
<p>When we move away from optimality,</p>
<ul>
<li>How do we think about evaluation?</li>
<li>How, precisely, can we define this form of learning, and differentiate it from others?</li>
<li>What are the basic algorithmic building blocks that carry out this form of learning, and how are they different from the algorithms we use today?</li>
<li>Do our standard analysis tools such as regret and sample complexity still apply?</li>
</ul>
<p>These questions are important, and require reorienting around this alternate view of learning.</p>
<p><img loading="lazy" src="standard_continual_rl.png#center" alt="standard rl vs continual rl"  />
</p>
<p>The authors introduce the book &ldquo;Finite and Infinite Games&rdquo;,</p>
<p><img loading="lazy" src="book2.jpg#center" alt="Finite and Infinite Games"  />
</p>
<p>And the concept of Finite and Infinite Games is summarized in the following quote,</p>
<blockquote>
<p>There are at least two kinds of games, One could be called finite; the other infinite. A finite game is played for the purpose of winning, an infinite game for the purpose of continuing the play.</p></blockquote>
<p>And argues alignment is an infinite game.</p>
<p><img loading="lazy" src="alignment.png#center" alt="alignment is an infinite games"  />
</p>
<p><img loading="lazy" src="dogma2.png#center" alt="Dogma Two: Learning as Finding a Solution"  />
</p>
<h2 id="dogma-three-the-reward-hypothesis">Dogma Three: The Reward Hypothesis<a hidden class="anchor" aria-hidden="true" href="#dogma-three-the-reward-hypothesis">#</a></h2>
<p>The third dogma is the reward hypothesis, which states &ldquo;All of what we mean by goals and purposes can be well thought of as maximization of the expected value of the cumulative sum of a received scalar signal (reward).&rdquo;</p>
<p><img loading="lazy" src="rewardhyp1.png#center" alt="reward hypothesis"  />
</p>
<p><img loading="lazy" src="dogma3pic.png#center" alt="Dogma Three: The Reward Hypothesis"  />
</p>
<p>The authors argue that the reward hypothesis is not truly a dogma. Nevertheless, it is crucial to understand its nuances as we continue to design intelligent agents.</p>
<p>The reward hypothesis basically says,</p>
<p><img loading="lazy" src="rewardhyp2.png#center" alt="reward hypothesis"  />
</p>
<p>In recent analysis by [2] fully characterizes the implicit conditions required for the hypothesis to be true. These conditions come in two forms. First, [2] provide
a pair of interpretative assumptions that clarify what it would mean for the reward hypothesis to be true or false‚Äîroughly, these amount to saying two things (brwon doors).</p>
<p><img loading="lazy" src="alldoors.png#center" alt="reward hypothesis conditions"  />
</p>
<ul>
<li>First, that &ldquo;goals and purposes&rdquo; can be understood in terms of a preference relation on possible outcomes.</li>
</ul>
<p><img loading="lazy" src="food_outcome.png#center" alt="food outcome"  />

<img loading="lazy" src="chess_outcome.png#center" alt="chess outcome"  />
</p>
<ul>
<li>Second, that a reward function captures these preferences if the ordering over agents induced by value functions matches that of the ordering induced by preference on agent outcomes.</li>
</ul>
<p><img loading="lazy" src="reward1.png#center" alt="reward hypothesis conditions"  />

<img loading="lazy" src="reward2.png#center" alt="reward hypothesis conditions"  />
</p>
<p>This leads to the following conjecture,</p>
<p><img loading="lazy" src="conjecture.png#center" alt="reward conjecture"  />
</p>
<p>Then, under this interpretation, a Markov reward function exists to capture a preference relation if and only if the preference relation satisfies the four von Neumann-Morgenstern axioms, and a fifth Bowling et al. call $\gamma$-Temporal Indifference.</p>
<p><img loading="lazy" src="conjecture-prefs.png#center" alt="reward conjecture"  />

<img loading="lazy" src="constraintdoors.png#center" alt="reward conjecture conditions"  />
</p>
<ul>
<li>
<p><strong>Axiom 1: Completeness &gt;</strong> You have a preference between every outcome pair.</p>
<ul>
<li>You can always compare any two choices.</li>
</ul>
</li>
<li>
<p><strong>Axiom 2: Transitivity &gt;</strong> No preference cycles.</p>
<ul>
<li>If you like chocolate more than vanilla, and vanilla more than strawberry, you must like chocolate more than strawberry.</li>
</ul>
</li>
<li>
<p><strong>Axiom 3: Independence &gt;</strong> Independent alternatives can&rsquo;t change your preference.</p>
<ul>
<li>If you like pizza more than salad, and you have to choose between a lottery of pizza or ice cream and a lottery of salad or ice cream, you should still prefer the pizza lottery over the salad lottery.</li>
</ul>
</li>
<li>
<p><strong>Axiom 4: Continuity &gt;</strong> There is always a break even chance.</p>
<ul>
<li>Imagine you like a 100 dollar bill more than a 50 dollar bill, and a 50 dollar bill more than a 1 dollar bill. There should be a scenario where getting a chance at 100 dollar and 1 dollar, with certain probabilities, is equally good as getting the 50 dollar for sure.</li>
</ul>
</li>
</ul>
<p>These 4 axioms are called the von Neumann-Morgenstern axioms.</p>
<p><img loading="lazy" src="photo.png#center" alt="von Neumann-Morgenstern"  />
</p>
<ul>
<li><strong>Axiom 5: Temporal $\boldsymbol{\gamma}$-Indifference &gt;</strong> Discounting is consistent throughout time.
<ul>
<li>Temporal $\gamma$-indifference says that if you are indifferent between receiving a reward at time $t$ and receiving the same reward at time $t+1$, then your preference should not change if we move both time points by the same amount. For instance, if you don&rsquo;t care whether you get a candy today or tomorrow, then you should also not care whether you get the candy next week or the week after.</li>
</ul>
</li>
</ul>
<p>Taking these axioms into account, the reward conjecture becomes the reward theorem,</p>
<p><img loading="lazy" src="theorem.png#center" alt="reward theorem"  />
</p>
<p>It is essential to consider that people do not always conform to these axioms, and human preferences can vary.</p>
<p><img loading="lazy" src="rational_human.png#center" alt="human preference"  />
</p>
<p>It is important that we are aware of the implicit restrictions we are placing on the viable goals and purposes under consideration when we represent a goal or purpose through a reward signal. We should become familiar with the requirements imposed by the five axioms, and be aware of what specifically we might be giving up when we choose to write down a reward function.</p>
<h2 id="see-also">See Also<a hidden class="anchor" aria-hidden="true" href="#see-also">#</a></h2>
<ul>
<li><a href="https://slideslive.com/39015036">David Abel Presentation @ ICML 2023</a></li>
<li><a href="https://david-abel.github.io">David Abel Personal Website</a></li>
<li><a href="https://markkho.github.io">Mark Ho Personal Website</a></li>
<li><a href="https://anna.harutyunyan.net">Anna Harutyunyan Personal Website</a></li>
</ul>
<h2 id="references">References<a hidden class="anchor" aria-hidden="true" href="#references">#</a></h2>
<p>[1] <a href="https://arxiv.org/abs/2407.10583">Abel, David, Mark K. Ho, and Anna Harutyunyan. &ldquo;Three Dogmas of Reinforcement Learning.&rdquo; arXiv preprint arXiv:2407.10583 (2024).</a></p>
<p>[2] <a href="https://arxiv.org/abs/2212.10420">Bowling, Michael, et al. &ldquo;Settling the reward hypothesis.&rdquo; International Conference on Machine Learning. PMLR, 2023.</a></p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="http://localhost:1313/tags/dogmas/">Dogmas</a></li>
      <li><a href="http://localhost:1313/tags/reward-hypothesis/">Reward-Hypothesis</a></li>
      <li><a href="http://localhost:1313/tags/philosophy/">Philosophy</a></li>
      <li><a href="http://localhost:1313/tags/paradigm/">Paradigm</a></li>
      <li><a href="http://localhost:1313/tags/icml/">ICML</a></li>
      <li><a href="http://localhost:1313/tags/rlc/">RLC</a></li>
      <li><a href="http://localhost:1313/tags/2024/">2024</a></li>
      <li><a href="http://localhost:1313/tags/rlc24/">RLC24</a></li>
    </ul>
  </footer>
</article>

<div id="disqus_thread"></div>
<script>
    window.disqus_config = function () {
    
    
    
    };
    (function() {
        if (["localhost", "127.0.0.1"].indexOf(window.location.hostname) != -1) {
            document.getElementById('disqus_thread').innerHTML = 'Disqus comments not available by default when the website is previewed locally.';
            return;
        }
        var d = document, s = d.createElement('script'); s.async = true;
        s.src = '//' + "rljclub-github-io" + '.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="https://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
    </main>
    
<footer class="footer">
        <span>&copy; 2025 <a href="http://localhost:1313/">üß† RL Journal Club</a></span> ¬∑ 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
