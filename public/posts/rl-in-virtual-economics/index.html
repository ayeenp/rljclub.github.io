<!DOCTYPE html>
<html lang="en" dir="auto">

<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="noindex, nofollow">
<title>Rl in Virtual Economics | üß† RL Journal Club</title>
<meta name="keywords" content="">
<meta name="description" content="Introduction
Reinforcement Learning (RL) has moved far beyond academic curiosity‚Äîit is now being applied to some of the world‚Äôs most complex and dynamic systems, from financial markets to decentralized protocols.
In particular, RL is reshaping how we think about governance, trading, and risk management in environments where rules are constantly changing.
This blog explores two major domains where RL is having impact:

Decentralized Finance (DeFi) ‚Äì where RL agents can govern lending pools, adjust interest rates, and prevent insolvency.
Algorithmic Trading ‚Äì where RL agents learn optimal trading strategies directly from market data.

Together, these works show how RL can serve as the decision-making engine of future virtual economies.">
<meta name="author" content="Mohammadjavad Ahmadpour, Amirmahdi Meighani">
<link rel="canonical" href="http://localhost:1313/posts/rl-in-virtual-economics/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.54405a410796490bc874ab6181fac9b675753cc2b91375d8f882566459eca428.css" integrity="sha256-VEBaQQeWSQvIdKthgfrJtnV1PMK5E3XY&#43;IJWZFnspCg=" rel="preload stylesheet" as="style">
<link rel="icon" href="http://localhost:1313/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="http://localhost:1313/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="http://localhost:1313/favicon-32x32.png">
<link rel="apple-touch-icon" href="http://localhost:1313/apple-touch-icon.png">
<link rel="mask-icon" href="http://localhost:1313/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="http://localhost:1313/posts/rl-in-virtual-economics/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>

    
        <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/katex.min.css" integrity="sha384-nB0miv6/jRmo5UMMR1wu3Gz6NLsoTkbqJghGIsx//Rlm+ZU03BU6SQNC66uf4l5+" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/katex.min.js" integrity="sha384-7zkQWkzuo3B5mTepMUcHkMB5jZaolc2xDwL6VFqjFALcbeS9Ggm/Yr2r3Dy4lfFg" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/contrib/auto-render.min.js" integrity="sha384-43gviWU0YVjaDtb/GhzOouOXtZMP/7XUzwPTstBeZFe/+rCMvRwr4yROQP43s0Xk" crossorigin="anonymous"></script>
<script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement(document.body, {
          
          
          delimiters: [
              {left: '$$', right: '$$', display: true},
              {left: '$', right: '$', display: false},
              {left: '\\(', right: '\\)', display: false},
              {left: '\\[', right: '\\]', display: true}
          ],
          
          throwOnError : false
        });
    });
</script>
    
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="http://localhost:1313/" accesskey="h" title="üß† RL Journal Club (Alt + H)">üß† RL Journal Club</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="http://localhost:1313/archives" title="üóÉÔ∏è Archive">
                    <span>üóÉÔ∏è Archive</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/search" title="üîç Search">
                    <span>üîç Search</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/tags/" title="üè∑Ô∏è Tags">
                    <span>üè∑Ô∏è Tags</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/us/" title="üë§ Us">
                    <span>üë§ Us</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title entry-hint-parent">
      Rl in Virtual Economics
    </h1>
    <div class="post-meta"><span title='2025-09-07 15:49:28 +0330 +0330'>September 7, 2025</span>&nbsp;¬∑&nbsp;Mohammadjavad Ahmadpour, Amirmahdi Meighani

</div>
  </header> <div class="toc">
    <details  open>
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul><ul>
                <li>
                    <a href="#introduction" aria-label="Introduction">Introduction</a></li></ul>
                    
                <li>
                    <a href="#reinforcement-learning-in-virtual-economies" aria-label="Reinforcement Learning in Virtual Economies:">Reinforcement Learning in Virtual Economies:</a><ul>
                        
                <li>
                    <a href="#table-of-contents" aria-label="Table of Contents">Table of Contents</a></li>
                <li>
                    <a href="#governing-defi-with-ai" aria-label="Governing DeFi with AI">Governing DeFi with AI</a></li>
                <li>
                    <a href="#defi-vs-traditional-finance-lending-by-numbers" aria-label="DeFi vs Traditional Finance: Lending by Numbers">DeFi vs Traditional Finance: Lending by Numbers</a><ul>
                        
                <li>
                    <a href="#traditional-lending-bank-example" aria-label="Traditional Lending (Bank Example)">Traditional Lending (Bank Example)</a></li>
                <li>
                    <a href="#defi-lending-aave-example" aria-label="DeFi Lending (Aave Example)">DeFi Lending (Aave Example)</a></li></ul>
                </li>
                <li>
                    <a href="#the-utilization-curve-how-interest-rates-work-in-defi" aria-label="The Utilization Curve: How Interest Rates Work in DeFi">The Utilization Curve: How Interest Rates Work in DeFi</a><ul>
                        
                <li>
                    <a href="#utilization-ratio" aria-label="Utilization Ratio">Utilization Ratio</a></li>
                <li>
                    <a href="#example" aria-label="Example">Example</a></li></ul>
                </li>
                <li>
                    <a href="#loan-to-value-ltv-and-overcollateralization" aria-label="Loan-to-Value (LTV) and Overcollateralization">Loan-to-Value (LTV) and Overcollateralization</a><ul>
                        
                <li>
                    <a href="#example-1--safe-position" aria-label="Example 1 ‚Äì Safe Position">Example 1 ‚Äì Safe Position</a></li>
                <li>
                    <a href="#example-2--risky-position" aria-label="Example 2 ‚Äì Risky Position">Example 2 ‚Äì Risky Position</a></li></ul>
                </li>
                <li>
                    <a href="#why-rl-in-defi-governance" aria-label="Why RL in DeFi Governance?">Why RL in DeFi Governance?</a></li>
                <li>
                    <a href="#the-rl-workflow-in-defi" aria-label="The RL Workflow in DeFi">The RL Workflow in DeFi</a><ul>
                        
                <li>
                    <a href="#offline-rl" aria-label="Offline RL">Offline RL</a></li>
                <li>
                    <a href="#online-rl" aria-label="Online RL">Online RL</a></li></ul>
                </li>
                <li>
                    <a href="#results-rl-vs-rule-based-governance" aria-label="Results: RL vs Rule-Based Governance">Results: RL vs Rule-Based Governance</a><ul>
                        
                <li>
                    <a href="#profitability" aria-label="Profitability">Profitability</a></li>
                <li>
                    <a href="#stress-events" aria-label="Stress Events">Stress Events</a></li>
                <li>
                    <a href="#capital-efficiency" aria-label="Capital Efficiency">Capital Efficiency</a></li>
                <li>
                    <a href="#governance-agility" aria-label="Governance Agility">Governance Agility</a></li></ul>
                </li>
                <li>
                    <a href="#conclusion" aria-label="Conclusion">Conclusion</a></li></ul>
                </li>
                <li>
                    <a href="#deep-reinforcement-learning-for-trading" aria-label="Deep Reinforcement Learning for Trading">Deep Reinforcement Learning for Trading</a><ul>
                        
                <li>
                    <a href="#introduction-1" aria-label="Introduction">Introduction</a></li>
                <li>
                    <a href="#table-of-contents-1" aria-label="Table of Contents">Table of Contents</a></li>
                <li>
                    <a href="#trading-as-an-mdp" aria-label="Trading as an MDP">Trading as an MDP</a></li>
                <li>
                    <a href="#deep-rl-techniques" aria-label="Deep RL Techniques">Deep RL Techniques</a></li>
                <li>
                    <a href="#environment-and-training" aria-label="Environment and Training">Environment and Training</a></li>
                <li>
                    <a href="#results" aria-label="Results">Results</a></li>
                <li>
                    <a href="#challenges-and-limitations" aria-label="Challenges and Limitations">Challenges and Limitations</a></li>
                <li>
                    <a href="#conclusion-1" aria-label="Conclusion">Conclusion</a></li>
                <li>
                    <a href="#references" aria-label="References">References</a>
                </li>
            </ul>
            </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><h2 id="introduction">Introduction<a hidden class="anchor" aria-hidden="true" href="#introduction">#</a></h2>
<p>Reinforcement Learning (RL) has moved far beyond academic curiosity‚Äîit is now being applied to some of the world‚Äôs most complex and dynamic systems, from <strong>financial markets</strong> to <strong>decentralized protocols</strong>.<br>
In particular, RL is reshaping how we think about <strong>governance</strong>, <strong>trading</strong>, and <strong>risk management</strong> in environments where rules are constantly changing.</p>
<p>This blog explores two major domains where RL is having impact:</p>
<ol>
<li><strong>Decentralized Finance (DeFi)</strong> ‚Äì where RL agents can govern lending pools, adjust interest rates, and prevent insolvency.</li>
<li><strong>Algorithmic Trading</strong> ‚Äì where RL agents learn optimal trading strategies directly from market data.</li>
</ol>
<p>Together, these works show how RL can serve as the <strong>decision-making engine</strong> of future virtual economies.</p>
<h1 id="reinforcement-learning-in-virtual-economies">Reinforcement Learning in Virtual Economies:<a hidden class="anchor" aria-hidden="true" href="#reinforcement-learning-in-virtual-economies">#</a></h1>
<h2 id="table-of-contents">Table of Contents<a hidden class="anchor" aria-hidden="true" href="#table-of-contents">#</a></h2>
<ul>
<li><a href="#Governing-DeFi-with-AI">Governing DeFi with AI</a></li>
<li><a href="#defi-vs-traditional-finance-lending-by-numbers">DeFi vs Traditional Finance: Lending by Numbers</a></li>
<li><a href="#the-utilization-curve-how-interest-rates-work-in-defi">The Utilization Curve: How Interest Rates Work in DeFi</a></li>
<li><a href="#loan-to-value-ltv-and-overcollateralization">Loan-to-Value (LTV) and Overcollateralization</a></li>
<li><a href="#why-rl-in-defi-governance">Why RL in DeFi Governance?</a></li>
<li><a href="#the-rl-workflow-in-defi">The RL Workflow in DeFi</a></li>
<li><a href="#results-rl-vs-rule-based-governance">Results: RL vs Rule-Based Governance</a></li>
<li><a href="#conclusion">Conclusion</a></li>
<li><a href="#references">References</a></li>
</ul>
<hr>
<h2 id="governing-defi-with-ai">Governing DeFi with AI<a hidden class="anchor" aria-hidden="true" href="#governing-defi-with-ai">#</a></h2>
<p>Virtual economies are no longer confined to video games. With the rise of <strong>cryptocurrencies</strong> and <strong>Decentralized Finance (DeFi)</strong>, we now have financial ecosystems that exist entirely on-chain. These systems facilitate lending, borrowing, and trading without traditional intermediaries like banks. Instead, they rely on <strong>smart contracts</strong>‚Äîself-executing code deployed on blockchains.</p>
<p>However, these protocols are still governed by <strong>human decisions</strong>‚Äîdevelopers and communities vote to set critical parameters like interest rates, collateral requirements, and liquidation thresholds. This manual governance process is slow, biased, and often ill-suited to crypto‚Äôs volatile markets.</p>
<p>This is where <strong>Reinforcement Learning (RL)</strong> comes in. RL agents can continuously monitor markets, simulate outcomes, and automatically adjust parameters to optimize long-term outcomes such as <strong>profitability, liquidity stability, and resilience to attacks</strong>. Two recent works exemplify this direction:</p>
<ul>
<li><strong>Auto.gov</strong>: An RL-based governance agent that dynamically adjusts risk parameters like collateral factors to prevent insolvency and resist price oracle attacks.</li>
<li><strong>From Rules to Rewards</strong>: An RL framework that learns to adjust interest rates in Aave lending pools, outperforming static rule-based models under stress scenarios like the 2021 crash and the 2023 USDC depeg.</li>
</ul>
<hr>
<h2 id="defi-vs-traditional-finance-lending-by-numbers">DeFi vs Traditional Finance: Lending by Numbers<a hidden class="anchor" aria-hidden="true" href="#defi-vs-traditional-finance-lending-by-numbers">#</a></h2>
<p>To understand why RL is valuable, let‚Äôs first compare <strong>traditional finance (TradFi)</strong> and <strong>DeFi lending</strong>.</p>
<h3 id="traditional-lending-bank-example">Traditional Lending (Bank Example)<a hidden class="anchor" aria-hidden="true" href="#traditional-lending-bank-example">#</a></h3>
<ul>
<li>You want to borrow $1000.</li>
<li>The bank checks your <strong>credit score</strong> and income.</li>
<li>If approved, you deposit collateral (e.g., your car worth $2000).</li>
<li>You borrow at a fixed <strong>interest rate</strong> (say 8% annually).</li>
<li>If you default, the bank repossesses your car.</li>
</ul>
<p>Key point: Banks rely on <strong>creditworthiness</strong> and <strong>legal enforcement</strong>.</p>
<h3 id="defi-lending-aave-example">DeFi Lending (Aave Example)<a hidden class="anchor" aria-hidden="true" href="#defi-lending-aave-example">#</a></h3>
<ul>
<li>Alice deposits <strong>$100 ETH</strong> into Aave.</li>
<li>Protocol sets <strong>Loan-to-Value (LTV) = 70%</strong>.</li>
<li>Alice can borrow <strong>$70 USDC</strong>.</li>
<li>If ETH price drops 40% (her $100 ETH is now worth $60), her <strong>collateral value &lt; debt</strong>.</li>
<li>Smart contract <strong>liquidates</strong> her position automatically, selling her ETH to repay lenders.</li>
</ul>
<p>Key point: DeFi relies on <strong>overcollateralization and liquidation bots</strong>, not credit checks.</p>
<hr>
<h2 id="the-utilization-curve-how-interest-rates-work-in-defi">The Utilization Curve: How Interest Rates Work in DeFi<a hidden class="anchor" aria-hidden="true" href="#the-utilization-curve-how-interest-rates-work-in-defi">#</a></h2>
<p>One of the most important mechanisms in DeFi lending is the <strong>utilization curve</strong>, which governs how interest rates change depending on market demand.</p>
<h3 id="utilization-ratio">Utilization Ratio<a hidden class="anchor" aria-hidden="true" href="#utilization-ratio">#</a></h3>
<p>$$
U = \frac{\text{Total Borrowed}}{\text{Total Supplied}}
$$</p>
<ul>
<li>
<p><strong>Low Utilization (U close to 0):</strong><br>
Few borrowers, plenty of liquidity. Interest rates stay low to encourage borrowing.</p>
</li>
<li>
<p><strong>High Utilization (U close to 1):</strong><br>
Most liquidity is borrowed, very little remains. Rates increase sharply to discourage borrowing and attract more deposits.</p>
</li>
</ul>
<h3 id="example">Example<a hidden class="anchor" aria-hidden="true" href="#example">#</a></h3>
<ul>
<li>Total Supplied = <strong>1000 USDC</strong></li>
<li>Total Borrowed = <strong>700 USDC</strong></li>
<li>Utilization = 700 / 1000 = <strong>70%</strong></li>
</ul>
<p>At 70%, the <strong>borrow rate</strong> might be 8% and the <strong>deposit rate</strong> 5%. If utilization rises to 90%, the borrow rate could spike to 25% to prevent liquidity shortages.</p>
<p>This creates a <strong>kinked utilization curve</strong>:</p>
<ul>
<li>Smooth slope at low utilization.</li>
<li>Sharp increase after a threshold (e.g., 80%).</li>
</ul>
<p>Such dynamics are crucial but also <strong>rigid</strong> when hard-coded into smart contracts. RL offers a way to <strong>adapt these curves dynamically</strong>.</p>
<p><img loading="lazy" src="utilization_curve.png" alt="Utilization Curve in DeFi Lending"  />
</p>
<hr>
<h2 id="loan-to-value-ltv-and-overcollateralization">Loan-to-Value (LTV) and Overcollateralization<a hidden class="anchor" aria-hidden="true" href="#loan-to-value-ltv-and-overcollateralization">#</a></h2>
<p>In DeFi, loans are <strong>overcollateralized</strong> to protect lenders.</p>
<ul>
<li><strong>Loan-to-Value (LTV):</strong><br>
Ratio of borrowed amount to collateral value.</li>
</ul>
<p>$$
\text{LTV} = \frac{\text{Loan Value}}{\text{Collateral Value}}
$$</p>
<ul>
<li><strong>Overcollateralization:</strong><br>
Borrowers must deposit more than they borrow.</li>
</ul>
<h3 id="example-1--safe-position">Example 1 ‚Äì Safe Position<a hidden class="anchor" aria-hidden="true" href="#example-1--safe-position">#</a></h3>
<ul>
<li>Deposit = <strong>$200 ETH</strong></li>
<li>LTV = 70%</li>
<li>Max Borrow = $140 USDC</li>
<li>Actual Borrow = $100 USDC</li>
<li>LTV = 50% ‚Üí Safe position.</li>
</ul>
<h3 id="example-2--risky-position">Example 2 ‚Äì Risky Position<a hidden class="anchor" aria-hidden="true" href="#example-2--risky-position">#</a></h3>
<ul>
<li>Same deposit = $200 ETH</li>
<li>Borrow = $140 USDC</li>
<li>ETH price falls 30% ‚Üí Collateral now worth $140.</li>
<li>New LTV = 100% ‚Üí Position is liquidated.</li>
</ul>
<p>Liquidation prevents systemic insolvency but can be <strong>brutal for borrowers</strong>. RL can help by <strong>tuning liquidation thresholds and collateral factors dynamically</strong> to balance risk and efficiency.</p>
<hr>
<h2 id="why-rl-in-defi-governance">Why RL in DeFi Governance?<a hidden class="anchor" aria-hidden="true" href="#why-rl-in-defi-governance">#</a></h2>
<p>Currently, DeFi governance relies on:</p>
<ol>
<li><strong>Manual proposals</strong> in forums.</li>
<li><strong>Community voting</strong> (governance tokens).</li>
<li><strong>Implementation delays</strong> (days or weeks).</li>
</ol>
<p>Problems:</p>
<ul>
<li><strong>Too slow</strong> for volatile markets.</li>
<li><strong>Centralized in practice</strong> (whales dominate votes).</li>
<li><strong>Static rules</strong> ignore market stress.</li>
</ul>
<p>RL provides:</p>
<ul>
<li><strong>Adaptive governance</strong> ‚Üí protocols adjust within minutes, not weeks.</li>
<li><strong>Objective optimization</strong> ‚Üí maximize profitability and stability.</li>
<li><strong>Resilience to attacks</strong> ‚Üí respond in real time to flash loan or oracle exploits.</li>
</ul>
<p>Example: Auto.gov prevented losses from simulated <strong>oracle attacks</strong> that drained protocols like bZx and Cream Finance.</p>
<hr>
<h2 id="the-rl-workflow-in-defi">The RL Workflow in DeFi<a hidden class="anchor" aria-hidden="true" href="#the-rl-workflow-in-defi">#</a></h2>
<p>Reinforcement Learning models DeFi as a <strong>Markov Decision Process (MDP)</strong>.</p>
<ol>
<li>
<p><strong>States (S):</strong><br>
Market conditions like tokens&rsquo; prices, utilization, liquidity, collateral ratios, volatility.</p>
</li>
<li>
<p><strong>Actions (A):</strong><br>
Adjust parameters (e.g., interest rates, collateral factor, liquidation threshold).</p>
</li>
<li>
<p><strong>Rewards (R):</strong><br>
Objectives such as:</p>
<ul>
<li>Maximize protocol profitability.</li>
<li>Maintain liquidity efficiency.</li>
<li>Minimize bad debt.</li>
</ul>
</li>
<li>
<p><strong>Policy (œÄ):</strong><br>
The agent‚Äôs learned strategy for choosing actions.</p>
</li>
</ol>
<h3 id="offline-rl">Offline RL<a hidden class="anchor" aria-hidden="true" href="#offline-rl">#</a></h3>
<ul>
<li>Trains on <strong>historical data</strong> (e.g., Aave v2/v3).</li>
<li>Safe: no risk of experimenting live.</li>
<li>Example: TD3-BC learned to preemptively adjust rates during the <strong>USDC depeg</strong> in 2023.</li>
</ul>
<h3 id="online-rl">Online RL<a hidden class="anchor" aria-hidden="true" href="#online-rl">#</a></h3>
<ul>
<li>Uses <strong>simulated environments</strong> (e.g., Auto.gov modeled Aave-like lending pools).</li>
<li>RL agents explore and learn in simulation.</li>
<li>Once trained, policies can be deployed on-chain.</li>
</ul>
<hr>
<h2 id="results-rl-vs-rule-based-governance">Results: RL vs Rule-Based Governance<a hidden class="anchor" aria-hidden="true" href="#results-rl-vs-rule-based-governance">#</a></h2>
<h3 id="profitability">Profitability<a hidden class="anchor" aria-hidden="true" href="#profitability">#</a></h3>
<ul>
<li>Auto.gov paper improved profitability by <strong>60%+</strong> in simulations and 10x over static baselines.</li>
</ul>
<h3 id="stress-events">Stress Events<a hidden class="anchor" aria-hidden="true" href="#stress-events">#</a></h3>
<ul>
<li>TD3-BC reacted faster to the <strong>FTX collapse (2022)</strong> and <strong>ETH crash (2024)</strong> than Aave‚Äôs rules, improving liquidity provider retention.</li>
</ul>
<h3 id="capital-efficiency">Capital Efficiency<a hidden class="anchor" aria-hidden="true" href="#capital-efficiency">#</a></h3>
<ul>
<li>RL produced <strong>smoother rate curves</strong>, reducing shocks for borrowers and improving returns for depositors.</li>
<li>Prevented excessive underutilization or overexposure.</li>
</ul>
<h3 id="governance-agility">Governance Agility<a hidden class="anchor" aria-hidden="true" href="#governance-agility">#</a></h3>
<ul>
<li>RL adapts in <strong>hours</strong>, while DAO votes take <strong>weeks</strong>.</li>
<li>Reduces vulnerability to manipulation and slow decision-making.</li>
</ul>
<hr>
<h2 id="conclusion">Conclusion<a hidden class="anchor" aria-hidden="true" href="#conclusion">#</a></h2>
<p>DeFi represents the world‚Äôs largest experiment in algorithmic finance. But its governance still relies heavily on human decision-making, leaving it exposed to <strong>bias, rigidity, and systemic risk</strong>.</p>
<p>Reinforcement Learning offers a way forward:</p>
<ul>
<li><strong>Offline RL</strong> learns from history.</li>
<li><strong>Online RL</strong> simulates future markets.</li>
<li>Together, they provide <strong>adaptive, attack-resistant governance</strong>.</li>
</ul>
<p>As shown in <strong>Auto.gov</strong> and <strong>TD3-BC</strong>, RL-based agents outperform static rule-based models, improving profitability, liquidity, and systemic resilience.</p>
<p>In many ways, RL could serve as the <strong>central bank of DeFi</strong>‚Äîa governor that adjusts policy dynamically, balancing borrower needs with lender rewards, ensuring that decentralized economies remain stable and sustainable.</p>
<hr>
<h1 id="deep-reinforcement-learning-for-trading">Deep Reinforcement Learning for Trading<a hidden class="anchor" aria-hidden="true" href="#deep-reinforcement-learning-for-trading">#</a></h1>
<h2 id="introduction-1">Introduction<a hidden class="anchor" aria-hidden="true" href="#introduction-1">#</a></h2>
<p>Financial trading is a natural environment for <strong>Reinforcement Learning (RL)</strong>. Markets are dynamic, uncertain, and reward-driven‚Äîmaking them well-suited to RL‚Äôs trial-and-error learning framework.<br>
In their work, <em>Deep Reinforcement Learning for Trading</em> (<a href="https://arxiv.org/abs/1911.10107">Deng et al., 2019</a>), the authors show how RL can be applied to design <strong>adaptive trading strategies</strong> that outperform traditional rule-based methods.</p>
<hr>
<h2 id="table-of-contents-1">Table of Contents<a hidden class="anchor" aria-hidden="true" href="#table-of-contents-1">#</a></h2>
<ul>
<li><a href="#trading-as-an-mdp">Trading as an MDP</a></li>
<li><a href="#deep-rl-techniques">Deep RL Techniques</a></li>
<li><a href="#environment-and-training">Environment and Training</a></li>
<li><a href="#results">Results</a></li>
<li><a href="#challenges-and-limitations">Challenges and Limitations</a></li>
<li><a href="#conclusion">Conclusion</a></li>
<li><a href="#references">References</a></li>
</ul>
<hr>
<h2 id="trading-as-an-mdp">Trading as an MDP<a hidden class="anchor" aria-hidden="true" href="#trading-as-an-mdp">#</a></h2>
<p>The authors model trading as a <strong>Markov Decision Process (MDP)</strong>:</p>
<ul>
<li><strong>State (S):</strong> Market indicators (e.g., price history, volume, technical factors).</li>
<li><strong>Action (A):</strong> Buy, sell, or hold.</li>
<li><strong>Reward (R):</strong> Profit &amp; Loss (PnL), adjusted for transaction fees and sometimes risk measures.</li>
</ul>
<p>This formulation allows RL agents to optimize strategies based not only on short-term profits but also <strong>risk-adjusted returns</strong> over time.</p>
<hr>
<h2 id="deep-rl-techniques">Deep RL Techniques<a hidden class="anchor" aria-hidden="true" href="#deep-rl-techniques">#</a></h2>
<p>Several RL algorithms are explored for trading:</p>
<ul>
<li><strong>Deep Q-Networks (DQN):</strong> Learn discrete trading policies (buy/sell/hold).</li>
<li><strong>Policy Gradient Methods:</strong> Directly optimize trading policies.</li>
<li><strong>Actor-Critic Methods:</strong> Combine value-based and policy-based approaches for more stable learning.</li>
</ul>
<p>Each method has strengths‚ÄîDQN handles discrete decisions well, while actor-critic models adapt better to continuous and noisy markets.</p>
<hr>
<h2 id="environment-and-training">Environment and Training<a hidden class="anchor" aria-hidden="true" href="#environment-and-training">#</a></h2>
<ul>
<li><strong>Historical Market Data:</strong> Training is performed on past stock and cryptocurrency data.</li>
<li><strong>Simulated Market Environment:</strong> Allows safe exploration before live trading.</li>
<li><strong>Feature Engineering:</strong> Market indicators, momentum signals, and volatility are fed into deep neural networks.</li>
</ul>
<hr>
<h2 id="results">Results<a hidden class="anchor" aria-hidden="true" href="#results">#</a></h2>
<ul>
<li>RL agents <strong>outperform rule-based baselines</strong> like moving-average crossovers.</li>
<li><strong>Adaptability:</strong> RL policies respond better to shifts between bullish and bearish markets.</li>
<li><strong>Risk Awareness:</strong> Reward functions that include Sharpe ratio or volatility lead to more robust strategies.<br>
<img loading="lazy" src="tr-res.png" alt="Cumulative trade returns for First row: commodity, equity index and fixed income; Second row: FX and the portfolio of using all contracts."  />
</li>
</ul>
<hr>
<h2 id="challenges-and-limitations">Challenges and Limitations<a hidden class="anchor" aria-hidden="true" href="#challenges-and-limitations">#</a></h2>
<p>While promising, RL trading systems face several hurdles:</p>
<ul>
<li><strong>Overfitting:</strong> RL agents may learn patterns that don‚Äôt generalize.</li>
<li><strong>Sample inefficiency:</strong> Deep RL requires large amounts of data.</li>
<li><strong>Execution risk:</strong> Live deployment must handle latency, slippage, and adversarial conditions.</li>
</ul>
<hr>
<h2 id="conclusion-1">Conclusion<a hidden class="anchor" aria-hidden="true" href="#conclusion-1">#</a></h2>
<p>Deep Reinforcement Learning offers a powerful new approach to trading by:</p>
<ul>
<li>Learning strategies <strong>directly from data</strong> instead of handcrafted rules.</li>
<li>Adapting to <strong>non-stationary market conditions</strong>.</li>
<li>Balancing risk and reward more effectively than traditional methods.</li>
</ul>
<p>However, robust deployment requires careful design to avoid overfitting and ensure stability in real markets.</p>
<hr>
<h2 id="references">References<a hidden class="anchor" aria-hidden="true" href="#references">#</a></h2>
<ol>
<li>Zihao Zhang, Stefan Zohren, Stephen Roberts. (2019). <strong>Deep Reinforcement Learning for Trading</strong>. <em>arXiv preprint</em>. <a href="https://arxiv.org/abs/1911.10107">arXiv:1911.10107</a>.</li>
</ol>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
    </ul>
  </footer>
</article>

<div id="disqus_thread"></div>
<script>
    window.disqus_config = function () {
    
    
    
    };
    (function() {
        if (["localhost", "127.0.0.1"].indexOf(window.location.hostname) != -1) {
            document.getElementById('disqus_thread').innerHTML = 'Disqus comments not available by default when the website is previewed locally.';
            return;
        }
        var d = document, s = d.createElement('script'); s.async = true;
        s.src = '//' + "rljclub-github-io" + '.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="https://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
    </main>
    
<footer class="footer">
        <span>&copy; 2025 <a href="http://localhost:1313/">üß† RL Journal Club</a></span> ¬∑ 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
